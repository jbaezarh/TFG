geom_sf(data = incendio_estudio, fill="transparent",col="red") +
labs(color = "Probabilidad\nestimada de\nincendio") +
coord_sf(xlim=c(bbox$xmin,bbox$xmax),ylim=c(bbox$ymin,bbox$ymax))
g
models
g <- ggplot(data = and) +
geom_sf() +
geom_sf(data = pred, aes(color=pred_class, alpha = .pred_1),size = 1.5) +
facet_wrap(~date) +
scale_color_gradientn(colours = rainbow(5,rev=T),limits=c(0,1)) +
# scale_color_gradient(low="blue", high="red")+
labs(title="Incendio de Sierra Bermeja",
subtitle = paste0("Model: ",model$model_name)) +
guides(alpha = "none") +
theme_minimal() +
theme(axis.text.x=element_blank(),
axis.ticks.x=element_blank(),
axis.text.y=element_blank(),
axis.ticks.y=element_blank(),
panel.grid.major = element_blank(),
panel.grid.minor = element_blank()) +
geom_sf(data = incendio_estudio, fill="transparent",col="red") +
labs(color = "Probabilidad\nestimada de\nincendio") +
coord_sf(xlim=c(bbox$xmin,bbox$xmax),ylim=c(bbox$ymin,bbox$ymax))
g
ggplot(data = and) +
geom_sf() +
geom_sf(data = pred, aes(color=pred_class, alpha = .pred_1),size = 1.5) +
facet_wrap(~date)
g <- ggplot(data = and) +
geom_sf() +
geom_sf(data = pred, aes(color=pred_class, alpha = pred_class),size = 1.5) +
facet_wrap(~date) +
scale_color_gradientn(colours = rainbow(5,rev=T),limits=c(0,1)) +
# scale_color_gradient(low="blue", high="red")+
labs(title="Incendio de Sierra Bermeja",
subtitle = paste0("Model: ",model$model_name)) +
guides(alpha = "none") +
theme_minimal() +
theme(axis.text.x=element_blank(),
axis.ticks.x=element_blank(),
axis.text.y=element_blank(),
axis.ticks.y=element_blank(),
panel.grid.major = element_blank(),
panel.grid.minor = element_blank()) +
geom_sf(data = incendio_estudio, fill="transparent",col="red") +
labs(color = "Probabilidad\nestimada de\nincendio") +
coord_sf(xlim=c(bbox$xmin,bbox$xmax),ylim=c(bbox$ymin,bbox$ymax))
g
# load("salidas_intermedias/trained_models_strat_2024_04_28.RData") # Modelos
load("Private/all_models_test.RData")
model <- models %>% filter(model_name=="lr")
rm(models) # Es un archivo muy pesado, lo eliminamos de la memoria
pred_class = model %>%
pull(last_fit) %>%
.[[1]] %>%
extract_workflow() %>%
predict(new_data = full_grid)
pred_probs = model %>%
pull(last_fit) %>%
.[[1]] %>%
extract_workflow() %>%
predict(new_data = full_grid,type="prob")
pred = cbind(full_grid,pred_class,pred_probs)
g <- ggplot(data = and) +
geom_sf() +
geom_sf(data = pred, aes(color=.pred_1, alpha = .pred_1),size = 1.5) +
facet_wrap(~date) +
scale_color_gradientn(colours = rainbow(5,rev=T),limits=c(0,1)) +
# scale_color_gradient(low="blue", high="red")+
labs(title="Incendio de Sierra Bermeja",
subtitle = paste0("Model: ",model$model_name)) +
guides(alpha = "none") +
theme_minimal() +
theme(axis.text.x=element_blank(),
axis.ticks.x=element_blank(),
axis.text.y=element_blank(),
axis.ticks.y=element_blank(),
panel.grid.major = element_blank(),
panel.grid.minor = element_blank()) +
geom_sf(data = incendio_estudio, fill="transparent",col="red") +
labs(color = "Probabilidad\nestimada de\nincendio") +
coord_sf(xlim=c(bbox$xmin,bbox$xmax),ylim=c(bbox$ymin,bbox$ymax))
g
g <- ggplot(data = and) +
geom_sf() +
geom_sf(data = pred, aes(color=pred_class, alpha = .pred_1),size = 1.5) +
facet_wrap(~date) +
scale_color_gradientn(colours = rainbow(5,rev=T),limits=c(0,1)) +
# scale_color_gradient(low="blue", high="red")+
labs(title="Incendio de Sierra Bermeja",
subtitle = paste0("Model: ",model$model_name)) +
guides(alpha = "none") +
theme_minimal() +
theme(axis.text.x=element_blank(),
axis.ticks.x=element_blank(),
axis.text.y=element_blank(),
axis.ticks.y=element_blank(),
panel.grid.major = element_blank(),
panel.grid.minor = element_blank()) +
geom_sf(data = incendio_estudio, fill="transparent",col="red") +
labs(color = "Probabilidad\nestimada de\nincendio") +
coord_sf(xlim=c(bbox$xmin,bbox$xmax),ylim=c(bbox$ymin,bbox$ymax))
g
ggplot(data = and) +
geom_sf() +
geom_sf(data = pred, aes(color=pred_class, alpha = .pred_1),size = 1.5) +
facet_wrap(~date)
summary(pred)
g <- ggplot(data = and) +
geom_sf() +
geom_sf(data = pred, aes(color=.pred_class, alpha = .pred_1),size = 1.5) +
facet_wrap(~date) +
scale_color_gradientn(colours = rainbow(5,rev=T),limits=c(0,1)) +
# scale_color_gradient(low="blue", high="red")+
labs(title="Incendio de Sierra Bermeja",
subtitle = paste0("Model: ",model$model_name)) +
guides(alpha = "none") +
theme_minimal() +
theme(axis.text.x=element_blank(),
axis.ticks.x=element_blank(),
axis.text.y=element_blank(),
axis.ticks.y=element_blank(),
panel.grid.major = element_blank(),
panel.grid.minor = element_blank()) +
geom_sf(data = incendio_estudio, fill="transparent",col="red") +
labs(color = "Probabilidad\nestimada de\nincendio") +
coord_sf(xlim=c(bbox$xmin,bbox$xmax),ylim=c(bbox$ymin,bbox$ymax))
g
g <- ggplot(data = and) +
geom_sf() +
geom_sf(data = pred, aes(color=.pred_class, alpha = .pred_1),size = 1.5) +
facet_wrap(~date) +
# scale_color_gradientn(colours = rainbow(5,rev=T),limits=c(0,1)) +
# scale_color_gradient(low="blue", high="red")+
labs(title="Incendio de Sierra Bermeja",
subtitle = paste0("Model: ",model$model_name)) +
guides(alpha = "none") +
theme_minimal() +
theme(axis.text.x=element_blank(),
axis.ticks.x=element_blank(),
axis.text.y=element_blank(),
axis.ticks.y=element_blank(),
panel.grid.major = element_blank(),
panel.grid.minor = element_blank()) +
geom_sf(data = incendio_estudio, fill="transparent",col="red") +
labs(color = "Probabilidad\nestimada de\nincendio") +
coord_sf(xlim=c(bbox$xmin,bbox$xmax),ylim=c(bbox$ymin,bbox$ymax))
g
library(tidyverse)
library(tidymodels)
library(sf)
library(ggplot2)
library(akima) # interp
library(magrittr)
library(ggpubr)
library(forcats)
?mlp
# 1º Construir el modelo
mlp <-
mlp(hidden_units = c(30,15,5))
# 2º Construir la receta con el preprocesamiento
mlp_recipe <-
recipe(fire ~ ., data = training) %>%
step_date(date,features = c("dow", "month")) %>%
step_rm(date, cod_municipio, municipio) %>%
step_dummy(all_nominal_predictors(),-output_column()) %>%
step_lincomb() %>% # Elimina variablies con dependencia lineal exacta
step_corr() %>% # Elimina variables con correlación superior a 0.9
step_zv(all_predictors()) %>%
step_normalize(all_predictors())
# 2º Construir la receta con el preprocesamiento
mlp_recipe <-
recipe(fire ~ ., data = training) %>%
step_date(date,features = c("dow", "month")) %>%
step_rm(date, cod_municipio, municipio) %>%
step_dummy(all_nominal_predictors(),-output_column()) %>%
step_lincomb() %>% # Elimina variablies con dependencia lineal exacta
step_corr() %>% # Elimina variables con correlación superior a 0.9
step_zv(all_predictors()) %>%
step_normalize(all_predictors())
# 2º Construir la receta con el preprocesamiento
mlp_recipe <-
recipe(fire ~ ., data = training) %>%
step_date(date,features = c("dow", "month")) %>%
step_rm(date, cod_municipio, municipio) %>%
step_dummy(all_nominal_predictors(),-output_column()) %>%
step_lincomb() %>% # Elimina variablies con dependencia lineal exacta
step_corr() %>% # Elimina variables con correlación superior a 0.9
step_zv(all_predictors()) %>%
step_normalize(all_predictors())
# 2º Construir la receta con el preprocesamiento
mlp_recipe <-
recipe(fire ~ ., data = training) %>%
step_date(date,features = c("dow", "month")) %>%
step_rm(date, cod_municipio, municipio) %>%
step_dummy(all_nominal_predictors()) %>%
step_lincomb() %>% # Elimina variablies con dependencia lineal exacta
step_corr() %>% # Elimina variables con correlación superior a 0.9
step_zv(all_predictors()) %>%
step_normalize(all_predictors())
# 1º Construir el modelo
mlp <-
mlp(hidden_units = c(30,15,5))
load("salidas_intermedias/datos_strat_depurados_geom_2024-05-03.RData")
# load("salidas_intermedias/datos_strat_depurados_geom_2024-04-27.RData")
# load("salidas_intermedias/datos_depurados_geom_2024-04-23.RData")
# load("salidas_intermedias/datos_depurados_geom_2024-04-26.RData")
datos <- datos |>
mutate(uso_suelo = fct_lump(uso_suelo,
n = 7, # nos quedamos con los 7 niveles del factor más frecuentes (clases 2 y 3)
other_level= "Otro"))
table(datos$uso_suelo,datos$fire)
set.seed(123)
splits = initial_validation_time_split(datos,
prop=c(0.6,0.2))
training <- training(splits) %>%  st_drop_geometry()
val_set <- validation_set(splits) %>% st_drop_geometry()
test  <- testing(splits) %>% st_drop_geometry()
# 1º Construir el modelo
mlp <-
mlp(hidden_units = c(30,15,5))
set_engine("brulee") %>%
set_mode("classification")
# 1º Construir el modelo
mlp <-
mlp(hidden_units = c(30,15,5))
# 1º Construir el modelo
mlp <-
mlp(hidden_units = c(30,15,5)) %>%
set_engine("brulee") %>%
set_mode("classification")
# 2º Construir la receta con el preprocesamiento
mlp_recipe <-
recipe(fire ~ ., data = training) %>%
step_date(date,features = c("dow", "month")) %>%
step_rm(date, cod_municipio, municipio) %>%
step_dummy(all_nominal_predictors()) %>%
step_lincomb() %>% # Elimina variablies con dependencia lineal exacta
step_corr() %>% # Elimina variables con correlación superior a 0.9
step_zv(all_predictors()) %>%
step_normalize(all_predictors())
# 3º Ensamblar todo con workflow
mlp_workflow <-
workflow() %>%
add_model(mlp) %>%
add_recipe(mlp_recipe)
validation()
training
validation_set()
val_set
mlp_res <-
mlp_workflow %>%
fit(training)
install.packages("brulee")
# 4º Train and tune
set.seed(345)
mlp_res <-
mlp_workflow %>%
fit(training)
library(tidyverse)
library(tidymodels)
library(sf)
library(ggplot2)
library(akima) # interp
library(magrittr)
library(ggpubr)
library(forcats)
load("salidas_intermedias/datos_strat_depurados_geom_2024-05-03.RData")
# load("salidas_intermedias/datos_strat_depurados_geom_2024-04-27.RData")
# load("salidas_intermedias/datos_depurados_geom_2024-04-23.RData")
# load("salidas_intermedias/datos_depurados_geom_2024-04-26.RData")
datos <- datos |>
mutate(uso_suelo = fct_lump(uso_suelo,
n = 7, # nos quedamos con los 7 niveles del factor más frecuentes (clases 2 y 3)
other_level= "Otro"))
table(datos$uso_suelo,datos$fire)
# Chunk 1: setup
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = "D:/usuario/Documents/Universidad/5º/TFG - organizado") # Para que haga knitr desde el directorio del proyecto
# Chunk 2
library(tidyverse)
library(tidymodels)
library(sf)
library(ggplot2)
library(akima) # interp
library(magrittr)
library(ggpubr)
library(forcats)
# Chunk 3
load("salidas_intermedias/datos_strat_depurados_geom_2024-05-03.RData")
# load("salidas_intermedias/datos_strat_depurados_geom_2024-04-27.RData")
# load("salidas_intermedias/datos_depurados_geom_2024-04-23.RData")
# load("salidas_intermedias/datos_depurados_geom_2024-04-26.RData")
datos <- datos |>
mutate(uso_suelo = fct_lump(uso_suelo,
n = 7, # nos quedamos con los 7 niveles del factor más frecuentes (clases 2 y 3)
other_level= "Otro"))
table(datos$uso_suelo,datos$fire)
# Chunk 4
# Función para obtener las medidas de rendimiento de los modelos a partir de un objeto predict
get_metrics <- function(pred) {
list(
res = tibble(
roc_auc = pred |> roc_auc(truth = fire, .pred_0) |> pull(.estimate),
accuracy = pred |> accuracy(truth = fire, .pred_class) |> pull(.estimate),
recall = pred |> sensitivity(truth = fire, .pred_class,event_level="second") |> pull(.estimate),
specificity = pred |> spec(truth = fire, .pred_class,event_level="second") |> pull(.estimate),
precision = pred |> precision(truth = fire, .pred_class,event_level="second") |> pull(.estimate)),
conf_mat = pred |> conf_mat(truth = fire, .pred_class))
}
# Función para mostrar graficamente los resultados del tuning de un modelo con dos parámetros
tuning_plot = function(mod_res) {
datos_metrics = mod_res %>%
collect_metrics()
# min = min(datos_metrics$mean)
# max = max(datos_metrics$mean)
plots = list()
for (metric in unique(datos_metrics$.metric)) {
datos = datos_metrics %>%
filter(.metric==metric)
# Interpolar los datos faltantes
datos_interp <- interp(datos[[1]], datos[[2]], datos$mean)
# Crear un nuevo dataframe con los datos interpolados
datos_interp_df <- data.frame(
expand.grid(x = datos_interp$x, y = datos_interp$y), z = as.vector(datos_interp$z))
# Crear el gráfico de mapa de calor con interpolación
p = ggplot(datos_interp_df, aes(x = x, y = y, fill = z)) +
geom_tile() +
# scale_fill_viridis_c(option = "turbo", limits = c(min,max), name = NULL)+
scale_fill_viridis_c(option = "turbo", name = NULL,na.value = "transparent")+
labs(title = "",
x = colnames(datos)[1],
y = colnames(datos)[2],
fill = metric) +
theme_minimal()
plots[[metric]] = p
}
# ggarrange(plotlist = plots,
#           legend = "right",
#           common.legend = T)
ggarrange(plotlist = plots,
labels=c("Accuracy","Specificy","ROC-AUC","Recall"),
align = "hv")
}
set.seed(123)
splits = initial_validation_time_split(datos,
prop=c(0.6,0.2))
training <- training(splits) %>%  st_drop_geometry()
val_set <- validation_set(splits) %>% st_drop_geometry()
test  <- testing(splits) %>% st_drop_geometry()
# 1º Construir el modelo
mlp <-
mlp(hidden_units = c(30,15,5)) %>%
set_engine("brulee") %>%
set_mode("classification")
# 2º Construir la receta con el preprocesamiento
mlp_recipe <-
recipe(fire ~ ., data = training) %>%
step_date(date,features = c("dow", "month")) %>%
step_rm(date, cod_municipio, municipio) %>%
step_dummy(all_nominal_predictors()) %>%
step_lincomb() %>% # Elimina variablies con dependencia lineal exacta
step_corr() %>% # Elimina variables con correlación superior a 0.9
step_zv(all_predictors()) %>%
step_normalize(all_predictors())
# 3º Ensamblar todo con workflow
mlp_workflow <-
workflow() %>%
add_model(mlp) %>%
add_recipe(mlp_recipe)
# 4º Train and tune
set.seed(345)
mlp_res <-
mlp_workflow %>%
fit(training)
install.packages("latern")
install.packages("lantern")
# Chunk 1: setup
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = "D:/usuario/Documents/Universidad/5º/TFG - organizado") # Para que haga knitr desde el directorio del proyecto
# Chunk 2
library(tidyverse)
library(tidymodels)
library(sf)
library(ggplot2)
library(akima) # interp
library(magrittr)
library(ggpubr)
library(forcats)
load("salidas_intermedias/datos_strat_depurados_geom_2024-05-03.RData")
# load("salidas_intermedias/datos_strat_depurados_geom_2024-04-27.RData")
# load("salidas_intermedias/datos_depurados_geom_2024-04-23.RData")
# load("salidas_intermedias/datos_depurados_geom_2024-04-26.RData")
datos <- datos |>
mutate(uso_suelo = fct_lump(uso_suelo,
n = 7, # nos quedamos con los 7 niveles del factor más frecuentes (clases 2 y 3)
other_level= "Otro"))
table(datos$uso_suelo,datos$fire)
# Función para obtener las medidas de rendimiento de los modelos a partir de un objeto predict
get_metrics <- function(pred) {
list(
res = tibble(
roc_auc = pred |> roc_auc(truth = fire, .pred_0) |> pull(.estimate),
accuracy = pred |> accuracy(truth = fire, .pred_class) |> pull(.estimate),
recall = pred |> sensitivity(truth = fire, .pred_class,event_level="second") |> pull(.estimate),
specificity = pred |> spec(truth = fire, .pred_class,event_level="second") |> pull(.estimate),
precision = pred |> precision(truth = fire, .pred_class,event_level="second") |> pull(.estimate)),
conf_mat = pred |> conf_mat(truth = fire, .pred_class))
}
# Función para mostrar graficamente los resultados del tuning de un modelo con dos parámetros
tuning_plot = function(mod_res) {
datos_metrics = mod_res %>%
collect_metrics()
# min = min(datos_metrics$mean)
# max = max(datos_metrics$mean)
plots = list()
for (metric in unique(datos_metrics$.metric)) {
datos = datos_metrics %>%
filter(.metric==metric)
# Interpolar los datos faltantes
datos_interp <- interp(datos[[1]], datos[[2]], datos$mean)
# Crear un nuevo dataframe con los datos interpolados
datos_interp_df <- data.frame(
expand.grid(x = datos_interp$x, y = datos_interp$y), z = as.vector(datos_interp$z))
# Crear el gráfico de mapa de calor con interpolación
p = ggplot(datos_interp_df, aes(x = x, y = y, fill = z)) +
geom_tile() +
# scale_fill_viridis_c(option = "turbo", limits = c(min,max), name = NULL)+
scale_fill_viridis_c(option = "turbo", name = NULL,na.value = "transparent")+
labs(title = "",
x = colnames(datos)[1],
y = colnames(datos)[2],
fill = metric) +
theme_minimal()
plots[[metric]] = p
}
# ggarrange(plotlist = plots,
#           legend = "right",
#           common.legend = T)
ggarrange(plotlist = plots,
labels=c("Accuracy","Specificy","ROC-AUC","Recall"),
align = "hv")
}
set.seed(123)
splits = initial_validation_time_split(datos,
prop=c(0.6,0.2))
training <- training(splits) %>%  st_drop_geometry()
val_set <- validation_set(splits) %>% st_drop_geometry()
test  <- testing(splits) %>% st_drop_geometry()
# 1º Construir el modelo
mlp <-
mlp(hidden_units = c(30,15,5)) %>%
set_engine("brulee") %>%
set_mode("classification")
# 2º Construir la receta con el preprocesamiento
mlp_recipe <-
recipe(fire ~ ., data = training) %>%
step_date(date,features = c("dow", "month")) %>%
step_rm(date, cod_municipio, municipio) %>%
step_dummy(all_nominal_predictors()) %>%
step_lincomb() %>% # Elimina variablies con dependencia lineal exacta
step_corr() %>% # Elimina variables con correlación superior a 0.9
step_zv(all_predictors()) %>%
step_normalize(all_predictors())
# 3º Ensamblar todo con workflow
mlp_workflow <-
workflow() %>%
add_model(mlp) %>%
add_recipe(mlp_recipe)
# 4º Train and tune
set.seed(345)
mlp_res <-
mlp_workflow %>%
fit(training)
load("salidas_intermedias/dataset_strat_completo2024-04-26.RData")
# Chunk 1: setup
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = "D:/usuario/Documents/Universidad/5º/TFG - organizado") # Para que haga knitr desde el directorio del proyecto
# Librerías:
library(tidyverse) # Manipulación de datos
library(sf) # Vector data
library(terra) # Raster data
library(mapSpain) # Polígonos de regiones de España
library(magrittr) # Operador %<>%
datos %>% skimr()
datos %>% skim()
library(skimr)
datos %>% skim()
dataset %>% skim()
nas = dataset %>% apply(1,anyNA) %>% sum()
nas # 200 registoros incompletos
nas/nrow(dataset) # aproximadamente un 0.1% de los registros
load("salidas_intermedias/datos_strat_depurados_geom_2024-05-03.RData")
datos_numeric = datos |> select(where(is.numeric)) %>% st_drop_geometry()
set.seed(12345)
# Probamos tipificando los datos:
clara = datos_numeric |>
scale() |>
clara(k=2)
library(cluster)
# Probamos tipificando los datos:
clara = datos_numeric |>
scale() |>
clara(k=2)
table(clara$clustering)
t = table(clara$clustering,datos$fire)
t = rbind(t[2,],t[1,])
rownames(t) <- paste0("cluster_",c(1,2))
colnames(t) <- c("No fire","Fire")
t
sum(diag(t))/nrow(datos) # Proporción de observaciones "correctamente separada"
