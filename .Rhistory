set_engine("glm")
lr_recipe <-
recipe(fire ~ ., data = training) %>%
step_pca(all_numeric_predictors(),num_comp = 15) %>%
step_date(date,features = c("dow","month")) %>%
# step_holiday(date, holidays = holidays) %>%
step_rm(date,cod_municipio,municipio) %>%
step_dummy(all_nominal_predictors()) %>%
step_zv(all_predictors()) %>%
step_normalize(all_predictors())
# 4º Creamos el workflow
lr_workflow <-
workflow() %>%
add_model(lr_mod) %>%
add_recipe(lr_recipe)
# 5º Ajustamos el modelo.
lr_res <-
lr_workflow %>%
fit(training)
lr_res %>% extract_fit_engine() %>% summary()
lr_mod <-
logistic_reg(penalty = NULL, mixture = NULL) %>%
set_engine("glm")
lr_recipe <-
recipe(fire ~ ., data = training) %>%
step_pca(all_numeric_predictors(),num_comp = 15) %>%
step_date(date,features = c("dow","month")) %>%
# step_holiday(date, holidays = holidays) %>%
step_rm(date,cod_municipio,municipio) %>%
step_dummy(all_nominal_predictors()) %>%
step_zv(all_predictors()) %>%
step_normalize(all_predictors())
# 4º Creamos el workflow
lr_workflow <-
workflow() %>%
add_model(lr_mod) %>%
add_recipe(lr_recipe)
# 5º Ajustamos el modelo.
lr_res <-
lr_workflow %>%
fit(training)
lr_res %>% extract_fit_engine() %>% summary()
pred = cbind(predict(lr_res,new_data = validation(splits),type="prob"),
predict(lr_res,new_data = validation(splits),type="class"),
fire = validation(splits)$fire)
pred
# 6º Se evalúa el modelo:
#   Curva ROC
lr_auc <- pred %>%
roc_curve(fire, .pred_0) %>%
mutate(model = "Logistic Regression")
autoplot(lr_auc)
#   Medidas de rendimiento
lr_metrics <- pred |>
get_metrics()
lr_metrics
lr_pca_mod <-
logistic_reg(penalty = NULL, mixture = NULL) %>%
set_engine("glm")
lr_recipe <-
recipe(fire ~ ., data = training) %>%
step_pca(all_numeric_predictors(),num_comp = 15) %>%
step_date(date,features = c("dow","month")) %>%
# step_holiday(date, holidays = holidays) %>%
step_rm(date,cod_municipio,municipio) %>%
step_dummy(all_nominal_predictors()) %>%
step_zv(all_predictors()) %>%
step_normalize(all_predictors())
# 4º Creamos el workflow
lr_workflow <-
workflow() %>%
add_model(lr_mod) %>%
add_recipe(lr_recipe)
# Chunk 1: setup
knitr::opts_chunk$set(echo = TRUE)
# Chunk 2
library(tidyverse)
library(tidymodels)
library(sf)
library(ggplot2)
library(akima) # interp
library(magrittr)
library(ggpubr)
# Chunk 3
load("salidas_intermedias/datos_strat_depurados_geom_2024-04-27.RData")
# load("salidas_intermedias/datos_depurados_geom_2024-04-23.RData")
# load("salidas_intermedias/datos_depurados_geom_2024-04-26.RData")
# Chunk 4
# Función para obtener las medidas de rendimiento de los modelos a partir de un objeto predict
get_metrics <- function(pred) {
list(
res = tibble(
roc_auc = pred |> roc_auc(truth = fire, .pred_0) |> pull(.estimate),
accuracy = pred |> accuracy(truth = fire, .pred_class) |> pull(.estimate),
recall = pred |> sensitivity(truth = fire, .pred_class) |> pull(.estimate),
specificity = pred |> spec(truth = fire, .pred_class) |> pull(.estimate)),
conf_mat = pred |> conf_mat(truth = fire, .pred_class))
}
# Función para mostrar graficamente los resultados del tuning de un modelo con dos parámetros
tuning_plot = function(mod_res) {
datos_metrics = mod_res %>%
collect_metrics()
# min = min(datos_metrics$mean)
# max = max(datos_metrics$mean)
plots = list()
for (metric in unique(datos_metrics$.metric)) {
datos = datos_metrics %>%
filter(.metric==metric)
# Interpolar los datos faltantes
datos_interp <- interp(datos[[1]], datos[[2]], datos$mean)
# Crear un nuevo dataframe con los datos interpolados
datos_interp_df <- data.frame(
expand.grid(x = datos_interp$x, y = datos_interp$y), z = as.vector(datos_interp$z))
# Crear el gráfico de mapa de calor con interpolación
p = ggplot(datos_interp_df, aes(x = x, y = y, fill = z)) +
geom_tile() +
# scale_fill_viridis_c(option = "turbo", limits = c(min,max), name = NULL)+
scale_fill_viridis_c(option = "turbo", name = NULL)+
labs(title = metric,
x = colnames(datos)[1],
y = colnames(datos)[2],
fill = metric)
plots[[metric]] = p
}
# ggarrange(plotlist = plots,
#           legend = "right",
#           common.legend = T)
ggarrange(plotlist = plots)
}
# Chunk 5
set.seed(123)
splits = initial_validation_time_split(datos,
prop=c(0.6,0.2))
training <- training(splits) %>%  st_drop_geometry()
val_set <- validation_set(splits) %>% st_drop_geometry()
test  <- testing(splits) %>% st_drop_geometry()
# # Para hacerlo por fecha manualmente:
# splits <- make_splits(
#   x = list(analysis = which(year(datos$date)<2021),
#            assessment = which(year(datos$date)>=2021)),
#   data=datos
# )
#
# training_val <- training(splits)
# test  <- testing(splits)
#
#
# length(splits$out_id)/length(splits$in_id)
# [1] 0.1149578
#
# 1.a. Partición train- validation
# set.seed(234)
# val <- make_splits(
#   x = list(analysis = which(year(datos$date)<2018),
#            assessment = which(year(datos$date)>=2018)),
#   data=datos
# )
#
# val_set <- validation_time_split(training_val,
#                                  prop = 0.80)
# Chunk 6
# 1º Definimos el modelo:
lr_mod <-
logistic_reg(penalty = tune(), mixture = tune()) %>%
set_engine("glmnet")
# 2º Creamos la receta
lr_recipe <-
recipe(fire ~ ., data = training) %>%
step_date(date,features = c("dow","month")) %>%
# step_holiday(date, holidays = holidays) %>%
step_rm(date,cod_municipio,municipio) %>% # Se eliminan variables identificadoras
step_dummy(all_nominal_predictors()) %>% # Se crean variables dummy para los factores
step_zv(all_predictors()) %>% # Eliminar variables con varianza nula
step_normalize(all_predictors()) # Se normalizan todos los predictores
# 3º Creamos el workflow
lr_workflow <-
workflow() %>%
add_model(lr_mod) %>%
add_recipe(lr_recipe)
# 4º Creamos el grid para los parámetros
lr_reg_grid <- expand_grid(penalty = 10^seq(-4, -1, length.out = 30),
mixture = c(0,1))
# 5º Ajustamos el modelo
lr_res <-
lr_workflow %>%
tune_grid(val_set,
grid = lr_reg_grid,
control = control_grid(save_pred = TRUE),
metrics = metric_set(accuracy,roc_auc,recall,spec))
# 6º Evaluación de modelos
# Se muestran las medidas de rendimiento en función del parámetro de penalización
lr_plot <-
lr_res %>%
collect_metrics() %>%
# filter(.metric == "accuracy") %>%
ggplot(aes(x = penalty, y = mean,col=.metric,linetype=as.factor(mixture))) +
geom_point() +
geom_line() +
ylab("Medidas de rendimiento") +
scale_x_log10(labels = scales::label_number())
lr_plot
lr_res |>
collect_metrics() |>
group_by(.metric)|>
summarise(max = max(mean),min=min(mean))
lr_res %>%
show_best(metric = "accuracy", n = 15)
lr_res %>% tuning_plot()
# 7º Selección del mejor modelo
lr_best <-
lr_res %>%
select_best(metric="accuracy")
lr_best
# 8º Se evalúa el modelo:
# Curva ROC
lr_auc <-
lr_res %>%
collect_predictions(parameters = lr_best) %>%
roc_curve(fire, .pred_0) %>%
mutate(model = "Logistic Regression")
autoplot(lr_auc)
#   Medidas de rendimiento
lr_metrics <- lr_res |>
collect_predictions(parameters = lr_best) |>
get_metrics()
lr_metrics
# Chunk 7
# 1º definimos el modelo
lrs_mod <-
logistic_reg(penalty = NULL, mixture = NULL) %>%
set_engine("glm")
# 2º Creamos la receta
lrs_recipe <-
recipe(fire ~ ., data = training) %>%
# step_date(date,features = c("dow","month")) %>%
# step_holiday(date, holidays = holidays) %>%
step_rm(date,cod_municipio,municipio) %>%
step_dummy(all_nominal_predictors()) %>%
step_zv(all_predictors()) %>%
step_normalize(all_predictors())
# 3º Creamos el workflow
lrs_workflow <-
workflow() %>%
add_model(lrs_mod) %>%
add_recipe(lrs_recipe)
# 4º Ajustamos el modelo.
lrs_res <-
lrs_workflow %>%
fit(training)
# 5º
lrs_res %>% extract_fit_engine() %>% summary()
lrs_pred = cbind(predict(lrs_res,new_data = validation(splits),type="prob"),
predict(lrs_res,new_data = validation(splits),type="class"),
fire = validation(splits)$fire)
# 6º Se evalúa el modelo:
#   Curva ROC
lrs_auc <- lrs_pred %>%
roc_curve(fire, .pred_0) %>%
mutate(model = "Logistic Regression")
autoplot(lrs_auc)
#   Medidas de rendimiento
lrs_metrics <- pred |>
get_metrics()
lrs_metrics
# Chunk 8
## No tengo claro como medir los residuos en un modelo de clasificación:
lr_residuals = lr_res %>%
collect_predictions(parameters = lr_best) %>%
mutate(residual = fire !=.pred_class,
geometry = st_geometry(validation(splits))) %>%
st_as_sf(sf_column_name ="geometry")
ggplot(lr_residuals,aes(col=residual)) +
geom_sf(alpha=0.6) +
labs(col = "Error")
# Chunk 9
# Cómo obtengo los p_valores de los contrastes individuales?
lr_workflow %>%
finalize_workflow(lr_best) %>%
fit(training) %>%
extract_fit_parsnip() %>%
tidy()
lr_workflow %>%
finalize_workflow(lr_best) %>%
fit(training) %>%
extract_fit_engine() %>%
tidy()
lr_pca_mod <-
logistic_reg(penalty = NULL, mixture = NULL) %>%
set_engine("glm")
lr_pca_recipe <-
recipe(fire ~ ., data = training) %>%
step_pca(all_numeric_predictors(),num_comp = 14) %>%
step_date(date,features = c("dow","month")) %>%
# step_holiday(date, holidays = holidays) %>%
step_rm(date,cod_municipio,municipio) %>%
step_dummy(all_nominal_predictors()) %>%
step_zv(all_predictors()) %>%
step_normalize(all_predictors())
# 4º Creamos el workflow
lr_pca_workflow <-
workflow() %>%
add_model(lr_pca_mod) %>%
add_recipe(lr_pca_recipe)
# 5º Ajustamos el modelo.
lr_pca_res <-
lr_pca_workflow %>%
fit(training)
lr_pca_res %>% extract_fit_engine() %>% summary()
pred = cbind(predict(lr_pca_res,new_data = validation(splits),type="prob"),
predict(lr_pca_res,new_data = validation(splits),type="class"),
fire = validation(splits)$fire)
# 6º Se evalúa el modelo:
#   Curva ROC
lr_pca_auc <- pred %>%
roc_curve(fire, .pred_0) %>%
mutate(model = "Logistic Regression")
autoplot(lr_pca_auc)
#   Medidas de rendimiento
lr_pca_metrics <- pred |>
get_metrics()
lr_pca_metrics
lr_metrics
lrs_metrics
# Detectar el número de núcleos para trabajar en paralelo
cores <- parallel::detectCores()
# Detectar el número de núcleos para trabajar en paralelo
cores <- parallel::detectCores()
cores
# 1º Construir el modelo
rf_mod <-
rand_forest(mtry = tune(), min_n = tune(), trees = 1000) %>%
set_engine("ranger", num.threads = cores) %>%
set_mode("classification")
# 2º Construir la receta con el preprocesamiento
rf_recipe <-
recipe(fire ~ ., data = training) %>%
step_date(date,features = c("dow", "month")) %>%
# step_holiday(date) %>%
step_rm(date, cod_municipio, municipio)
# 3º Ensamblar todo con workflow
rf_workflow <-
workflow() %>%
add_model(rf_mod) %>%
add_recipe(rf_recipe)
# Detectar el número de núcleos para trabajar en paralelo
cores <- parallel::detectCores()
cores
# 1º Construir el modelo
rf_mod <-
rand_forest(mtry = tune(), min_n = tune(), trees = 1000) %>%
set_engine("ranger", num.threads = cores) %>%
set_mode("classification")
# 2º Construir la receta con el preprocesamiento
rf_recipe <-
recipe(fire ~ ., data = training) %>%
step_date(date,features = c("dow", "month")) %>%
# step_holiday(date) %>%
step_rm(date, cod_municipio, municipio)
# 2º Construir la receta con el preprocesamiento
rf_recipe <-
recipe(fire ~ ., data = training) %>%
step_date(date,features = c("dow", "month")) %>%
# step_holiday(date) %>%
step_rm(date, cod_municipio, municipio)
# 2º Construir la receta con el preprocesamiento
rf_recipe <-
recipe(fire ~ ., data = training) %>%
step_date(date,features = c("dow", "month")) #%>%
load("salidas_intermedias/datos_strat_depurados_geom_2024-04-27.RData")
# Detectar el número de núcleos para trabajar en paralelo
cores <- parallel::detectCores()
cores
# 1º Construir el modelo
rf_mod <-
rand_forest(mtry = tune(), min_n = tune(), trees = 1000) %>%
set_engine("ranger", num.threads = cores) %>%
set_mode("classification")
# 2º Construir la receta con el preprocesamiento
rf_recipe <-
recipe(fire ~ ., data = training) %>%
step_date(date,features = c("dow", "month")) %>%
# step_holiday(date) %>%
step_rm(date, cod_municipio, municipio)
set.seed(123)
splits = initial_validation_time_split(datos,
prop=c(0.6,0.2))
training <- training(splits) %>%  st_drop_geometry()
val_set <- validation_set(splits) %>% st_drop_geometry()
test  <- testing(splits) %>% st_drop_geometry()
# # Para hacerlo por fecha manualmente:
# splits <- make_splits(
#   x = list(analysis = which(year(datos$date)<2021),
#            assessment = which(year(datos$date)>=2021)),
#   data=datos
# )
#
# training_val <- training(splits)
# test  <- testing(splits)
#
#
# length(splits$out_id)/length(splits$in_id)
# [1] 0.1149578
#
# 1.a. Partición train- validation
# set.seed(234)
# val <- make_splits(
#   x = list(analysis = which(year(datos$date)<2018),
#            assessment = which(year(datos$date)>=2018)),
#   data=datos
# )
#
# val_set <- validation_time_split(training_val,
#                                  prop = 0.80)
# Detectar el número de núcleos para trabajar en paralelo
cores <- parallel::detectCores()
cores
# 1º Construir el modelo
rf_mod <-
rand_forest(mtry = tune(), min_n = tune(), trees = 1000) %>%
set_engine("ranger", num.threads = cores) %>%
set_mode("classification")
# 2º Construir la receta con el preprocesamiento
rf_recipe <-
recipe(fire ~ ., data = training) %>%
step_date(date,features = c("dow", "month")) %>%
# step_holiday(date) %>%
step_rm(date, cod_municipio, municipio)
# 3º Ensamblar todo con workflow
rf_workflow <-
workflow() %>%
add_model(rf_mod) %>%
add_recipe(rf_recipe)
# 4º Train and tune
rf_mod
extract_parameter_set_dials(rf_mod)
set.seed(345)
rf_res <-
rf_workflow %>%
tune_grid(val_set,
grid = 25,
control = control_grid(save_pred = TRUE),
metrics = metric_set(accuracy,roc_auc,recall,spec))
rf_res |>
collect_metrics() |>
group_by(.metric)|>
summarise(max = max(mean),min=min(mean))
tuning_plot(rf_res)
tuning_plot = function(mod_res) {
datos_metrics = mod_res %>%
collect_metrics()
# min = min(datos_metrics$mean)
# max = max(datos_metrics$mean)
plots = list()
for (metric in unique(datos_metrics$.metric)) {
datos = datos_metrics %>%
filter(.metric==metric)
# Interpolar los datos faltantes
datos_interp <- interp(datos[[1]], datos[[2]], datos$mean)
# Crear un nuevo dataframe con los datos interpolados
datos_interp_df <- data.frame(
expand.grid(x = datos_interp$x, y = datos_interp$y), z = as.vector(datos_interp$z))
# Crear el gráfico de mapa de calor con interpolación
p = ggplot(datos_interp_df, aes(x = x, y = y, fill = z)) +
geom_tile() +
# scale_fill_viridis_c(option = "turbo", limits = c(min,max), name = NULL)+
scale_fill_viridis_c(option = "turbo", name = NULL)+
labs(title = metric,
x = colnames(datos)[1],
y = colnames(datos)[2],
fill = metric)
plots[[metric]] = p
}
# ggarrange(plotlist = plots,
#           legend = "right",
#           common.legend = T)
ggarrange(plotlist = plots)
}
tuning_plot(rf_res)
# Mejor modelo
rf_best <-
rf_res %>%
select_best(metric = "accuracy")
rf_best
# Curva ROC
rf_auc <-
rf_res %>%
collect_predictions(parameters = rf_best) %>%
roc_curve(fire, .pred_0) %>%
mutate(model = "Random Forest")
autoplot(rf_auc)
rf_metrics <- rf_res |>
collect_predictions(parameters = rf_best) |>
get_metrics()
get_metrics <- function(pred) {
list(
res = tibble(
roc_auc = pred |> roc_auc(truth = fire, .pred_0) |> pull(.estimate),
accuracy = pred |> accuracy(truth = fire, .pred_class) |> pull(.estimate),
recall = pred |> sensitivity(truth = fire, .pred_class) |> pull(.estimate),
specificity = pred |> spec(truth = fire, .pred_class) |> pull(.estimate)),
conf_mat = pred |> conf_mat(truth = fire, .pred_class))
}
rf_metrics <- rf_res |>
collect_predictions(parameters = rf_best) |>
get_metrics()
rf_metrics
lr_metrics
rf_metrics$conf_mat
save(rf_res,"/salidas_intermedias/trained_models/trained_models_strat_24_04_27.RData")
save(rf_res,file = "/salidas_intermedias/trained_models/trained_models_strat_24_04_27.RData")
?save
save(rf_res,file = "salidas_intermedias/trained_models/trained_models_strat_24_04_27.RData")
