# )
#
# val_set <- validation_time_split(training_val,
#                                  prop = 0.80)
# Chunk 6
# 1º Definimos el modelo:
lr_mod <-
logistic_reg(penalty = tune(), mixture = tune()) %>%
set_engine("glmnet")
# 2º Creamos la receta
lr_recipe <-
recipe(fire ~ ., data = training) %>%
step_date(date,features = c("dow","month")) %>%
# step_holiday(date, holidays = holidays) %>%
step_rm(date,cod_municipio,municipio) %>% # Se eliminan variables identificadoras
step_dummy(all_nominal_predictors()) %>% # Se crean variables dummy para los factores
step_zv(all_predictors()) %>% # Eliminar variables con varianza nula
step_normalize(all_predictors()) # Se normalizan todos los predictores
# 3º Creamos el workflow
lr_workflow <-
workflow() %>%
add_model(lr_mod) %>%
add_recipe(lr_recipe)
# 4º Creamos el grid para los parámetros
lr_reg_grid <- expand_grid(penalty = 10^seq(-4, -1, length.out = 30),
mixture = c(0,1))
# 5º Ajustamos el modelo
lr_res <-
lr_workflow %>%
tune_grid(val_set,
grid = lr_reg_grid,
control = control_grid(save_pred = TRUE),
metrics = metric_set(accuracy,roc_auc,recall,spec))
# 6º Evaluación de modelos
# Se muestran las medidas de rendimiento en función del parámetro de penalización
lr_plot <-
lr_res %>%
collect_metrics() %>%
# filter(.metric == "accuracy") %>%
ggplot(aes(x = penalty, y = mean,col=.metric,linetype=as.factor(mixture))) +
geom_point() +
geom_line() +
ylab("Medidas de rendimiento") +
scale_x_log10(labels = scales::label_number())
lr_plot
lr_res |>
collect_metrics() |>
group_by(.metric)|>
summarise(max = max(mean),min=min(mean))
lr_res %>%
show_best(metric = "accuracy", n = 15)
lr_res %>% tuning_plot()
# 7º Selección del mejor modelo
lr_best <-
lr_res %>%
select_best(metric="accuracy")
lr_best
# 8º Se evalúa el modelo:
# Curva ROC
lr_auc <-
lr_res %>%
collect_predictions(parameters = lr_best) %>%
roc_curve(fire, .pred_0) %>%
mutate(model = "Logistic Regression")
autoplot(lr_auc)
#   Medidas de rendimiento
lr_metrics <- lr_res |>
collect_predictions(parameters = lr_best) |>
get_metrics()
lr_metrics
# Chunk 7
## No tengo claro como medir los residuos en un modelo de clasificación:
lr_residuals = lr_res %>%
collect_predictions(parameters = lr_best) %>%
mutate(residual = fire !=.pred_class,
geometry = st_geometry(validation(splits))) %>%
st_as_sf(sf_column_name ="geometry")
ggplot(lr_residuals,aes(col=residual)) +
geom_sf(alpha=0.6) +
labs(col = "Error")
# Chunk 8
# Cómo obtengo los p_valores de los contrastes individuales?
lr_workflow %>%
finalize_workflow(lr_best) %>%
fit(training) %>%
extract_fit_parsnip() %>%
tidy()
lr_workflow %>%
finalize_workflow(lr_best) %>%
fit(training) %>%
extract_fit_engine() %>%
tidy()
# Chunk 9
# 1º definimos el modelo
# lrs: logistic regression simple
lrs_mod <-
logistic_reg(penalty = NULL, mixture = NULL) %>%
set_engine("glm")
# 2º Creamos la receta
lrs_recipe <-
recipe(fire ~ ., data = training) %>%
step_date(date,features = c("dow","month")) %>%
# step_holiday(date, holidays = holidays) %>%
step_rm(date,cod_municipio,municipio) %>%
step_dummy(all_nominal_predictors()) %>%
step_zv(all_predictors()) %>%
step_normalize(all_predictors())
# 3º Creamos el workflow
lrs_workflow <-
workflow() %>%
add_model(lrs_mod) %>%
add_recipe(lrs_recipe)
# 4º Ajustamos el modelo.
lrs_res <-
lrs_workflow %>%
fit(training)
# 5º Se evalúa su rendimiento en los datos de validación.
lrs_pred = cbind(predict(lrs_res,new_data = validation(splits),type="prob"),
predict(lrs_res,new_data = validation(splits),type="class"),
fire = validation(splits)$fire)
# 6º Se evalúa el modelo:
#   Curva ROC
lrs_auc <- lrs_pred %>%
roc_curve(fire, .pred_0) %>%
mutate(model = "Logistic Regression")
autoplot(lrs_auc)
#   Medidas de rendimiento
lrs_metrics <- lrs_pred |>
get_metrics()
lrs_metrics
# Chunk 10
lrs_res %>% extract_fit_engine() %>% summary()
# Chunk 11
# 1º definimos el modelo
# lrs2: logistic regression simple
lrs2_mod <-
logistic_reg(penalty = NULL, mixture = NULL) %>%
set_engine("glm")
# 2º Creamos la receta
lrs2_recipe <-
recipe(fire ~ ., data = training) %>%
step_date(date,features = c("month")) %>%
# step_holiday(date, holidays = holidays) %>%
step_rm(date,cod_municipio,municipio, GWETTOP, WD10M, orientacion) %>%
step_dummy(all_nominal_predictors()) %>%
step_zv(all_predictors()) %>%
step_normalize(all_predictors())
# 3º Creamos el workflow
lrs2_workflow <-
workflow() %>%
add_model(lrs2_mod) %>%
add_recipe(lrs2_recipe)
# 4º Ajustamos el modelo.
lrs2_res <-
lrs2_workflow %>%
fit(training)
# 5º Se evalúa su rendimiento en los datos de validación.
lrs2_pred = cbind(predict(lrs2_res,new_data = validation(splits),type="prob"),
predict(lrs2_res,new_data = validation(splits),type="class"),
fire = validation(splits)$fire)
# 6º Se evalúa el modelo:
#   Curva ROC
lrs2_auc <- lrs2_pred %>%
roc_curve(fire, .pred_0) %>%
mutate(model = "Logistic Regression")
autoplot(lrs2_auc)
#   Medidas de rendimiento
lrs2_metrics <- lrs2_pred |>
get_metrics()
# Chunk 12
lr_pca_mod <-
logistic_reg(penalty = NULL, mixture = NULL) %>%
set_engine("glm")
# 3º Creamos la receta
lr_pca_recipe <-
recipe(fire ~ ., data = training) %>%
step_pca(all_numeric_predictors(),num_comp = 14) %>%
step_date(date,features = c("dow","month")) %>%
# step_holiday(date, holidays = holidays) %>%
step_rm(date,cod_municipio,municipio) %>%
step_dummy(all_nominal_predictors()) %>%
step_zv(all_predictors()) %>%
step_normalize(all_predictors())
# 4º Creamos el workflow
lr_pca_workflow <-
workflow() %>%
add_model(lr_pca_mod) %>%
add_recipe(lr_pca_recipe)
# 5º Ajustamos el modelo.
lr_pca_res <-
lr_pca_workflow %>%
fit(training)
lr_pca_res %>% extract_fit_engine() %>% summary()
lr_pca_pred = cbind(predict(lr_pca_res,new_data = validation(splits),type="prob"),
predict(lr_pca_res,new_data = validation(splits),type="class"),
fire = validation(splits)$fire)
# 6º Se evalúa el modelo:
#   Curva ROC
lr_pca_auc <- lr_pca_pred %>%
roc_curve(fire, .pred_0) %>%
mutate(model = "Logistic Regression")
autoplot(lr_pca_auc)
#   Medidas de rendimiento
lr_pca_metrics <- lr_pca_pred |>
get_metrics()
lr_pca_metrics
# Chunk 13
lr_metrics
lrs_metrics
lrs2_metrics
lr_pca_metrics
source("D:/usuario/Documents/Universidad/5º/TFG - organizado/salidas_intermedias/trained_models/trained_models_strat_24_04_27.RData")
source(salidas_intermedias/trained_models/trained_models_strat_24_04_27.RData")
ws
source("salidas_intermedias/trained_models/trained_models_strat_24_04_27.RData")
load("salidas_intermedias/trained_models/trained_models_strat_24_04_27.RData")
load("salidas_intermedias/trained_models/svm_res_strat.RData")
results = rbind(
lr = lr_res %>%
collect_predictions(parameters = lr_best) %>%
get_metrics() %>%
.$res %>%
as.data.frame(),
rf = rf_res %>%
collect_predictions(parameters = rf_best) %>%
get_metrics() %>%
.$res %>%
as.data.frame(),
svm = svm_res %>%
collect_predictions(parameters = svm_best) %>%
get_metrics() %>%
.$res %>%
as.data.frame()
)
# Mejor modelo
rf_best <-
rf_res %>%
select_best(metric = "accuracy")
# Mejor modelo
svm_best <-
svm_res %>%
select_best(metric="accuracy")
results = rbind(
lr = lr_res %>%
collect_predictions(parameters = lr_best) %>%
get_metrics() %>%
.$res %>%
as.data.frame(),
rf = rf_res %>%
collect_predictions(parameters = rf_best) %>%
get_metrics() %>%
.$res %>%
as.data.frame(),
svm = svm_res %>%
collect_predictions(parameters = svm_best) %>%
get_metrics() %>%
.$res %>%
as.data.frame()
)
library(knitr)
kable(results,digits=3)
results = rbind(
lr = lr_res %>%
collect_predictions(parameters = lr_best) %>%
get_metrics() %>%
.$res %>%
as.data.frame(),
lrs = lrs_res %>%
collect_predictions() %>%
get_metrics() %>%
.$res %>%
as.data.frame(),
lrs1 = lrs1_res %>%
collect_predictions() %>%
get_metrics() %>%
.$res %>%
as.data.frame(),
lr_pca = lr_pca_res %>%
collect_predictions() %>%
get_metrics() %>%
.$res %>%
as.data.frame(),
rf = rf_res %>%
collect_predictions(parameters = rf_best) %>%
get_metrics() %>%
.$res %>%
as.data.frame(),
svm = svm_res %>%
collect_predictions(parameters = svm_best) %>%
get_metrics() %>%
.$res %>%
as.data.frame()
)
results = rbind(
lr = lr_res %>%
collect_predictions(parameters = lr_best) %>%
get_metrics() %>%
.$res %>%
as.data.frame(),
lrs = lrs_metrics$res %>%
as.data.frame(),
lrs2 = lrs2_metrics$res %>%
as.data.frame(),
lr_pca = lr_pca_metrics$res %>%
as.data.frame(),
rf = rf_res %>%
collect_predictions(parameters = rf_best) %>%
get_metrics() %>%
.$res %>%
as.data.frame(),
svm = svm_res %>%
collect_predictions(parameters = svm_best) %>%
get_metrics() %>%
.$res %>%
as.data.frame()
)
library(knitr)
kable(results,digits=3)
# the last model
lr_best # penalty = 0.000259
# mixture = 1
last_lr_mod <- logistic_reg(penalty = lr_best$penalty, mixture = lr_best$penalty) |>
set_engine("glmnet")
# the last workflow
last_lr_workflow <-
lr_workflow %>%
update_model(last_lr_mod)
# the last fit
set.seed(345)
last_lr_fit <-
last_lr_workflow %>%
last_fit(splits,
add_validation_set = T)
last_lr_fit
last_lr_fit %>%
collect_metrics()
last_lr_fit %>%
collect_predictions() %>%
get_metrics()
last_lr_fit %>% extract_workflow() %>% predict(new_data = full_grid,type="prob")
# the last model
rf_best # mtry = 1 --> BOOSTING?
last_rf_mod <-
rand_forest(mtry = rf_best$mtry, min_n = rf_best$min_n, trees = 1000) %>%
set_engine("ranger", num.threads = cores,importance="impurity") %>%
set_mode("classification")
# the last workflow
last_rf_workflow <-
rf_workflow %>%
update_model(last_rf_mod)
# Detectar el número de núcleos para trabajar en paralelo
cores <- parallel::detectCores()
cores
# Construimos el modelo, especificando el número de núcleos a usar en la computación en paralelo de forma que la computación sea más eficiente
# 1º Construir el modelo
rf_mod <-
rand_forest(mtry = tune(), min_n = tune(), trees = 1000) %>%
set_engine("ranger", num.threads = cores) %>%
set_mode("classification")
# 2º Construir la receta con el preprocesamiento
rf_recipe <-
recipe(fire ~ ., data = training) %>%
step_date(date,features = c("dow", "month")) %>%
# step_holiday(date) %>%
step_rm(date, cod_municipio, municipio)
# No normalizamos en este caso pues no es necesario
# 3º Ensamblar todo con workflow
rf_workflow <-
workflow() %>%
add_model(rf_mod) %>%
add_recipe(rf_recipe)
# 4º Train and tune
rf_mod
extract_parameter_set_dials(rf_mod)
# 1º Construir el modelo
svm_mod <-
svm_linear(cost = tune(), margin = tune()) %>%
set_engine("kernlab") %>%
set_mode("classification")
# 2º Construir la receta con el preprocesamiento
svm_recipe <-
recipe(fire ~ ., data = training) %>%
step_date(date,features = c("dow", "month")) %>%
step_rm(date, cod_municipio, municipio) %>%
step_dummy(all_nominal_predictors()) %>%
step_zv(all_predictors()) %>%
step_normalize(all_predictors())
# 3º Ensamblar todo con workflow
svm_workflow <-
workflow() %>%
add_model(svm_mod) %>%
add_recipe(svm_recipe)
# the last model
rf_best # mtry = 4
# min_n = 35
last_rf_mod <-
rand_forest(mtry = rf_best$mtry, min_n = rf_best$min_n, trees = 1000) %>%
set_engine("ranger", num.threads = cores,importance="impurity") %>%
set_mode("classification")
# the last workflow
last_rf_workflow <-
rf_workflow %>%
update_model(last_rf_mod)
# the last fit
set.seed(345)
last_rf_fit <-
last_rf_workflow %>%
last_fit(splits,
add_validation_set = T)
last_rf_fit
last_rf_fit %>%
collect_metrics()
last_rf_fit %>%
collect_predictions() %>%
get_metrics()
library(vip)
last_rf_fit %>%
extract_fit_parsnip() %>%
vip(num_features = 20)
last_rf_fit %>%
collect_predictions() %>%
roc_curve(fire, .pred_0) %>%
autoplot()
last_rf_fit %>%
extract_fit_parsnip() %>%
vip(num_features = 20)
last_rf_fit %>%
collect_predictions() %>%
get_metrics()
# the last model
svm_best
# margin = 0.0270
last_svm_mod <-
svm_linear(cost = svm_best$cost, margin = svm_best$margin) %>%
set_engine("kernlab") %>%
set_mode("classification")
# the last workflow
last_svm_workflow <-
svm_workflow %>%
update_model(last_svm_mod)
# the last fit
set.seed(345)
last_svm_fit <-
last_svm_workflow %>%
last_fit(splits,
add_validation_set = T)
last_svm_fit %>%
collect_metrics()
last_svm_fit %>%
collect_predictions() %>%
get_metrics()
bind_rows(rf_auc, lr_auc,svm_auc) %>%
ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) +
geom_path(lwd = 1, alpha = 0.7) +
geom_abline(lty = 3) +
coord_equal() +
scale_color_viridis_d(option = "turbo", end = .6)
# Curva ROC
rf_auc <-
rf_res %>%
collect_predictions(parameters = rf_best) %>%
roc_curve(fire, .pred_0) %>%
mutate(model = "Random Forest")
bind_rows(rf_auc, lr_auc,svm_auc) %>%
ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) +
geom_path(lwd = 1, alpha = 0.7) +
geom_abline(lty = 3) +
coord_equal() +
scale_color_viridis_d(option = "turbo", end = .6)
# Curva ROC
svm_auc <-
svm_res %>%
collect_predictions(parameters = svm_best) %>%
roc_curve(fire, .pred_0) %>%
mutate(model = "Support Vector Machine")
```{r}
bind_rows(rf_auc, lr_auc,svm_auc) %>%
ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) +
geom_path(lwd = 1, alpha = 0.7) +
geom_abline(lty = 3) +
coord_equal() +
scale_color_viridis_d(option = "turbo", end = .6)
last_lr_fit
results_test = rbind(
lr = last_lr_fit %>%
collect_predictions() %>%
get_metrics() %>%
.$res %>%
as.data.frame(),
rf = last_rf_fit %>%
collect_predictions() %>%
get_metrics() %>%
.$res %>%
as.data.frame(),
svm = last_svm_fit %>%
collect_predictions() %>%
get_metrics() %>%
.$res %>%
as.data.frame()
)
library(knitr)
kable(results_test,digits=3)
save(lr_res,rf_res,svm_res,last_svm_fit,last_lr_fit,last_rf_fit,
file = "salidas_intermedias/trained_models_strat_2024_04_28.RData")
load("salidas_intermedias/trained_models_strat_2024_04_28.RData")
last_lr_fit %>% extract_workflow() %>% predict(new_data = full_grid,type="prob")
full_grid
tidymodels
library(tidymodels)
?svm
?svm_linear
?svm_rbf
