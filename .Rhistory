rf_best2 <-
rf_res2 %>%
select_best(metric = "spec")
rf_best2
rf_metrics2 <- rf_res2 |>
collect_predictions(parameters = rf_best2) |>
get_metrics()
rf_metrics2
?rand_forest
rf_res3 <-
rf_workflow3 %>%
tune_grid(val_set,
grid = expand_grid(trees = c(50,100,200,300,500,1000)),
control = control_grid(save_pred = TRUE),
metrics = metric_set(accuracy,roc_auc,recall,spec))
# 3º Ensamblar todo con workflow
rf_workflow3 <-
workflow() %>%
add_model(rf_mod3) %>%
add_recipe(rf_recipe)
# 1º Se construye el modelo
rf_mod3 <-
rand_forest(mtry =rf_best2$mtry, min_n = rf_best1$min_n, trees = tune()) %>%
set_engine("ranger", num.threads = cores) %>%
set_mode("classification")
# 3º Ensamblar todo con workflow
rf_workflow3 <-
workflow() %>%
add_model(rf_mod3) %>%
add_recipe(rf_recipe)
# 4º Train and tune
set.seed(345)
rf_res3 <-
rf_workflow3 %>%
tune_grid(val_set,
grid = expand_grid(trees = c(50,100,200,300,500,1000)),
control = control_grid(save_pred = TRUE),
metrics = metric_set(accuracy,roc_auc,recall,spec))
rf_tuning3 <- rf_res3 |>
collect_metrics() |>
group_by(.metric)|>
summarise(max = max(mean),min=min(mean))
rf_tuning3
#plot
rf_plot3 <-
rf_res3 %>%
collect_metrics() %>%
mutate(.metric = ifelse(.metric == "recall","spec",
ifelse(.metric == "spec","recall",
.metric))) %>%
ggplot(aes(x = mtry, y = mean,col=.metric)) +
geom_point() +
geom_line() +
ylab("") +
theme_minimal()+
labs(title = paste0("Etapa 2\nFijado min_n = ", rf_best1$min_n, " se ajusta mtry"))
rf_plot3
#plot
rf_plot3 <-
rf_res3 %>%
collect_metrics() %>%
mutate(.metric = ifelse(.metric == "recall","spec",
ifelse(.metric == "spec","recall",
.metric))) %>%
ggplot(aes(x = trees, y = mean,col=.metric)) +
geom_point() +
geom_line() +
ylab("") +
theme_minimal()+
labs(title = paste0("Etapa 2\nFijado min_n = ", rf_best1$min_n, " se ajusta mtry"))
rf_plot3
rf_best3
# Mejor modelo
rf_best3 <-
rf_res3 %>%
select_best(metric = "spec")
rf_best3
rf_metrics3 <- rf_res3 |>
collect_predictions(parameters = rf_best2) |>
get_metrics()
rf_metrics3 <- rf_res3 |>
collect_predictions(parameters = rf_best3) |>
get_metrics()
rf_metrics3
rf_plot1 <-
rf_res1 %>%
collect_metrics() %>%
mutate(.metric = ifelse(.metric == "recall","spec",
ifelse(.metric == "spec","recall",
.metric))) %>%
ggplot(aes(x = min_n, y = mean,col=.metric)) +
geom_point() +
geom_line() +
ylab("") +
theme_minimal()+
labs(title = "Etapa 1\nFijado mtry = 4, se ajusta min_n")
rf_plot1
rf_best3$trees
# 1º Se construye el modelo
rf_mod3 <-
rand_forest(trees = tune()) %>%
set_engine("ranger", num.threads = cores) %>%
set_mode("classification")
# 2º Se usa la misma receta que antes
rf_recipe <-
recipe(fire ~ ., data = training) %>%
step_date(date,features = c("dow", "month")) %>%
# step_holiday(date) %>%
step_rm(date, cod_municipio, municipio)
# 3º Ensamblar todo con workflow
rf_workflow3 <-
workflow() %>%
add_model(rf_mod3) %>%
add_recipe(rf_recipe)
# 4º Train and tune
set.seed(345)
rf_res3 <-
rf_workflow3 %>%
tune_grid(val_set,
grid = expand_grid(trees = c(50,100,200,300,500,1000)),
control = control_grid(save_pred = TRUE),
metrics = metric_set(accuracy,roc_auc,recall,spec))
rf_tuning3 <- rf_res3 |>
collect_metrics() |>
group_by(.metric)|>
summarise(max = max(mean),min=min(mean))
rf_tuning3
#plot
rf_plot3 <-
rf_res3 %>%
collect_metrics() %>%
mutate(.metric = ifelse(.metric == "recall","spec",
ifelse(.metric == "spec","recall",
.metric))) %>%
ggplot(aes(x = trees, y = mean,col=.metric)) +
geom_point() +
geom_line() +
ylab("") +
theme_minimal()+
labs(title = paste0("Etapa 2\nFijado min_n = ", rf_best1$min_n, " se ajusta mtry"))
rf_plot3
# Mejor modelo
rf_best3 <-
rf_res3 %>%
select_best(metric = "spec")
rf_best3
rf_metrics3 <- rf_res3 |>
collect_predictions(parameters = rf_best3) |>
get_metrics()
rf_metrics3
# 1º Construir el modelo
rf_mod1 <-
rand_forest(mtry = 4, min_n = tune(), trees = 1000) %>%
set_engine("ranger", num.threads = cores) %>%
set_mode("classification")
# 1º Construir el modelo
rf_mod1 <-
rand_forest(mtry = 4, min_n = tune(), trees = rf_best3$trees) %>%
set_engine("ranger", num.threads = cores) %>%
set_mode("classification")
# 2º Construir la receta con el preprocesamiento
rf_recipe <-
recipe(fire ~ ., data = training) %>%
step_date(date,features = c("dow", "month")) %>%
# step_holiday(date) %>%
step_rm(date, cod_municipio, municipio)
# 3º Ensamblar todo con workflow
rf_workflow1 <-
workflow() %>%
add_model(rf_mod1) %>%
add_recipe(rf_recipe)
# 4º Train and tune
set.seed(345)
rf_res1 <-
rf_workflow1 %>%
tune_grid(val_set,
grid = expand_grid(min_n = seq(1000,2500,100)),
control = control_grid(save_pred = TRUE),
metrics = metric_set(accuracy,roc_auc,recall,spec))
rf_tuning1 <- rf_res1 |>
collect_metrics() |>
group_by(.metric)|>
summarise(max = max(mean),min=min(mean))
rf_tuning1
rf_plot1 <-
rf_res1 %>%
collect_metrics() %>%
mutate(.metric = ifelse(.metric == "recall","spec",
ifelse(.metric == "spec","recall",
.metric))) %>%
ggplot(aes(x = min_n, y = mean,col=.metric)) +
geom_point() +
geom_line() +
ylab("") +
theme_minimal()+
labs(title = "Etapa 1\nFijado mtry = 4, se ajusta min_n")
rf_plot1
# Mejor modelo
rf_best1 <-
rf_res1 %>%
select_best(metric = "spec")
rf_best1
rf_metrics1 <- rf_res1 |>
collect_predictions(parameters = rf_best1) |>
get_metrics()
rf_metrics1
# ETAPA 2: fijado min_n de la etapa anterior, se ajusta mtry
# -----------------
# 1º Se construye el modelo
rf_mod2 <-
rand_forest(mtry = tune(), min_n = rf_best1$min_n, trees = 1000) %>%
set_engine("ranger", num.threads = cores) %>%
set_mode("classification")
# 3º Ensamblar todo con workflow
rf_workflow2 <-
workflow() %>%
add_model(rf_mod2) %>%
add_recipe(rf_recipe)
# 4º Train and tune
set.seed(345)
rf_res2 <-
rf_workflow2 %>%
tune_grid(val_set,
grid = expand_grid(mtry = 1:10),
control = control_grid(save_pred = TRUE),
metrics = metric_set(accuracy,roc_auc,recall,spec))
rf_tuning2 <- rf_res2 |>
collect_metrics() |>
group_by(.metric)|>
summarise(max = max(mean),min=min(mean))
rf_tuning2
#plot
rf_plot2 <-
rf_res2 %>%
collect_metrics() %>%
mutate(.metric = ifelse(.metric == "recall","spec",
ifelse(.metric == "spec","recall",
.metric))) %>%
ggplot(aes(x = mtry, y = mean,col=.metric)) +
geom_point() +
geom_line() +
ylab("") +
theme_minimal()+
labs(title = paste0("Etapa 2\nFijado min_n = ", rf_best1$min_n, " se ajusta mtry"))
# Mejor modelo
rf_best2 <-
rf_res2 %>%
select_best(metric = "spec")
rf_best2
rf_metrics2 <- rf_res2 |>
collect_predictions(parameters = rf_best2) |>
get_metrics()
rf_metrics2
# 1º Se construye el modelo
rf_mod3 <-
rand_forest(mtry = 4, min_n = 1500,trees = tune()) %>%
set_engine("ranger", num.threads = cores) %>%
set_mode("classification")
# 2º Se usa la misma receta que antes
rf_recipe <-
recipe(fire ~ ., data = training) %>%
step_date(date,features = c("dow", "month")) %>%
# step_holiday(date) %>%
step_rm(date, cod_municipio, municipio)
# 3º Ensamblar todo con workflow
rf_workflow3 <-
workflow() %>%
add_model(rf_mod3) %>%
add_recipe(rf_recipe)
# 4º Train and tune
set.seed(345)
rf_res3 <-
rf_workflow3 %>%
tune_grid(val_set,
grid = expand_grid(trees = c(50,100,200,300,500,1000)),
control = control_grid(save_pred = TRUE),
metrics = metric_set(accuracy,roc_auc,recall,spec))
rf_tuning3 <- rf_res3 |>
collect_metrics() |>
group_by(.metric)|>
summarise(max = max(mean),min=min(mean))
rf_tuning3
#plot
rf_plot3 <-
rf_res3 %>%
collect_metrics() %>%
mutate(.metric = ifelse(.metric == "recall","spec",
ifelse(.metric == "spec","recall",
.metric))) %>%
ggplot(aes(x = trees, y = mean,col=.metric)) +
geom_point() +
geom_line() +
ylab("") +
theme_minimal()+
labs(title = paste0("Etapa 2\nFijado min_n = ", rf_best1$min_n, " se ajusta mtry"))
rf_plot3
# Mejor modelo
rf_best3 <-
rf_res3 %>%
select_best(metric = "spec")
rf_best3
rf_metrics3 <- rf_res3 |>
collect_predictions(parameters = rf_best3) |>
get_metrics()
rf_metrics3
# 1º Construir el modelo
rf_mod1 <-
rand_forest(mtry = 4, min_n = tune(), trees = rf_best3$trees) %>%
set_engine("ranger", num.threads = cores) %>%
set_mode("classification")
# 2º Construir la receta con el preprocesamiento
rf_recipe <-
recipe(fire ~ ., data = training) %>%
step_date(date,features = c("dow", "month")) %>%
# step_holiday(date) %>%
step_rm(date, cod_municipio, municipio)
# 3º Ensamblar todo con workflow
rf_workflow1 <-
workflow() %>%
add_model(rf_mod1) %>%
add_recipe(rf_recipe)
# 4º Train and tune
set.seed(345)
rf_res1 <-
rf_workflow1 %>%
tune_grid(val_set,
grid = expand_grid(min_n = seq(1000,2500,100)),
control = control_grid(save_pred = TRUE),
metrics = metric_set(accuracy,roc_auc,recall,spec))
rf_tuning1 <- rf_res1 |>
collect_metrics() |>
group_by(.metric)|>
summarise(max = max(mean),min=min(mean))
rf_tuning1
rf_plot1 <-
rf_res1 %>%
collect_metrics() %>%
mutate(.metric = ifelse(.metric == "recall","spec",
ifelse(.metric == "spec","recall",
.metric))) %>%
ggplot(aes(x = min_n, y = mean,col=.metric)) +
geom_point() +
geom_line() +
ylab("") +
theme_minimal()+
labs(title = "Etapa 1\nFijado mtry = 4, se ajusta min_n")
rf_plot1
# Mejor modelo
rf_best1 <-
rf_res1 %>%
select_best(metric = "spec")
rf_best1
# Mejor modelo
rf_best1 <-
rf_res1 %>%
select_best(metric = "accuracy")
rf_best1
rf_metrics1 <- rf_res1 |>
collect_predictions(parameters = rf_best1) |>
get_metrics()
rf_metrics1
# ETAPA 2: fijado min_n de la etapa anterior, se ajusta mtry
# -----------------
# 1º Se construye el modelo
rf_mod2 <-
rand_forest(mtry = tune(), min_n = rf_best1$min_n, trees = 1000) %>%
set_engine("ranger", num.threads = cores) %>%
set_mode("classification")
# 3º Ensamblar todo con workflow
rf_workflow2 <-
workflow() %>%
add_model(rf_mod2) %>%
add_recipe(rf_recipe)
# 4º Train and tune
set.seed(345)
rf_res2 <-
rf_workflow2 %>%
tune_grid(val_set,
grid = expand_grid(mtry = 1:10),
control = control_grid(save_pred = TRUE),
metrics = metric_set(accuracy,roc_auc,recall,spec))
rf_tuning2 <- rf_res2 |>
collect_metrics() |>
group_by(.metric)|>
summarise(max = max(mean),min=min(mean))
rf_tuning2
#plot
rf_plot2 <-
rf_res2 %>%
collect_metrics() %>%
mutate(.metric = ifelse(.metric == "recall","spec",
ifelse(.metric == "spec","recall",
.metric))) %>%
ggplot(aes(x = mtry, y = mean,col=.metric)) +
geom_point() +
geom_line() +
ylab("") +
theme_minimal()+
labs(title = paste0("Etapa 2\nFijado min_n = ", rf_best1$min_n, " se ajusta mtry"))
# Mejor modelo
rf_best2 <-
rf_res2 %>%
select_best(metric = "spec")
rf_best2
rf_metrics2 <- rf_res2 |>
collect_predictions(parameters = rf_best2) |>
get_metrics()
rf_metrics2
rf_plot2
cores = 8
set.seed(345)
models = tibble(model_name = c("lr","lr_pca","dt","rf","svm_linear","svm_rbf","knn"),
models_tune = list(lr_res,lr_pca_res,dt_res,rf_res2,svm_res,svm_rbf_res,knn_res),
models_workflow = list(lr_workflow,lr_pca_workflow,dt_workflow,rf_workflow2,svm_workflow,svm_rbf_workflow,knn_workflow))
models = tibble(model_name = c("rf"),
models_tune = list(rf_res2),
models_workflow = list(rf_workflow2))
models = models %>% mutate(best_tuning = map(models_tune,function(x) select_best(x,metric = "accuracy")),
best_metrics = map2(models_tune,
best_tuning,
~ collect_predictions(.x,parameters = .y) %>%
get_metrics() %>%
extract2(1)), # Para extraer solo las medidas y no la matriz de confusión
roc = map2(models_tune,
best_tuning,
~ collect_predictions(.x,parameters = .y) %>%
roc_curve(fire, .pred_0))
)
metrics = models %>% select(model_name,best_metrics) %>% unnest(best_metrics)
metrics
# curva roc
metrics %>%
pivot_longer(cols = c(roc_auc, accuracy, recall, specificity, precision),
names_to = "metric") %>%
ggplot(aes(x = metric, y = value, group = model_name)) +
geom_line(aes(col = model_name),size=1) +
geom_point(aes(col = model_name),size=2.3) +
scale_color_viridis_d(option="turbo") +
geom_vline(xintercept=1:5, linetype="dotted") +
labs(col = "Modelo", title = "Métricas sobre validación") +
theme_minimal() +
theme(axis.line.x = element_line(color="black", size = 1),
axis.line.y = element_line(color="black", size = 1))
# plot medidas
models %>% select(model_name,roc) %>% unnest(roc) %>%
ggplot(aes(x = 1 - specificity, y = sensitivity, col = model_name)) +
geom_path(lwd = 1, alpha = 0.7) +
geom_abline(lty = 3) +
coord_equal() +
scale_color_viridis_d(option="turbo") +
labs(color="Modelo")+
# scale_color_viridis_d(option = "turbo",name="Modelo") +
theme_minimal() +
theme(axis.line.x = element_line(color="black", size = 1),
axis.line.y = element_line(color="black", size = 1))+
ggtitle("Curva ROC en validación")
library(knitr)
kable(metrics,digits=3)
cores = 8
set.seed(345)
models <- models %>% mutate(final_workflow = map2(models_workflow,best_tuning,finalize_workflow),
last_fit = map(final_workflow,function(x) last_fit(x,splits,add_validation_set=T)))
models = models %>% mutate(test_metrics = map(last_fit,
~collect_predictions(.x) %>%
get_metrics() %>%
extract2(1)), # Para extraer solo las medidas
test_roc = map(last_fit,
~collect_predictions(.x) %>%
roc_curve(fire, .pred_0))
)
test_metrics = models %>% select(model_name,test_metrics) %>% unnest(test_metrics)
test_metrics
last_rf_fit %>%
extract_fit_parsnip() %>%
vip(num_features = 50,aesthetics = list(fill="lightblue")) +
theme_minimal()
library(vip)
last_rf_fit %>%
extract_fit_parsnip() %>%
vip(num_features = 50,aesthetics = list(fill="lightblue")) +
theme_minimal()
# the last model
last_rf_mod <-
rand_forest(mtry = rf_best2$mtry, min_n = rf_best1$min_n, trees = rf_best3$trees) %>%
set_engine("ranger", num.threads = cores,importance="impurity") %>%
set_mode("classification")
# the last workflow
last_rf_workflow <-
rf_workflow2 %>%
update_model(last_rf_mod)
# the last fit
set.seed(345)
last_rf_fit <-
last_rf_workflow %>%
last_fit(splits,
add_validation_set = T)
library(vip)
last_rf_fit %>%
extract_fit_parsnip() %>%
vip(num_features = 50,aesthetics = list(fill="lightblue")) +
theme_minimal()
?decision_tree
rf_tuning3 <- rf_res3 |>
collect_metrics() |>
group_by(.metric)|>
summarise(max = max(mean),min=min(mean))
rf_tuning3
rf_best$tress
rf_best3$tress
rf_best3$trees
test_metrics
rf_plot3
