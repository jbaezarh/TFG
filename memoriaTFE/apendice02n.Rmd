---
output:
  pdf_document:
    keep_tex: yes
    number_sections: yes
    citation_package: natbib  # comentado usa: pandoc-citeproc
    template: latex/templateMemoriaTFE.tex
    pandoc_args: ["--metadata-file=cabecera_capitulos.yaml"]
    includes:
       in_header: "wrap-code.tex"
  html_document: default
#bibliography: bib/library.bib # descomentar si: editor visual RStudio  
---

<!-- escribir 2 para capítulo 3 -->
<!-- \setcounter{chapter}{2} --> 
<!-- \pagenumbering{arabic} -->

`r xfun::file_string('cabecera_capitulos.tex')`

```{r include=FALSE}
source("cabecera_chunk_inicio.R")
```

```{r include=FALSE}
knitr::opts_chunk$set(eval = FALSE)

```

# Apéndice: Código

Para elaborar el presente trabajo, ha sido necesario escribir una gran cantidad de código, debido al gran número de técnicas empleadas y de resultados y gráficos presentados. Siendo consciente del papel fundamental que la programación ha tenido en este trabajo, y de la necesidad de ilustrar algunas de los procedimientos seguidos, se ha optado por incluir un apéndice con los principales fragmentos del código empleado. El objetivo de este apéndice es, por tanto, ilustrar las técnicas empleadas, sirviendo de apoyo al texto. 

El lector interesado encontrará aquí el código en lenguaje *R* empleado para llevar a cabo algunas de las tareas esenciales descritas en la memoria, con los comentarios necesarios para su correcta comprensión. Se ha decidido incluir los paquetes usados en cada sección por considerarse de interés. 

Se han omitido o acortado muchas secciones, con el objetivo de no excederse en la extensión del apéndice, pero ilustrando adecuadamente las técnicas empleadas. El código completo, junto con los datos originales y los conjuntos de datos generados, puede consultarse en el repositorio de github \url{https://github.com/jbaezarh/TFG} o en el archivo adjunto proporcionado (en este caso, sin los datos originales).


## Generación de la muestra {#code:generar_muestra}

```{r eval=FALSE}
# Librerías -------------------------------------------------------
# Se cargan las librerías que se usarán en esta sección

library(terra) # Raster data
library(sf) # Vector data
library(mapSpain) # Polígonos de las regiones de España
library(tidyverse) # Manipulación de datos
library(lubridate) # Manipulación de fechas


# CRS de referencia -----------------------------------------------
# Será el CRS que se use en todo el proyecto

pend <- rast("data_raw/topograficas/pendiente.tif")
crs_reference = crs(pend)
rm(pend) # Se elimina de la memoria para liberar espacio


# Polígono de Andalucía -------------------------------------------
Andalucia <- esp_get_ccaa(ccaa = "Andalucía") # Se obtiene el polígono de la comunidad autónoma de Andalucía
andalucia_proj <- st_transform(Andalucia,crs_reference) # Se transforma al sistema de referencia usado en el proyecto

# area_monte es el área donde se generarán las muestras negativas.

# Dado que no hay un mapa que indique claramente cuales son las zonas que se consideran "monte" en Andalucía y dado que los polígonos de incendios también cubren zonas agrícolas y urbanas (aunque menores en número que las zonas forestales), se considerará "monte" toda Andalucía, sin distinción. El sentido de esta variable es, precisamente, que pueda modificarse en futuros estudios
area_monte <- andalucia_proj

# Generación de la muestra ------------------------------------

# Generación de la muestra estratificando por mes de forma que la proporción de observaciones positivas y negativas por mes (en todo el periodo) sea la misma

##  Tamaño muestral --------------------------------------------
# Se dispone de 1089 incencios correctamente registrados entre 2002 y 2022

n_in=10 # Número de puntos a muestrear dentro de cada poligono
n_out=1089*10 # Número de muestras negativas

##  Generación aleatoria de fechas para las muestras negativas ---

# Primero se leen todos los datos de todos los archivos de incendios y se almacenan en la variable incendios
incendios = NULL

for (year in 2002:2022) {
  incendios = rbind(incendios, 
                    st_read(paste0("./data_raw/incendios_2000-2022/incendios_",
                                   year,".shp")) %>% 
                      select("FECHA_INIC" = matches("(?i)^FECHA_INIC$|^fecha_inic.$"))) 
}

# Se cuenta el número de incendios con fecha de inicio correcta en cada mes
incendios_mes = incendios %>% 
  mutate(FECHA_INIC = ymd(FECHA_INIC),.keep="unused") %>% 
  filter(!is.na(FECHA_INIC)) %>% 
  filter(year(FECHA_INIC)<=2022,year(FECHA_INIC)>=2002) %>% 
  st_drop_geometry() %>% 
  mutate(MES = month(month(FECHA_INIC))) %>% 
  count(MES) 

# Fechas posibles para las muestras negativas
possible_dates = tibble (date = seq(as.Date('2002/01/01'), as.Date('2022/12/31'), by="day")) %>% 
  mutate(MES = month(date)) %>% 
  left_join(incendios_mes,
            join_by(MES)) 

set.seed(12345) # Se fija la semilla para que sea reproducible

# Se generan fechas aleatorias para las muestras negativas entre 2002 y 2022 siguiendo con una distribución de probabilidad proporcional a la cantidad de incendios observados en cada mes
dates = sample(possible_dates$date, 
               n_out,replace = T,
               prob = possible_dates$n) 

rm(incendios, possible_dates) # Se borran para liberar memoria

##  Selección de localizaciones aleatorias ------------------------
# Para la selección de la muestra se seguirá el siguiente procedimiento:
# 1. Para las muestra positivas: Se tomarán n_in puntos aleatorios dentro de cada polígono de incendio y se le asociará a cada uno de ellos la fecha de inicio del incendio.
# 2. Para la muestras negativas: Se le asociará una localización aleatoria dentro de area_monte a cada una de las fechas aleatorias generadas dentro del periodo de estudio (dates). Se tendrá en cuenta que no pueden haber muestras negativas a menos de 15km de una zona en la que haya habido un incendio en una franja de 6 días alrededor de la fecha de la observación (3 días antes a 3 días después).

points_in = NULL  # Almacena las muestras positvas
points_out = NULL # Almacena las muestras negativas

for (year in 2002:2022) {
  
  cat("YEAR ", year," : -------------------------------------\n")
  cat("  Generando muestras positivas...\n")
  incendios <- st_read(paste0("./data_raw/incendios_2000-2022/incendios_",year,
                              ".shp"),quiet=T) |> 
    st_transform(crs = crs_reference) |> 
    rename_with(.fn=tolower) |> 
    mutate(fecha_inic=ymd(fecha_inic),geometry,.keep="none")
  
  
  ## Generación de puntos positivos
  
  for (i in 1:nrow(incendios)) {
    point_in_sfc <- st_sample(incendios[i,],size=n_in) # Se generan n_i puntos dentro de cada incendio
    point_in_attr <- data.frame(fire = rep(1,n_in),date = rep(incendios[i,]$fecha_inic,n_in))
    point_in <- st_sf(point_in_attr,geometry= point_in_sfc)
    
    if (is.null(points_in)) {
      points_in <- point_in 
    } else {
      points_in <- points_in |> 
        add_row(point_in) 
    }
  }
  
  ## Generación de puntos negativos
  
  cat("  Generando muestras negativas...\n")
  # ---> Nota: los puntos se generan en area_monte
  
  dates_year <- dates[year(dates) == year]
  locations = NULL
  
  for (day in dates_year) {
    incendios_day = filter(incendios,fecha_inic>=day-3 & fecha_inic<=day+3)
    if (nrow(incendios_day)==0){ 
      # Si no ha habido incendios en una franja de 6 días en Andalucía
      if (is.null(locations)) {
        locations = st_sample(area_monte,size=1) 
      } else {
        locations = c(locations, st_sample(area_monte,size=1))
      }
    } else { 
      # Si ha habido algún incendio en una franja de 6 días en Andalucía 
      # (3 días antes a 3 días después)
      repeat {
        possible_location = st_sample(area_monte,size=1)
        # Se comprueba si está a 15km o menos de un incendio registrado
        if (!st_is_within_distance(possible_location, 
                                   st_union(incendios_day), 
                                   dist = 15000, sparse = FALSE)) {
          if (is.null(locations)) {
            locations = possible_location
            break
          } else {
            locations = c(locations, possible_location)
            break
          }
        }
      }
    }
  }
  
  
  points_out_attr <- data.frame(fire = rep(0,length(dates_year)),date = dates_year)
  
  if (is.null(points_out)) {
    points_out <- st_sf(points_out_attr,geometry= locations)
  } else {
    points_out <- points_out |> 
      add_row(st_sf(points_out_attr,geometry= locations)) 
  }
  
}


sample <- rbind(points_in,points_out) # La muestra generada


# Comprobación y corrección -----------------------------------
summary(sample) # Hay una fecha de un incendio errónea
max(sample$date,na.rm=T) # "2033-08-15"

# Se eliminan las observaciones con fecha de incendio errónea que se han detectado
sample <- sample[-which(sample$date==max(sample$date,na.rm=T)),]
summary(sample) # Corregido

# Almacenamiento de resultados ----------------------------------
save(sample,file=paste0("salidas_intermedias/sample_strat_",
                        Sys.Date(),".RData"))
```

## Asignación de variables a localizaciones {#code:asignar_variables}

A continuación se define la función `asignar_variables` que dada una muestra de puntos en Andalucía con fechas comprendidas entre 2002 y 2022 le asocia a cada observación todos los valores de las variables consideradas en el estudio. Esta función se usará varias veces a lo largo del trabajo.

```{r eval=FALSE}
# Librerías -----------------------------------------
# Se cargan las librerías que se usarán en esta sección
library(nasapower) # Para obtener la información meteorológica
library(raster, include.only = c("rasterFromXYZ"))  # Función para construir rásteres a partir de data.frames
library(tidyverse) # Manipulación de datos
library(sf) # Vector data
library(terra) # Raster data
library(mapSpain) # Polígonos de las regiones de España
library(lubridate) # Manipulación de fechas

asignar_variables = function(sample) {
  # Argumentos:
  # * sample: objeto sf con una columna de geometrías de tipo POINT (dentro de los límites de Andalucía) y fechas comprendidas entre 01/01/2002 y 31/12/2022 

  
  crs_reference = st_crs(sample) # Se usa el sistema de referencia de coordenadas de la muestra
  and = esp_get_ccaa(ccaa = "Andalucía") %>% st_transform(st_crs(sample)) # Polígono de Andalucía
  
  # Variables meteorológicas ----------------------------
  cat("Asignando variables meteorológicas...\n")
  
  # Tranformamos los datos a WGS84
  andalucia_WGS84 <- st_transform(and,crs="WGS84")
  
  dataset = NULL # Variable en la que se almacenará el conjunto completo
  
  # Se trabaja anualmente, pues la API de NASA POWER solo admite consultas de hasta 366 días

  for (year in sort(unique(year(sample$date)))) {
    
    cat("YEAR ", year," : -------------------------------------\n")
    
    # Los puntos de cada año
    points = filter(sample,year(date)==year)  
    points_WGS84 <- st_transform(points,crs="WGS84")
    
    # Consulta a la api para obtener todo los valores del año
    daily_single_ag <- get_power(
      community = "ag",
      lonlat = c(-8,35.5,-1.5,39),  # Límites de Andalucía
      pars = c("T2M","GWETTOP", "RH2M","WD10M","WS10M","PRECTOTCORR"),
      dates = paste0(year,c("-01-01","-12-31")),
      temporal_api = "daily")
    
    # Identificador
    daily_single_ag$clim_id <- 1:nrow(daily_single_ag)
    points$clim_id = NA # Se inicializa el identificador
    
    for (day in unique(points$date)) {
      
      points_day = points$date==day
      
      # Seleccionar un día
      clim_day  <- filter(daily_single_ag,YYYYMMDD==day) |> 
        dplyr::select(x = LON,y = LAT,clim_id= clim_id)
      
      id_rast_day = rast(rasterFromXYZ(clim_day,crs="WGS84")) # Se crea el raster con los identificadores
      
      points[points_day,]$clim_id <- terra::extract(id_rast_day, points_WGS84[points_day,])$clim_id # Se asocia a cada registro de la muestra el identificador correspondiente
    }
    
    # Haciendo uso del identificador se asocian todas las variables meteorológicas correspondientes a cada registro
    points <- points |> 
      left_join(select(daily_single_ag, -c(LAT,LON,DOY,YYYYMMDD)),
                by=join_by(clim_id)) |> 
      select(-clim_id) 
    
    dataset = rbind(dataset,points)
  }
  
  rm(points,points_WGS84,daily_single_ag,clim_day,
     id_rast_day,points_day,day,year,andalucia_WGS84)
  
  
  # Variables topográficas -----------------------------
  cat("Asignando variables topográficas...\n")
  elev <- rast("data_raw/topograficas/elevacion.tif")
  pend <- rast("data_raw/topograficas/pendiente.tif")
  orient <- rast("data_raw/topograficas/orientacion.tif")
  curv <- rast("data_raw/topograficas/curvatura.tif")
  
  # Se extraen los valores de cada una de las capas
  var_topograficas <- list(elevacion = elev,pendiente = pend,
                           orientacion = orient,curvatura = curv) |> 
    lapply(as.numeric) # Es necesario pasarlas a numeric para poder 
                       # trabajar con ellas y extraer los valores
  
  
  points_topograficas <- sapply(var_topograficas, function(x) terra::extract(x,dataset))[2,] |> 
    as_tibble()
  
  dataset <- cbind(dataset,points_topograficas)
  
  rm(elev,pend,orient,curv,var_topograficas,points_topograficas)
  
  
  # Variables antropogénicas -------------------------------
  cat("Asignando variables antropogénicas...\n")
  
  ## Para optimizar el cálculo evitando que se repitan cálculos si hay puntos repetidos:!!
  dataset_geoms <- dataset %>% 
    group_by(geometry) %>% 
    group_keys() %>% 
    st_sf(crs = st_crs(dataset)) 
  
  ### Carreteras: ----
  carreteras <- read_sf("data_raw/antropologicas/RedCarreteras/09_14_RedCarreteras.shp") |> 
    st_union()
  
  dataset_geoms$dist_carretera <- st_distance(dataset_geoms,carreteras) |> 
    as.numeric()      # metres

  rm(carreteras)
  
  ### Poblaciones: ----
  poblaciones <- read_sf("data_raw/antropologicas/Poblaciones/07_01_Poblaciones.shp") |> 
    st_union()
  
  dataset_geoms$dist_poblacion <- st_distance(dataset_geoms,poblaciones) |> 
    as.numeric()    # metres
  
  rm(poblaciones)
  
  ### Linea Eléctrica: ----
  linea_electrica <- read_sf("data_raw/antropologicas/LineaElectrica/10_14_LineaElectrica.shp") |> 
    st_union()
  
  dataset_geoms$dist_electr <- st_distance(dataset_geoms,linea_electrica) |> 
    as.numeric() # metres
  
  rm(linea_electrica)
  
  ### Ferrocarril: ----
  ferrocarril <- read_sf("data_raw/antropologicas/Ferrocarril/09_21_Ferrocarril.shp") |> 
    st_union()
  dataset_geoms$dist_ferrocarril <- st_distance(dataset_geoms,ferrocarril) |> 
    as.numeric()
  
  rm(ferrocarril)
  
  ### Camino / Via: ----
  camino <- read_sf("data_raw/antropologicas/Camino/09_19_Camino.shp") 
  viapec <- read_sf("data_raw/antropologicas/Camino/09_22_ViasPecuarias.shp") 
  
  camino_viapec <- c(st_geometry(camino),st_geometry(viapec))
  rm(camino,viapec)
  
  camino_viapec <- st_union(camino_viapec)
  
  dataset_geoms$dist_camino <- st_distance(dataset_geoms,camino_viapec) |> 
    as.numeric()
  
  rm(camino_viapec)
  
  ### Sendero / Vía Verde / CarrilBici: ----
  viaverde <- read_sf("data_raw/antropologicas/Sendero_ViaVerde/09_24_ViaVerde.shp") 
  sendero <- read_sf("data_raw/antropologicas/sendero_ViaVerde/09_20_Sendero.shp") 
  carrilbic <- read_sf("data_raw/antropologicas/sendero_ViaVerde/09_23_CarrilBici.shp")
  
  sendero_viaverde_carrilbici <- c(st_geometry(viaverde),st_geometry(sendero),st_geometry(carrilbic)) |> 
    st_union()
  
  dataset_geoms$dist_sendero <- st_distance(dataset_geoms,sendero_viaverde_carrilbici) |> 
    as.numeric()
  
  rm(sendero,sendero_viaverde_carrilbici,viaverde,carrilbic)
  
  ### ENP: ----
  enp1 <- read_sf("data_raw/antropologicas/ENP/11_07_Enp_FiguraProteccion.shp" )
  enp2 <- read_sf("data_raw/antropologicas/ENP/11_07_Enp_RegimenProteccion.shp")
  
  enp <- c(st_geometry(enp1),st_geometry(enp2)) |>  st_union()
  enp_sf <- st_sf(enp)
  
  # Se rasteriza para aumentar la eficiencia computacional
  enp_rast <- rasterize(enp_sf,
                        rast("data_raw/topograficas/pendiente.tif"), # Modelo
                        background = 0)
  dataset_geoms$enp= terra::extract(enp_rast,dataset_geoms)[,2]
  
  rm(enp,enp1,enp2,enp_sf,enp_rast)
  
  ### Uso Suelo: ----
  # Inicialmente se ha rasterizado para aumentar la eficiencia computacional
  # UsoSuelo <- read_sf("data_raw/antropologicas/UsoSuelo/06_01_UsoSuelo.shp")
  # UsoSuelo_rast <- rasterize(UsoSuelo,
  #                            rast("data_raw/topograficas/pendiente.tif"), # Modelo
  #                            field="cod_uso")
  
  UsoSuelo_rast <- rast("data_cleaning/uso_suelo_rast.tiff")
  
  dataset_geoms$uso_suelo = terra::extract(UsoSuelo_rast,dataset_geoms)[,2]
  
  # Hidrográficas -------------------------------------
  cat("Asignando variables hidrográficas...\n")
  
  ### Distancia a ríos: ----
  rios <- read_sf("data_raw/hidrograficas/Rios_Espana.shp") |> 
    st_transform(st_crs(dataset)) |> 
    st_crop(xmin = 100394.4, # Esto se hace solo para no tener que considerar todo el file y que sea más eficiente computacionalmente
            ymin = 3976888.6,
            xmax = 690000.8,
            ymax = 4350000.0) |> 
    st_union()
  
  dataset_geoms$dist_rios <- st_distance(dataset_geoms,rios) |> 
    as.numeric() # metres
  
  rm(rios)
  
  ##  Se vuelven a desagrupar los registros y se le asigna a cada registro los valores correspondientes calculados!!
  dataset <- dataset %>% 
    st_join(dataset_geoms,left = TRUE) # Es un left join espacial
  
  # Demográficas --------------------------------------
  cat("Asignando variables demográficas...\n")
  
  ### Población y densidad de población: ----
  
  poblacion <- read_csv2("data_raw/antropologicas/Población/poblacion_municipios.txt",
                         locale=locale(decimal_mark = ","),
                         col_select = 1:5,col_types = "ccifn") |> 
    mutate(Valor=as.integer(round(Valor))) # La población debe ser un entero
  
  
  area_municipios <- read_csv2("data_raw/antropologicas/Población/extension_municipal.txt",
                               locale=locale(decimal_mark = ","),
                               col_select = 1:6, col_types = "fffffn")

  area_municipios <- area_municipios %>% 
    filter(!is.na(CODIGO_INE3)) %>% 
    select(CODIGO_INE3,Valor) %>% 
    rename("Area" = "Valor")
  
  # Se calcula la densidad de población anual como el cociente del número de habitantes entre la extensión del municipio
  dens_poblacion <- poblacion %>% 
    select(-Medida) %>% 
    rename("Poblacion" = "Valor",
           "Municipio" = "Lugar de residencia") %>% 
    left_join(area_municipios,
              join_by("CODIGO_INE3")) %>% 
    mutate(dens_poblacion = Poblacion/Area) %>% 
    select(-Area)
  
  municipios <- esp_get_munic(epsg = 4258,region = "Andalucía") |> 
    st_transform(crs_reference)
    
  # Se asocia cada observacion su código de municipio correspondiente 
  
  num_mun = st_intersects(dataset,municipios) 
  
  # Se eliminan las observaciones que no están en ningún municipio 
  if (any(sapply(num_mun,function(x) length(x) == 0))) {
    cat("Eliminamos las observaciones:\n",which(sapply(num_mun,function(x) length(x) == 0)))
    dataset = dataset[-which(sapply(num_mun,function(x) length(x) == 0)),]
  }
  
  dataset$cod_municipio <- municipios[unlist(st_intersects(dataset,municipios)),]$LAU_CODE
  
  dataset <- dataset |> 
    left_join(dens_poblacion,
              join_by(cod_municipio==CODIGO_INE3,YEAR==Anual)) |> 
    rename("municipio" = "Municipio",
           "poblacion" = "Poblacion")
  
  # Vegetación ----------------------------------------
  cat("Asignando variables de vegetación...\n")
  
  ### NDVI ----
  dataset$NDVI = NA
  
  for (YEAR in 2002:2022) {
    for (MONTH in 1:12) {
      MM = str_pad(MONTH,2,"left",pad = "0")
      YY = substr(as.character(YEAR),3,4)
  
      if (as.numeric(YY)<=06) {
        ruta <- paste0("data_raw/vegetacion/",YEAR,
                       "TERMODMEDMNDVI/InfGeografica/
                       InfRaster/TIFF/TERMOD_",
                       YY,MM,"01_h17v05_medmndvi.tif")
      } else if (as.numeric(YY)<=11) {
        ruta <- paste0("data_raw/vegetacion/",YEAR,
                       "TERMODMEDMNDVI/InfGeografica/
                       InfRaster/TIF/TERMOD_",
                       YY,MM,"01_h17v05_medmndvi.tif")
      } else if (as.numeric(YY)<=21) {
        ruta <- paste0("data_raw/vegetacion/",YEAR,
                       "TERMODMEDMNDVI/InfGeografica/
                       InfRaster/TIFF/termod_",
                       YY,MM,"01_h17v05_medmndvi.tif")
      }else {
        ruta <- paste0("data_raw/vegetacion/",YEAR,
                       "TERMODMEDMNDVI/InfGeografica/
                       InfRaster/COG/termod_",
                       YY,MM,"01_h17v05_medmndvi_COG.tif")
      }
      
      if (file.exists(ruta)) {
        cat(YEAR,MONTH,"\n")
        # Observaciones en ese mes y año
        isMY = dataset$YEAR==YEAR & dataset$MM==MONTH
        if (any(isMY)) {
          NDVI_rast = as.numeric(rast(ruta))
          if (MONTH==4 & YEAR==2011){
            # Ese archivo viene defectuoso y se le asigna el CRS de los otros archivos del mismo año (todos los demás del año tienen el mismo)
            crs(NDVI_rast) = crs(rast(
              "data_raw/vegetacion/2011TERMODMEDMNDVI/InfGeografica/
              InfRaster/TIF/TERMOD_110501_h17v05_medmndvi.tif"))
              } 
          dataset[isMY,]$NDVI = terra::extract(NDVI_rast,dataset[isMY,])[,2]
        }
      } else
        cat("No existe: ",YEAR,"-",MONTH,"\n")
    } 
  }
  
  
  # Factores ------------------------------------------
  # Codificación de las variables categóricas como factores:
  
  dataset <- dataset |> 
    mutate(enp = as.factor(enp),
           orientacion = cut(orientacion,
                             breaks = c(-Inf, -1, 22.5, 67.5, 112.5, 157.5, 202.5, 247.5, 292.5, 337.5, 360),
                             labels = c("Plano", "N", "NE", "E", "SE", "S", "SW", "W", "NW", "N")),
           WD10M = cut(WD10M,
                       breaks = c(0, 22.5, 67.5, 112.5, 157.5, 202.5, 247.5, 292.5, 337.5, 360),
                       labels = c("N", "NE", "E", "SE", "S", "SW", "W", "NW", "N")),
           uso_suelo = uso_suelo |> 
             as.character() |> 
             str_sub(0,2) |> 
             as.factor()
           )  |>  
    select(-c(YEAR,MM,DD))
  
  return(dataset)
}
```

Se usa la función definida para asignar las variables explicativas a la muestra generada:
```{r eval=FALSE}
# Se carga la muestra generada en el paso anterior:
load("salidas_intermedias/sample_strat_2024-04-26.RData")

# Se eliminan las observaciones que no tienen fecha pues no se pueden usar para el estudio
sample <- na.omit(sample)

# Se aplica la función a la muestra 
dataset <- asignar_variables(sample)

# Se almacenan los resultados
save(dataset,
     file = paste0("salidas_intermedias/dataset_strat_completo",
                   Sys.Date(),".RData"))

# Eliminación de casos faltantes --------------------------------------------
datos <- dataset |> 
  mutate(fire = as.factor(fire)) |> 
  drop_na()
```

La eliminación de los datos faltantes no se realizó inmediatamente, previamente se llevó a cabo un estudio exhaustivo de los valores perdidos y se valoraron otras opciones. No se incluye aquí por no alargar el apéndice, aunque puede revisarse en la sección "Depuración de la Muestra" del código adjunto. 


## Modelos

Esta sección es muy extensa, dada la cantidad de modelos que se construyen. Para evitar extender demasiado el apéndice, tan solo se mostrará aquí el código de algunos de los modelos construidos, con el objetivo de ilustrar el uso de las funciones de la librería *tidymodels* y el flujo de trabajo seguido para construir los modelos. 

Se incluye la definición de dos funciones propias usadas para la evaluación de los modelos `get_metrics` y `tuning_plot`. La primera usada para obtener las métricas de rendimiento de los modelos, y la segunda para construir gráficos como la Figura \ref{fig:lr_tuningplot}.

```{r}
# Librerías -----------------------------------------
# Se cargan las librerías que se usarán en esta sección
library(tidyverse) # Manipulación de datos 
library(sf) # Vector data
library(tidymodels) # Ecosistema para la construcción de modelos
library(akima) # Función interp
library(magrittr) # Operador %<>% 
library(ggpubr) # Función ggarrange
library(knitr) # Función kable

# Carga de datos ------------------------------------ 
load("salidas_intermedias/datos_strat_depurados_geom_2024-05-03.RData")

# Agrupación clases uso_suelo -----------------------
# Nos quedamos con los 7 niveles del factor más frecuentes (clases 2 y 3)
datos <- datos |> 
  mutate(uso_suelo = fct_lump(uso_suelo,
                              n = 7,
                              other_level= "Otro"))

# Funciones para la evaluación de modelos ------------
# Función para obtener las medidas de rendimiento de los modelos a partir de un objeto predict 
get_metrics <- function(pred) {
  list(
    res = tibble(
      roc_auc = pred |> 
        roc_auc(truth = fire, .pred_0) |> 
        pull(.estimate),
      accuracy = pred |> 
        accuracy(truth = fire, .pred_class) |> 
        pull(.estimate),
      recall = pred |> 
        sensitivity(truth = fire, .pred_class, 
                    event_level="second") |> 
        pull(.estimate),
      specificity = pred |> 
        spec(truth = fire, .pred_class, 
             event_level="second") |> 
        pull(.estimate),
      precision = pred |> 
        precision(truth = fire, .pred_class,
                  event_level="second") |> 
        pull(.estimate)),
    conf_mat = pred |> conf_mat(truth = fire, .pred_class))
}

# Función para mostrar gráficamente los resultados del tuning de un modelo con dos parámetros 

tuning_plot = function(mod_res) {
  datos_metrics = mod_res %>% 
    collect_metrics()

  plots = list()
  
  for (metric in unique(datos_metrics$.metric)) {
    
    datos = datos_metrics %>% 
      filter(.metric==metric) 
    
    # Interpolar los datos faltantes
    datos_interp <- interp(datos[[1]], datos[[2]], datos$mean)
    
    # Crear un nuevo dataframe con los datos interpolados
    datos_interp_df <- data.frame(
      expand.grid(x = datos_interp$x, y = datos_interp$y), z = as.vector(datos_interp$z))
    
    # Crear el gráfico de mapa de calor con interpolación
    p = ggplot(datos_interp_df, aes(x = x, y = y, fill = z)) +
      geom_tile() +
      scale_fill_viridis_c(option = "turbo", name = NULL,na.value = "transparent")+
      labs(title = "",
           x = colnames(datos)[1],
           y = colnames(datos)[2],
           fill = metric) +
      theme_minimal()
    
    plots[[metric]] = p
  }
  ggarrange(plotlist = plots,
            labels=c("Accuracy","Specificy","ROC-AUC","Recall"),
            align = "hv")
}
```

### Partición temporal entrenamiento-validación-test

```{r}
set.seed(123) # Se fijan semillas para que sea reproducible

splits = initial_validation_time_split(datos, 
                                       prop=c(0.6,0.2))

training <- training(splits) %>%  st_drop_geometry()
val_set <- validation_set(splits) %>% st_drop_geometry()
test  <- testing(splits) %>% st_drop_geometry()
```

### Regresión Logística con penalización

Se ilustra el flujo de trabajo seguido en casi todos los modelos con el caso de la Regresión Logística con penalización.

```{r}
# 1º Definimos el modelo:
lr_mod <- 
  logistic_reg(penalty = tune(), mixture = tune()) %>% 
  set_engine("glmnet")

# 2º Creamos la receta
lr_recipe <- 
  recipe(fire ~ ., data = training) %>% 
  step_date(date,features = c("dow","month")) %>% # Se crean las variables día de la semana y mes
  step_rm(date,cod_municipio,municipio) %>% # Se eliminan variables identificadoras
  step_dummy(all_nominal_predictors()) %>% # Se crean variables dummy para los factores
  step_lincomb() %>% # Elimina variablies con dependencia lineal exacta
  step_corr() %>% # Elimina variables con correlación superior a 0.9
  step_zv(all_predictors()) %>% # Eliminar variables con varianza nula
  step_normalize(all_predictors()) # Se normalizan todos los predictores
# Si bien step_corr, step_lincomb y step_zv en este caso no tienen ningún efecto, se incluyen por ser una buena práctica para este modelo.


# 3º Creamos el workflow
lr_workflow <- 
  workflow() %>% 
  add_model(lr_mod) %>% 
  add_recipe(lr_recipe)

# 4º Creamos el grid para los parámetros
lr_reg_grid <- expand_grid(penalty = 10^seq(-4, -1, length.out = 10),
                           mixture = seq(0,1,length.out=10))

# 5º Ajustamos el modelo
lr_res <- 
  lr_workflow %>% 
  tune_grid(val_set,
            grid = lr_reg_grid,
            control = control_grid(save_pred = TRUE),
            metrics = metric_set(accuracy,roc_auc,recall,spec))

# 6º Evaluación de modelos
tuning_plot(lr_res)

lr_res |> 
  collect_metrics() |> 
  group_by(.metric)|> 
  mutate(.metric = ifelse(.metric == "recall","spec",
                          ifelse(.metric == "spec","recall",
                                 .metric))) |> 
  summarise(max = max(mean),min=min(mean))   
# Para los valores máximo y mínimo alcanzados en cada métrica


# 7º Selección del mejor modelo
lr_best <- 
  lr_res %>% 
  select_best(metric="accuracy")
lr_best

# Extraer coeficientes
lr_workflow %>% 
  finalize_workflow(lr_best) %>%
  fit(training) %>% 
  extract_fit_parsnip() %>% 
  tidy() %>% 
  print(n=100)
```


### Bosques Aleatorios

Se incluye el código usado para el Bosque Aleatorio pues tiene el interés de que el ajuste se realiza en dos etapas, a diferencia de todos los demás modelos.

```{r}
# Detectar el número de núcleos para trabajar en paralelo
cores <- parallel::detectCores()
cores

# Construimos el modelo especificando el número de núcleos a usar en la computación en paralelo, de forma que la computación sea más eficiente

# ETAPA 1: fijado mtry=4, se ajusta min_n
# -----------------

# 1º Construir el modelo
rf_mod1 <- 
  rand_forest(mtry = 4, min_n = tune(), trees = 1000) %>% 
  set_engine("ranger", num.threads = cores) %>% 
  set_mode("classification")

# 2º Construir la receta con el preprocesamiento
rf_recipe <- 
  recipe(fire ~ ., data = training) %>% 
  step_date(date,features = c("dow", "month")) %>% 
  # step_holiday(date) %>% 
  step_rm(date, cod_municipio, municipio) 
# No normalizamos en este caso pues no es necesario

# 3º Ensamblar todo con workflow
rf_workflow1 <- 
  workflow() %>% 
  add_model(rf_mod1) %>% 
  add_recipe(rf_recipe)

# 4º Train and tune
set.seed(345)

rf_res1 <-
  rf_workflow1 %>%
  tune_grid(val_set,
            grid = expand_grid(min_n = seq(1000,2500,100)),
            control = control_grid(save_pred = TRUE),
            metrics = metric_set(accuracy,roc_auc,recall,spec))

# Resultados del tuning

rf_tuning1 <- rf_res1 |> 
  collect_metrics() |> 
  group_by(.metric)|> 
  summarise(max = max(mean),min=min(mean))
rf_tuning1

# plot

rf_plot1 <- 
  rf_res1 %>% 
  collect_metrics() %>%  
  mutate(.metric = ifelse(.metric == "recall","spec",
                          ifelse(.metric == "spec","recall",
                                 .metric))) %>% 
  ggplot(aes(x = min_n, y = mean,col=.metric)) + 
  geom_point() + 
  geom_line() + 
  ylab("") +
  theme_minimal()+
  labs(title = "Etapa 1\nFijado mtry = 4, se ajusta min_n")


# Mejor modelo
rf_best1 <- 
  rf_res1 %>% 
  select_best(metric = "spec")
rf_best1

rf_metrics1 <- rf_res1 |> 
  collect_predictions(parameters = rf_best1) |> 
  get_metrics()
rf_metrics1


# ETAPA 2: fijado min_n de la etapa anterior, se ajusta mtry
# -----------------

# 1º Se construye el modelo
rf_mod2 <- 
  rand_forest(mtry = tune(), min_n = rf_best1$min_n, trees = 1000) %>% 
  set_engine("ranger", num.threads = cores) %>% 
  set_mode("classification")

# 2º Se usa la misma receta que antes

# 3º Ensamblar todo con workflow
rf_workflow2 <- 
  workflow() %>% 
  add_model(rf_mod2) %>% 
  add_recipe(rf_recipe)

# 4º Train and tune
set.seed(345)

rf_res2 <-
  rf_workflow2 %>%
  tune_grid(val_set,
            grid = expand_grid(mtry = 1:10),
            control = control_grid(save_pred = TRUE),
            metrics = metric_set(accuracy,roc_auc,recall,spec))


# Resultados del tuning
rf_tuning2 <- rf_res2 |> 
  collect_metrics() |> 
  group_by(.metric)|> 
  summarise(max = max(mean),min=min(mean))

rf_tuning2

# plot
rf_plot2 <- 
  rf_res2 %>% 
  collect_metrics() %>%  
  mutate(.metric = ifelse(.metric == "recall","spec",
                          ifelse(.metric == "spec","recall",
                                 .metric))) %>% 
  ggplot(aes(x = mtry, y = mean,col=.metric)) + 
  geom_point() + 
  geom_line() + 
  ylab("") +
  theme_minimal()+
  labs(title = paste0("Etapa 2\nFijado min_n = ", rf_best1$min_n, " se ajusta mtry"))

# Mejor modelo
rf_best2 <- 
  rf_res2 %>% 
  select_best(metric = "spec")
rf_best2

rf_metrics2 <- rf_res2 |> 
  collect_predictions(parameters = rf_best2) |> 
  get_metrics()

rf_metrics2

# ---------

# Plots
ggarrange(rf_plot1,rf_plot2,nrow=1,common.legend = T,legend = "bottom")
```


### Comparativa en validación

```{r eval=FALSE}
models = tibble(model_name = c("lr","lr_pca","dt","rf","svm_linear","svm_rbf","knn"),
                models_tune = list(lr_res, lr_pca_res, dt_res, rf_res2, svm_res, svm_rbf_res, knn_res),
                models_workflow = list(lr_workflow, lr_pca_workflow, dt_workflow, rf_workflow2, svm_workflow, svm_rbf_workflow, knn_workflow))

# save(models, file="salidas_intermedias/all_models.RData")

load("salidas_intermedias/all_models.RData")
models = models %>% 
  mutate(best_tuning = map(models_tune, 
                           function(x) select_best(x, metric = "accuracy")),
         best_metrics = map2(models_tune,
                             best_tuning,       
                             ~ collect_predictions(.x,parameters = .y) %>%    
                               get_metrics() %>%          
                               extract2(1)), # Para extraer solo las medidas y no la matriz de confusión
         roc = map2(models_tune,
                    best_tuning,
                    ~ collect_predictions(.x,parameters = .y) %>%    
                      roc_curve(fire, .pred_0))
) 

# métricas
metrics = models %>% 
  select(model_name,best_metrics) %>% 
  unnest(best_metrics)
kable(metrics,digits=3) 

# curva roc
metrics %>% 
  pivot_longer(cols = c(roc_auc, accuracy, recall, specificity, precision),
               names_to = "metric") %>% 
  ggplot(aes(x = metric, y = value, group = model_name)) +
  geom_line(aes(col = model_name),size=1) +
  geom_point(aes(col = model_name),size=2.3) +
  scale_color_viridis_d(option="turbo") +
  geom_vline(xintercept=1:5, linetype="dotted") +
  labs(col = "Modelo", title = "Métricas sobre validación") +
  theme_minimal() +
  theme(axis.line.x = element_line(color="black", size = 1),
        axis.line.y = element_line(color="black", size = 1))
  
# plot medidas
models %>% select(model_name,roc) %>% unnest(roc) %>% 
  ggplot(aes(x = 1 - specificity, y = sensitivity, col = model_name)) + 
  geom_path(lwd = 1, alpha = 0.7) +
  geom_abline(lty = 3) + 
  coord_equal() + 
  scale_color_viridis_d(option="turbo") +
  labs(color="Modelo")+
  # scale_color_viridis_d(option = "turbo",name="Modelo") +
  theme_minimal() + 
  theme(axis.line.x = element_line(color="black", size = 1),
        axis.line.y = element_line(color="black", size = 1))+
  ggtitle("Curva ROC en validación")
```

### Comparativa en test

Se unen los conjuntos training y validation para entrenar el modelo final.
```{r eval=FALSE}
set.seed(345)
models = models %>% 
  mutate(final_workflow = map2(models_workflow, 
                               best_tuning, 
                               finalize_workflow),
         last_fit = map(final_workflow, 
                        function(x) last_fit(x,splits,add_validation_set=T)),
         test_metrics = map(last_fit,      
                            ~collect_predictions(.x) %>%  
                              get_metrics() %>%  
                              extract2(1)), # Para extraer solo las medidas
         test_roc = map(last_fit,
                        ~collect_predictions(.x) %>%     
                          roc_curve(fire, .pred_0)) 
         )

# save(models, file="salidas_intermedias/all_models_test.RData")

# metricas en test
test_metrics = models %>% 
  select(model_name, test_metrics) %>% 
  unnest(test_metrics)

kable(test_metrics,digits=3)

# plot
test_metrics %>% 
  pivot_longer(cols = c(roc_auc, accuracy, recall, specificity, precision),
               names_to = "metric") %>% 
  ggplot(aes(x = metric, y = value, group = model_name)) +
  geom_line(aes(col = model_name),size=1) +
  geom_point(aes(col = model_name),size=2.3) +
  scale_color_viridis_d(option="turbo") +
  geom_vline(xintercept=1:5, linetype="dotted") +
  labs(col = "Modelo", title = "Métricas sobre test") +
  theme_minimal() +
  theme(axis.line.x = element_line(color="black", size = 1),
        axis.line.y = element_line(color="black", size = 1))
  
# roc
models %>% 
  select(model_name,test_roc) %>% 
  unnest(test_roc) %>% 
  ggplot(aes(x = 1 - specificity, y = sensitivity, col = model_name)) + 
  geom_path(lwd = 1, alpha = 0.7) +
  geom_abline(lty = 3) + 
  coord_equal() + 
  scale_color_viridis_d(option="turbo") +
  labs(color="Modelo")+
  # scale_color_viridis_d(option = "turbo",name="Modelo") +
  theme_minimal() + 
  theme(axis.line.x = element_line(color="black", size = 1),
        axis.line.y = element_line(color="black", size = 1))+
  ggtitle("Curva ROC en test")
```

## Aplicación de los modelos

Solo se incluye el código del primer caso de aplicación de los modelos, ya que las técnicas usadas en el otro son similares. Puede revisarse el código completo en la sección del mismo nombre el código adjunto.

```{r eval=FALSE}
# Librerías -------------------------------------------
# Se cargan las librerías que se usarán en esta sección
library(tidyverse) # Manipulación de datos 
library(sf) # Vector data
library(ggpubr) # Función ggarrange
library(terra) # Raster data

# Carga de datos ----------------------------------------
load("salidas_intermedias/datos_strat_depurados_geom_2024-04-27.RData") 

# Polígono de Andalucía ---------------------------------
and <- esp_get_ccaa(ccaa = "Andalucía") %>% 
  st_transform(st_crs(datos))

```

### Visión general del desempeño del modelo

Se ilustra la construcción de la malla de puntos usada para el análisis. Para asignarle los valores de las variables predictoras se usa la función `asignar_variables`.

```{r}
# grid de puntos 10km x 10km de andalucía
grid = st_make_grid(and,
                    cellsize = c(10000,10000), 
                    what = "centers")[and]
sample = NULL

for (m in 1:12) {
  if (is.null(sample)){
    sample <- tibble(date = rep(ymd(paste("2022",m,"15",sep="/"))),
                     geometry = grid) %>% st_sf()
  } else
  sample <- sample %>% 
    bind_rows(tibble(date = rep(ymd(paste("2022",m,"15",sep="/"))),
                     geometry = grid))
}

source("scripts/strat/fun_asignar_variables.R")
full_grid  = asignar_variables(sample)
```


Se imputan los valores faltantes de NDVI asignándoles los del año anterior, ya que faltan los archivos de marzo y diciembre de 2022.
```{r}
# La siguiente función lee el archivo con el NDVI correspondiente a un mes y a un año dados (si está disponible)

read_NDVI = function(MM,YYYY) {
  MM = str_pad(as.character(MM),2,"left",pad = "0")
  YY = substr(as.character(YYYY),3,4)
  if (as.numeric(YY)<=06) {
    ruta <- paste0("data_raw/vegetacion/",YYYY,
                   "TERMODMEDMNDVI/InfGeografica/InfRaster/TIFF/TERMOD_",
                   YY,MM,"01_h17v05_medmndvi.tif")
  } else if (as.numeric(YY)<=11) {
    ruta <- paste0("data_raw/vegetacion/",YYYY,
                   "TERMODMEDMNDVI/InfGeografica/InfRaster/TIF/TERMOD_",
                   YY,MM,"01_h17v05_medmndvi.tif")
  } else if (as.numeric(YY)<=21){
    ruta <- paste0("data_raw/vegetacion/",YYYY,
                   "TERMODMEDMNDVI/InfGeografica/InfRaster/TIFF/termod_",
                   YY,MM,"01_h17v05_medmndvi.tif")
  }else {
    ruta <- paste0("data_raw/vegetacion/",YYYY,
                   "TERMODMEDMNDVI/InfGeografica/InfRaster/COG/termod_",
                   YY,MM,"01_h17v05_medmndvi_COG.tif")
  }

  if (file.exists(ruta)) {
    NDVI = rast(ruta)
  } else
    NDVI = NA
  return(NDVI)
}

# Los meses para los cuales no está disponible el archivo de NDVI:
year_month_missing_NDVI = c(
"2003-01",
"2003-04",
"2017-02",
"2018-11",
"2020-11",
"2021-12",
"2022-03",
"2022-12")

# Para cada observación para la que no está disponible el NDVI (porque la información de ese mes no está disponible), se obtiene el correspondiente al mismo mes del año anterior, si este está disponible, si no, el del año posterior:





missing_NDVI = full_grid |> 
  filter(is.na(NDVI)) |> 
  mutate(year = year(date),month = month(date)) |> 
  group_by(year,month) |> 
  filter(paste(year,str_pad(as.character(month),2,"left",pad = "0"),
               sep="-") %in%   year_month_missing_NDVI) |>  # Se filtra porque también hay observaciones que tienen NA en el NDVI no porque no exista el archivo, si no lo mismo que en ocasiones anteriores, porques discrepancias entre los límites de los polígonos
  nest() |>    
  mutate(NDVI_rast = map2(month,year-1,read_NDVI), # Se lee primero el del año anterior
         NDVI_rast = ifelse(is.na(unlist(NDVI_rast)),
                            map2(month,year+1,read_NDVI),
                            NDVI_rast), # Si no está disponible, se toma el del año posterior
         NDVI_nuevo = map2(NDVI_rast,data,~terra::extract(.x,.y)[,2])) |> 
  select(-NDVI_rast) |> 
  unnest(c(data,NDVI_nuevo)) |> 
  mutate(NDVI = NDVI_nuevo,.keep="unused")

ind_modificados = is.na(full_grid$NDVI) & (paste(year(full_grid$date), str_pad(as.character(month(full_grid$date)),2,"left",pad = "0"),sep="-") 
                                           %in% year_month_missing_NDVI) 
# Los elementos que han sido modificados

# Se asignan los valores imputados del NDVI:
full_grid[ind_modificados,]$NDVI = missing_NDVI$NDVI

# Resumen
datos %>%  st_drop_geometry %>% skim(.data_name = "datos")
```

Finalmente, se agrupan los niveles de uso de suelo
```{r}
full_grid <- full_grid |> 
  mutate(uso_suelo = fct_other(uso_suelo,
                               keep = c("21", "22", "23", "24", "31", "32", "33"),
                               other_level= "Otro"))

# save(full_grid,file = full_grid_meses_2022_processed.RData"
```

Se usará el modelo de regresión logística lasso final,
```{r}
load("Private/all_models_test.RData")

model <- models %>% filter(model_name=="lr")
# model <- models %>% filter(model_name=="svm_linear")
# model <- models %>% filter(model_name=="rf")

rm(models) # Es un archivo muy pesado, lo eliminamos de la memoria

# Predicciones
pred_class = model %>% 
  pull(last_fit) %>% 
  .[[1]] %>% 
  extract_workflow() %>% 
  predict(new_data = full_grid)
pred_probs = model %>% 
  pull(last_fit) %>% 
  .[[1]] %>% 
  extract_workflow() %>% 
  predict(new_data = full_grid,type="prob")
pred = cbind(full_grid,pred_class,pred_probs)

# Se cargan los incendios producidos en el año 2022
incendios22 <- st_read(paste0("./data_raw/incendios_2000-2022/incendios_",2022,".shp")) %>% 
  st_transform(st_crs(full_grid)) %>% 
  mutate(date=ymd(fecha_inic))

```

Se muestra el gráfico con la estimación de la probabilidad de incendio el día 15 de cada mes en todos los puntos. Se añaden los incendios producidos en cada mes del año 2022
```{r}
# Gráfico predicciones mes + Incendios producidos
ggplot(data = and) + 
  geom_sf() +
  geom_sf(data = pred, aes(color=.pred_1),alpha=0.8,size = 1.5) +
  facet_wrap(~month(date,label=TRUE)) + 
  scale_color_gradientn(colours = rainbow(5,rev=T),limits=c(0,1)) +
  # scale_color_gradient(low="blue", high="red")+
  guides(alpha = "none") + 
  theme_minimal() +
  theme(axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank(),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank()) +
  geom_sf(data = incendios22 %>% st_centroid, 
          color = "black", shape =24, size=1,fill = "red") + 
  labs(title = "Probabilidad de incendio estimada el día 15 de cada mes de 2022",
       subtitle = paste0("Modelo: ", model$model_name),
       color = "Probalidad\nde incendio\nestimada")
```



