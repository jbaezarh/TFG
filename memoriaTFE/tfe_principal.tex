\documentclass[12pt,a4paper,]{book}
\def\ifdoblecara{} %% set to true
\def\ifprincipal{} %% set to true
\def\ifcitapandoc{} %% set to true
\let\ifcitapandoc\undefined %% set to false
\usepackage{lmodern}
% sin fontmathfamily
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
%\usepackage{fixltx2e} % provides \textsubscript %PLLC
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage[margin=2.5cm]{geometry}
\usepackage{hyperref}
\hypersetup{unicode=true,
              pdfborder={0 0 0},
              breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
%
\usepackage[usenames,dvipsnames]{xcolor}  %new PLLC
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}


  \title{}
    \author{}
    \date{}
  

%%%%%%% inicio: latex_preambulo.tex PLLC


%% UTILIZA CODIFICACIÓN UTF-8
%% MODIFICARLO CONVENIENTEMENTE PARA USARLO CON OTRAS CODIFICACIONES


%\usepackage[spanish,es-nodecimaldot,es-noshorthands]{babel}
\usepackage[spanish,es-nodecimaldot,es-noshorthands,es-tabla]{babel}
% Ver: es-tabla (en: https://osl.ugr.es/CTAN/macros/latex/contrib/babel-contrib/spanish/spanish.pdf)
% es-tabla (en: https://tex.stackexchange.com/questions/80443/change-the-word-table-in-table-captions)
\usepackage[spanish, plain, datebegin,sortcompress,nocomment,
noabstract]{flexbib}
 
\usepackage{float}
\usepackage{placeins}
\usepackage{fancyhdr}
% Solucion: ! LaTeX Error: Command \counterwithout already defined.
% https://tex.stackexchange.com/questions/425600/latex-error-command-counterwithout-already-defined
\let\counterwithout\relax
\let\counterwithin\relax
\usepackage{chngcntr}
%\usepackage{microtype}  %antes en template PLLC
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc} % Usa codificación 8-bit que tiene 256 glyphs

%\usepackage[dvipsnames]{xcolor}
%\usepackage[usenames,dvipsnames]{xcolor}  %new
\usepackage{pdfpages}
%\usepackage{natbib}




% Para portada: latex_paginatitulo_mod_ST02.tex (inicio)
\usepackage{tikz}
\usepackage{epigraph}
\input{portadas/latex_paginatitulo_mod_ST02_add.sty}
% Para portada: latex_paginatitulo_mod_ST02.tex (fin)

% Para portada: latex_paginatitulo_mod_OV01.tex (inicio)
\usepackage{cpimod}
% Para portada: latex_paginatitulo_mod_OV01.tex (fin)

% Para portada: latex_paginatitulo_mod_OV03.tex (inicio)
\usepackage{KTHEEtitlepage}
% Para portada: latex_paginatitulo_mod_OV03.tex (fin)

\renewcommand{\contentsname}{Índice}
\renewcommand{\listfigurename}{Índice de figuras}
\renewcommand{\listtablename}{Índice de tablas}
\newcommand{\bcols}{}
\newcommand{\ecols}{}
\newcommand{\bcol}[1]{\begin{minipage}{#1\linewidth}}
\newcommand{\ecol}{\end{minipage}}
\newcommand{\balertblock}[1]{\begin{alertblock}{#1}}
\newcommand{\ealertblock}{\end{alertblock}}
\newcommand{\bitemize}{\begin{itemize}}
\newcommand{\eitemize}{\end{itemize}}
\newcommand{\benumerate}{\begin{enumerate}}
\newcommand{\eenumerate}{\end{enumerate}}
\newcommand{\saltopagina}{\newpage}
\newcommand{\bcenter}{\begin{center}}
\newcommand{\ecenter}{\end{center}}
\newcommand{\beproof}{\begin{proof}} %new
\newcommand{\eeproof}{\end{proof}} %new
%De: https://texblog.org/2007/11/07/headerfooter-in-latex-with-fancyhdr/
% \fancyhead
% E: Even page
% O: Odd page
% L: Left field
% C: Center field
% R: Right field
% H: Header
% F: Footer
%\fancyhead[CO,CE]{Resultados}

%OPCION 1
% \fancyhead[LE,RO]{\slshape \rightmark}
% \fancyhead[LO,RE]{\slshape \leftmark}
% \fancyfoot[C]{\thepage}
% \renewcommand{\headrulewidth}{0.4pt}
% \renewcommand{\footrulewidth}{0pt}

%OPCION 2
% \fancyhead[LE,RO]{\slshape \rightmark}
% \fancyfoot[LO,RE]{\slshape \leftmark}
% \fancyfoot[LE,RO]{\thepage}
% \renewcommand{\headrulewidth}{0.4pt}
% \renewcommand{\footrulewidth}{0.4pt}
%%%%%%%%%%
\usepackage{calc,amsfonts}
% Elimina la cabecera de páginas impares vacías al finalizar los capítulos
\usepackage{emptypage}
\makeatletter

%\definecolor{ocre}{RGB}{25,25,243} % Define el color azul (naranja) usado para resaltar algunas salidas
\definecolor{ocre}{RGB}{0,0,0} % Define el color a negro (aparece en los teoremas

%\usepackage{calc} 


%era if(csl-refs) con dolares
% metodobib: true


\usepackage{lipsum}

%\usepackage{tikz} % Requerido para dibujar formas personalizadas

%\usepackage{amsmath,amsthm,amssymb,amsfonts}
\usepackage{amsthm}


% Boxed/framed environments
\newtheoremstyle{ocrenumbox}% % Theorem style name
{0pt}% Space above
{0pt}% Space below
{\normalfont}% % Body font
{}% Indent amount
{\small\bf\sffamily\color{ocre}}% % Theorem head font
{\;}% Punctuation after theorem head
{0.25em}% Space after theorem head
{\small\sffamily\color{ocre}\thmname{#1}\nobreakspace\thmnumber{\@ifnotempty{#1}{}\@upn{#2}}% Theorem text (e.g. Theorem 2.1)
\thmnote{\nobreakspace\the\thm@notefont\sffamily\bfseries\color{black}---\nobreakspace#3.}} % Optional theorem note
\renewcommand{\qedsymbol}{$\blacksquare$}% Optional qed square

\newtheoremstyle{blacknumex}% Theorem style name
{5pt}% Space above
{5pt}% Space below
{\normalfont}% Body font
{} % Indent amount
{\small\bf\sffamily}% Theorem head font
{\;}% Punctuation after theorem head
{0.25em}% Space after theorem head
{\small\sffamily{\tiny\ensuremath{\blacksquare}}\nobreakspace\thmname{#1}\nobreakspace\thmnumber{\@ifnotempty{#1}{}\@upn{#2}}% Theorem text (e.g. Theorem 2.1)
\thmnote{\nobreakspace\the\thm@notefont\sffamily\bfseries---\nobreakspace#3.}}% Optional theorem note

\newtheoremstyle{blacknumbox} % Theorem style name
{0pt}% Space above
{0pt}% Space below
{\normalfont}% Body font
{}% Indent amount
{\small\bf\sffamily}% Theorem head font
{\;}% Punctuation after theorem head
{0.25em}% Space after theorem head
{\small\sffamily\thmname{#1}\nobreakspace\thmnumber{\@ifnotempty{#1}{}\@upn{#2}}% Theorem text (e.g. Theorem 2.1)
\thmnote{\nobreakspace\the\thm@notefont\sffamily\bfseries---\nobreakspace#3.}}% Optional theorem note

% Non-boxed/non-framed environments
\newtheoremstyle{ocrenum}% % Theorem style name
{5pt}% Space above
{5pt}% Space below
{\normalfont}% % Body font
{}% Indent amount
{\small\bf\sffamily\color{ocre}}% % Theorem head font
{\;}% Punctuation after theorem head
{0.25em}% Space after theorem head
{\small\sffamily\color{ocre}\thmname{#1}\nobreakspace\thmnumber{\@ifnotempty{#1}{}\@upn{#2}}% Theorem text (e.g. Theorem 2.1)
\thmnote{\nobreakspace\the\thm@notefont\sffamily\bfseries\color{black}---\nobreakspace#3.}} % Optional theorem note
\renewcommand{\qedsymbol}{$\blacksquare$}% Optional qed square
\makeatother



% Define el estilo texto theorem para cada tipo definido anteriormente
\newcounter{dummy} 
\numberwithin{dummy}{section}
\theoremstyle{ocrenumbox}
\newtheorem{theoremeT}[dummy]{Teorema}  % (Pedro: Theorem)
\newtheorem{problem}{Problema}[chapter]  % (Pedro: Problem)
\newtheorem{exerciseT}{Ejercicio}[chapter] % (Pedro: Exercise)
\theoremstyle{blacknumex}
\newtheorem{exampleT}{Ejemplo}[chapter] % (Pedro: Example)
\theoremstyle{blacknumbox}
\newtheorem{vocabulary}{Vocabulario}[chapter]  % (Pedro: Vocabulary)
\newtheorem{definitionT}{Definición}[section]  % (Pedro: Definition)
\newtheorem{corollaryT}[dummy]{Corolario}  % (Pedro: Corollary)
\theoremstyle{ocrenum}
\newtheorem{proposition}[dummy]{Proposición} % (Pedro: Proposition)


\usepackage[framemethod=default]{mdframed}



\newcommand{\intoo}[2]{\mathopen{]}#1\,;#2\mathclose{[}}
\newcommand{\ud}{\mathop{\mathrm{{}d}}\mathopen{}}
\newcommand{\intff}[2]{\mathopen{[}#1\,;#2\mathclose{]}}
\newtheorem{notation}{Notation}[chapter]


\mdfdefinestyle{exampledefault}{%
rightline=true,innerleftmargin=10,innerrightmargin=10,
frametitlerule=true,frametitlerulecolor=green,
frametitlebackgroundcolor=yellow,
frametitlerulewidth=2pt}


% Theorem box
\newmdenv[skipabove=7pt,
skipbelow=7pt,
backgroundcolor=black!5,
linecolor=ocre,
innerleftmargin=5pt,
innerrightmargin=5pt,
innertopmargin=10pt,%5pt
leftmargin=0cm,
rightmargin=0cm,
innerbottommargin=5pt]{tBox}

% Exercise box	  
\newmdenv[skipabove=7pt,
skipbelow=7pt,
rightline=false,
leftline=true,
topline=false,
bottomline=false,
backgroundcolor=ocre!10,
linecolor=ocre,
innerleftmargin=5pt,
innerrightmargin=5pt,
innertopmargin=10pt,%5pt
innerbottommargin=5pt,
leftmargin=0cm,
rightmargin=0cm,
linewidth=4pt]{eBox}	

% Definition box
\newmdenv[skipabove=7pt,
skipbelow=7pt,
rightline=false,
leftline=true,
topline=false,
bottomline=false,
linecolor=ocre,
innerleftmargin=5pt,
innerrightmargin=5pt,
innertopmargin=10pt,%0pt
leftmargin=0cm,
rightmargin=0cm,
linewidth=4pt,
innerbottommargin=0pt]{dBox}	

% Corollary box
\newmdenv[skipabove=7pt,
skipbelow=7pt,
rightline=false,
leftline=true,
topline=false,
bottomline=false,
linecolor=gray,
backgroundcolor=black!5,
innerleftmargin=5pt,
innerrightmargin=5pt,
innertopmargin=10pt,%5pt
leftmargin=0cm,
rightmargin=0cm,
linewidth=4pt,
innerbottommargin=5pt]{cBox}

% Crea un entorno para cada tipo de theorem y le asigna un estilo 
% con ayuda de las cajas coloreadas anteriores
\newenvironment{theorem}{\begin{tBox}\begin{theoremeT}}{\end{theoremeT}\end{tBox}}
\newenvironment{exercise}{\begin{eBox}\begin{exerciseT}}{\hfill{\color{ocre}\tiny\ensuremath{\blacksquare}}\end{exerciseT}\end{eBox}}				  
\newenvironment{definition}{\begin{dBox}\begin{definitionT}}{\end{definitionT}\end{dBox}}	
\newenvironment{example}{\begin{exampleT}}{\hfill{\tiny\ensuremath{\blacksquare}}\end{exampleT}}		
\newenvironment{corollary}{\begin{cBox}\begin{corollaryT}}{\end{corollaryT}\end{cBox}}	

%	ENVIRONMENT remark
\newenvironment{remark}{\par\vspace{10pt}\small 
% Espacio blanco vertical sobre la nota y tamaño de fuente menor
\begin{list}{}{
\leftmargin=35pt % Indentación sobre la izquierda
\rightmargin=25pt}\item\ignorespaces % Indentación sobre la derecha
\makebox[-2.5pt]{\begin{tikzpicture}[overlay]
\node[draw=ocre!60,line width=1pt,circle,fill=ocre!25,font=\sffamily\bfseries,inner sep=2pt,outer sep=0pt] at (-15pt,0pt){\textcolor{ocre}{N}}; \end{tikzpicture}} % R naranja en un círculo (Pedro)
\advance\baselineskip -1pt}{\end{list}\vskip5pt} 
% Espaciado de línea más estrecho y espacio en blanco después del comentario


\newenvironment{solutionExe}{\par\vspace{10pt}\small 
\begin{list}{}{
\leftmargin=35pt 
\rightmargin=25pt}\item\ignorespaces 
\makebox[-2.5pt]{\begin{tikzpicture}[overlay]
\node[draw=ocre!60,line width=1pt,circle,fill=ocre!25,font=\sffamily\bfseries,inner sep=2pt,outer sep=0pt] at (-15pt,0pt){\textcolor{ocre}{S}}; \end{tikzpicture}} 
\advance\baselineskip -1pt}{\end{list}\vskip5pt} 

\newenvironment{solutionExa}{\par\vspace{10pt}\small 
\begin{list}{}{
\leftmargin=35pt 
\rightmargin=25pt}\item\ignorespaces 
\makebox[-2.5pt]{\begin{tikzpicture}[overlay]
\node[draw=ocre!60,line width=1pt,circle,fill=ocre!55,font=\sffamily\bfseries,inner sep=2pt,outer sep=0pt] at (-15pt,0pt){\textcolor{ocre}{S}}; \end{tikzpicture}} 
\advance\baselineskip -1pt}{\end{list}\vskip5pt} 

\usepackage{tcolorbox}

\usetikzlibrary{trees}

\theoremstyle{ocrenum}
\newtheorem{solutionT}[dummy]{Solución}  % (Pedro: Corollary)
\newenvironment{solution}{\begin{cBox}\begin{solutionT}}{\end{solutionT}\end{cBox}}	


\newcommand{\tcolorboxsolucion}[2]{%
\begin{tcolorbox}[colback=green!5!white,colframe=green!75!black,title=#1] 
 #2
 %\tcblower  % pone una línea discontinua
\end{tcolorbox}
}% final definición comando

\newtcbox{\mybox}[1][green]{on line,
arc=0pt,outer arc=0pt,colback=#1!10!white,colframe=#1!50!black, boxsep=0pt,left=1pt,right=1pt,top=2pt,bottom=2pt, boxrule=0pt,bottomrule=1pt,toprule=1pt}



\mdfdefinestyle{exampledefault}{%
rightline=true,innerleftmargin=10,innerrightmargin=10,
frametitlerule=true,frametitlerulecolor=green,
frametitlebackgroundcolor=yellow,
frametitlerulewidth=2pt}





\newcommand{\betheorem}{\begin{theorem}}
\newcommand{\eetheorem}{\end{theorem}}
\newcommand{\bedefinition}{\begin{definition}}
\newcommand{\eedefinition}{\end{definition}}

\newcommand{\beremark}{\begin{remark}}
\newcommand{\eeremark}{\end{remark}}
\newcommand{\beexercise}{\begin{exercise}}
\newcommand{\eeexercise}{\end{exercise}}
\newcommand{\beexample}{\begin{example}}
\newcommand{\eeexample}{\end{example}}
\newcommand{\becorollary}{\begin{corollary}}
\newcommand{\eecorollary}{\end{corollary}}


\newcommand{\besolutionExe}{\begin{solutionExe}}
\newcommand{\eesolutionExe}{\end{solutionExe}}
\newcommand{\besolutionExa}{\begin{solutionExa}}
\newcommand{\eesolutionExa}{\end{solutionExa}}


%%%%%%%%


% Caja Salida Markdown
\newmdenv[skipabove=7pt,
skipbelow=7pt,
rightline=false,
leftline=true,
topline=false,
bottomline=false,
backgroundcolor=GreenYellow!10,
linecolor=GreenYellow!80,
innerleftmargin=5pt,
innerrightmargin=5pt,
innertopmargin=10pt,%5pt
innerbottommargin=5pt,
leftmargin=0cm,
rightmargin=0cm,
linewidth=4pt]{mBox}	

%% RMarkdown
\newenvironment{markdownsal}{\begin{mBox}}{\end{mBox}}	

\newcommand{\bmarkdownsal}{\begin{markdownsal}}
\newcommand{\emarkdownsal}{\end{markdownsal}}


\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{subfig} %new
%\usepackage{booktabs,dcolumn,rotating,thumbpdf,longtable}
\usepackage{dcolumn,rotating}  %new
\usepackage[graphicx]{realboxes} %new de: https://stackoverflow.com/questions/51633434/prevent-pagebreak-in-kableextra-landscape-table

%define el interlineado vertical
%\renewcommand{\baselinestretch}{1.5}

%define etiqueta para las Tablas o Cuadros
%\renewcommand\spanishtablename{Tabla}

%%\bibliographystyle{plain} %new no necesario


%%%%%%%%%%%% PARA USO CON biblatex
% \DefineBibliographyStrings{english}{%
%   backrefpage = {ver pag.\adddot},%
%   backrefpages = {ver pags.\adddot}%
% }

% \DefineBibliographyStrings{spanish}{%
%   backrefpage = {ver pag.\adddot},%
%   backrefpages = {ver pags.\adddot}%
% }
% 
% \DeclareFieldFormat{pagerefformat}{\mkbibparens{{\color{red}\mkbibemph{#1}}}}
% \renewbibmacro*{pageref}{%
%   \iflistundef{pageref}
%     {}
%     {\printtext[pagerefformat]{%
%        \ifnumgreater{\value{pageref}}{1}
%          {\bibstring{backrefpages}\ppspace}
%          {\bibstring{backrefpage}\ppspace}%
%        \printlist[pageref][-\value{listtotal}]{pageref}}}}
% 
%%% de kableExtra
\usepackage{booktabs}
\usepackage{longtable}
%\usepackage{array}
%\usepackage{multirow}
%\usepackage{wrapfig}
%\usepackage{float}
%\usepackage{colortbl}
%\usepackage{pdflscape}
%\usepackage{tabu}
%\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
%\usepackage{xcolor}

%%%%%%% fin: latex_preambulo.tex PLLC


\begin{document}

\bibliographystyle{flexbib}


% nada
\begin{titlepage}

\newcommand{\HRule}{\rule{\linewidth}{0.5mm}} % Defines a new command for the horizontal lines, change thickness here

\center % Center everything on the page


\begin{minipage}{14cm}
%----------------------------------------------------------------------------------------
%  LOGO SECTION
%----------------------------------------------------------------------------------------
\center

\includegraphics[width=8cm,height=8cm]{logo}\\[0.5cm] % Include a department/university logo - this will require the graphicx package

%----------------------------------------------------------------------------------------

%----------------------------------------------------------------------------------------
%	HEADING SECTIONS
%----------------------------------------------------------------------------------------
\textsc{\LARGE Doble Grado en Matemáticas y Estadística}\\[2.5cm] 


%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\rule[1.7mm]{2cm}{0.5mm}
\hfill
\textsc{\Large TRABAJO FIN DE ESTUDIOS} 
\hfill
\rule[1.7mm]{2cm}{0.5mm} 
\\[0.75cm]

%\bfseries
{\Huge
\textbf{\textit{
Modelos de predicción de\\[0.2cm]
incendios forestales en\\[0.35cm]
Andalucía
% Modelos de predicción \\[0.2cm]
% de incendios forestales en Andalucía \\[0.5cm]
}}}\\[0.75cm] 

\HRule \\[4cm]


{\Large

Juan Baeza Ruiz-Henestrosa} \\[0.5cm]

{\large
Sevilla, Mayo de 2024
}

\end{minipage}

\vfill % Fill the rest of the page with whitespace

\cleardoublepage
%\newpage{\ }
\thispagestyle{empty}
\end{titlepage}

\raggedbottom




\setlength{\parindent}{1em}

\pagestyle{fancy}
\ifdefined\ifdoblecara
\fancyhead[LE,RO]{}
\fancyhead[LO,RE]{}
\else
\fancyhead[RO]{}
\fancyhead[LO]{}
\fi
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}
\pagenumbering{roman}

\setcounter{tocdepth}{4}
\subpdfbookmark{Índice General}{indice}
\tableofcontents

\cleardoublepage

\section*{Prólogo}

\addcontentsline{toc}{section}{Prólogo}

Escrito colocado al comienzo de una obra en el que se hacen comentarios
sobre la obra o su autor, o se introduce en su lectura; a menudo está
realizado por una persona distinta del autor.

También se podrían incluir aquí los agradecimientos.

\cleardoublepage

\section*{Resumen}
\addcontentsline{toc}{section}{Resumen}

Resumen\ldots{}

\clearpage
\section*{Abstract}
\addcontentsline{toc}{section}{Abstract}

Abstract\ldots{}

\cleardoublepage   
\listoffigures
\addcontentsline{toc}{section}{Índice de Figuras}

\cleardoublepage   
\listoftables

\addcontentsline{toc}{section}{Índice de Tablas}

\cleardoublepage

\pagenumbering{arabic}

\ifdefined\ifdoblecara
\fancyhead[LE,RO]{\scriptsize\rightmark}
\fancyfoot[LO,RE]{\scriptsize\slshape \leftmark}
\fancyfoot[C]{}
\fancyfoot[LE,RO]{\footnotesize\thepage}
\else
\fancyhead[RO]{\scriptsize\rightmark}
\fancyfoot[LO]{\scriptsize\slshape \leftmark}
\fancyfoot[C]{}
\fancyfoot[RO]{\footnotesize\thepage}
\fi

\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0.4pt}

\ifdefined\ifprincipal
\else
\setlength{\parindent}{1em}
\pagestyle{fancy}
\setcounter{tocdepth}{4}
\tableofcontents

\fi

\ifdefined\ifdoblecara
\fancyhead{}{}
\fancyhead[LE,RO]{\scriptsize\rightmark}
\fancyfoot[LO,RE]{\scriptsize\slshape \leftmark}
\fancyfoot[C]{}
\fancyfoot[LE,RO]{\footnotesize\thepage}
\else
\fancyhead{}{}
\fancyhead[RO]{\scriptsize\rightmark}
\fancyfoot[LO]{\scriptsize\slshape \leftmark}
\fancyfoot[C]{}
\fancyfoot[RO]{\footnotesize\thepage}
\fi

\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0.4pt}

\hypertarget{capuxedtulo-1-introducciuxf3n}{%
\chapter{Capítulo 1: Introducción}\label{capuxedtulo-1-introducciuxf3n}}

\hypertarget{introducciuxf3n}{%
\section{Introducción}\label{introducciuxf3n}}

\hypertarget{objetivos}{%
\section{Objetivos}\label{objetivos}}

El objetivo de esta investigación será construir modelos que permitan
predecir el riesgo de incendio forestal en la Comunidad Autónoma de
Andalucía.

Subobjetivos:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Construir un conjunto de datos que permita la realización de análisis
  y la posterior construcción de modelos de Machine Learning para la
  predicción de incendioa forestalwa en Andalucía a partir de un estudio
  previo del problema.
\item
  Modelizar el riesgo de de incendio forestal usando distintos
  algoritmos de ML y comparar sus resultados
\item
  Analizar potenciales casos de interés.
\end{enumerate}

\hypertarget{hipuxf3tesis}{%
\section{Hipótesis}\label{hipuxf3tesis}}

``Spatial Prediction of Wildfire Susceptibility Using Field Survey GPS
Data and Machine Learning Approaches, Omid Ghorbanzadeh, Khalil
Valizadeh Kamran, Thomas Blaschke, Jagannath Aryal, Amin Naboureh,
Jamshid Einali and Jinhu Bian''

\begin{figure}
\centering
\includegraphics{D:/usuario/Documents/Universidad/5º/TFG - organizado/memoriaTFE/private/hipotesis_efecto_variables.png}
\caption{Spatial Prediction of Wildfire Susceptibility Using Field
Survey GPS Data and Machine Learning Approaches, Omid Ghorbanzadeh,
Khalil Valizadeh Kamran, Thomas Blaschke, Jagannath Aryal, Amin
Naboureh, Jamshid Einali and Jinhu Bian.}
\end{figure}

\hypertarget{revisiuxf3n-bibliogruxe1fica}{%
\section{Revisión bibliográfica}\label{revisiuxf3n-bibliogruxe1fica}}

--\textgreater{}

--\textgreater{}

--\textgreater{}

--\textgreater{}

\FloatBarrier

\ifdefined\ifprincipal
\else
\setlength{\parindent}{1em}
\pagestyle{fancy}
\setcounter{tocdepth}{4}
\tableofcontents

\fi

\ifdefined\ifdoblecara
\fancyhead{}{}
\fancyhead[LE,RO]{\scriptsize\rightmark}
\fancyfoot[LO,RE]{\scriptsize\slshape \leftmark}
\fancyfoot[C]{}
\fancyfoot[LE,RO]{\footnotesize\thepage}
\else
\fancyhead{}{}
\fancyhead[RO]{\scriptsize\rightmark}
\fancyfoot[LO]{\scriptsize\slshape \leftmark}
\fancyfoot[C]{}
\fancyfoot[RO]{\footnotesize\thepage}
\fi

\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0.4pt}

\hypertarget{preliminares}{%
\chapter{Preliminares}\label{preliminares}}

\hypertarget{datos-georreferenciados}{%
\section{Datos georreferenciados}\label{datos-georreferenciados}}

Todos los datos empleados en este trabajo son georreferenciados, lo que
significa que están asociados a ubicaciones geográficas específicas. Por
ello, resulta esencial introducir, aunque sea de forma general, los
tipos de datos más utilizados para trabajar con esta información, sus
características y las herramientas disponibles para manipularlos. Se
tratarán los datos vectoriales y los datos rasters, al ser los tipos de
datos fundamentales en este contexto, con características bien
diferenciadas entre ellos.

\hypertarget{datos-vectoriales}{%
\subsection{Datos Vectoriales}\label{datos-vectoriales}}

El modelo de datos vectoriales geográficos se basa en puntos ubicados
dentro de un sistema de referencia de coordenadas (CRS, por sus siglas
en inglés). Estos puntos pueden representar características
independientes o pueden estar conectados para formar geometrías más
complejas como líneas y polígonos.

\hypertarget{simple-features}{%
\subsubsection{Simple features}\label{simple-features}}

Las ``Simple features'' son un estándar abierto ampliamente usado para
la representación de datos vectoriales, desarrollado y respaldado por el
Open Geospatial Consortium (OGC, por sus siglas en inglés), una
organización sin ánimo de lucro dedicada a la creación de estándares
abiertos e interoperables a nivel global dentro del marco de los
sistemas geográficos de información (GIS, por sus siglas en ingés) y de
la World Wide Web.

El paquete sf proporciona clases para datos vectoriales geográficos y
una interfaz de línea de comandos consistente para importantes
bibliotecas de bajo nivel para geoprocesamiento (GDAL, PROJ, GEOS,
S2,\ldots).

Los objetos sf son fáciles de manipular ya que son dataframes o tibbles
con dos características fundamentales En primer lugar, contienen
metadatos geográficos adicionales: tipo de geometríca, dimensión,
``Bounding Box'' (límites o extensión geográfica) e información sobre el
Sistema de referencia de coordenadas. Y además, presentan una columna de
geometrías que tiene el nombre de ``geom''. Algunas ventajas del uso del
modelo de ``simple features'' en R son que en la mayoría de operaciones
los objeto sf se pueden tratar como data frames, los nombres de las
funciones son consistentes (todos empiezan por st\_), las funciones se
pueden combinar con el operador tubería y además funcionan bien con el
ecosistema de paquetes tidyverse.

El paquete sf de R soporta 18 tipos de geometrías para las simple
features, de las cuales las más utilizadas son: POINT, LINESTRING,
POLYGON, MULTIPOINT, MULTILINESTRING, MULTIPOLYGON and
GEOMETRYCOLLECTION.

\hypertarget{datos-raster}{%
\subsection{Datos Raster}\label{datos-raster}}

El modelo de datos raster representa el espacio con una cuadrícula de
celdas (también llamadas píxeles), que generalmente es regular, es
decir, con todas las celdas de igual tamaño. Aunque no se tratarán en el
presente trabajo, cabe mencionar que existen otros modelos de raster más
complejos en los que se usan cuadrículas irregulares (rotadas,
truncadas, rectilíneas o curvilíneas) y que pueden manipularse con el
paquete de R
(stars){[}\url{https://cran.r-project.org/web/packages/stars/index.html}{]}.
A cada una de estas celdas se le asocia uno (rasters de una sola capa) o
varios (rasters multicapa).

Los datos en formato raster constan de una cabecera y una matriz cuyos
elementos representan celdas equipespaciadas. En la cabecera del raster
se definen el Sistema de referencia de coordenadas, la extensión (o
límites espaciales del área cubierta por el ráster), la resolución y el
origen. El origen son las coordenadas de uno de los píxeles del ráster,
que sirve de referencia para los demás, siendo generalmente utilizado el
de la esquina inferior izquierda (aunque el paquete TERRA usado en este
trabajo usa por defecto el de la esquina superior izquierda). La
resolución se calcula como:

\[ resolution = \frac{x_{max}-x_{min}}{ncol},\frac{y_{max}-y_{min}}{nrow} \]

La representación en forma de matriz evita tener que almacenar
explícitamente las coordenadas de cada una de las cuatro esquinas de
cada píxel, debiendo almacenar solamente las coordenadas de un punto (el
origen). Esto, unido a las operaciones del álgebra de mapas hacen que el
procesamiento de datos raster sea mucho más eficiente que el de datos
vectoriales.

Se usará el paquete TERRA para tratar los datos en formato ráster. Este
paquete permite tratar el modelo de rásters regulares con una o varias
capas a través de la clase de objetos \texttt{SpatRaster}. Sin embargo,
existen otras alternativas, como el paquete
(stars){[}\url{https://cran.r-project.org/web/packages/stars/index.html}{]},
que además de ser más potente, permite trabajar con rásters no regulares
y ofrece una mejor integración con el paquete sf y el entorno tidyverse.

\hypertarget{sistemas-de-referencia-de-coordenadas}{%
\subsection{Sistemas de Referencia de
Coordenadas}\label{sistemas-de-referencia-de-coordenadas}}

Intrínseco a cualquier modelo de datos espaciales está el concepto de
Sistema de referencia de coordenadas (CRS), que establece cómo la
geometría de los datos se relaciona con la superficie terrestre. Es
decir, es el nexo de unión entre el modelo de datos y la realidad, por
lo que juega un papel fundamental. Los CRS pueden ser de dos tipos:
geográficos o proyectados.

\hypertarget{sistemas-de-coordenadas-geogruxe1ficas}{%
\subsubsection{Sistemas de Coordenadas
Geográficas}\label{sistemas-de-coordenadas-geogruxe1ficas}}

Los sistemas de coordenadas geográficas (GCS por sus siglas en inglés)
identifican cada punto de la superficie terrestre utilizando la longitud
y la latitud. La longitud es la distancia angular al Meridiano de
Greenwich medida en la dirección Este-Oeste. La longitud es la distancia
angular al Ecuador medida en la dirección Sur-Norte.

Cualquier sistema de coordenadas geográficas se compone de tres
elementos: el elipsoide, el geoide y el datum. El primero es el
elipsoide (o esfera) utilizado para representar de forma simplificada la
superficie terrestre, sobre el que se supone que se encuentran los datos
y el que permitirá realizar mediciones. El segundo, el geoide, es el
modelo matemático que representa la verdadera forma de la Tierra, que no
es suave sino que presenta ondulaciones debidas a las fluctuaciones del
campo gravitatorio a lo largo de la superficie terrestre, que además
cambian a una amplia escala temporal. Y el tercero, el datum, indica
cómo se alinean el elipsoide y el geoide, es decir, cómo el modelo
matemático se ajusta a la realidad. Este puede ser local o geocéntrico,
en función de si el elipsoide se ajusta al geoide en un punto concreto
de la superficie terrestre o de si el el centro del elipsoide el que se
alinea con el centro de la Tierra. Ejemplos de datums geocéntricos
usados en este trabajo son:

\begin{itemize}
\tightlist
\item
  European Terrestrial Reference System 1989 (ETRS89), usado ampliamente
  en la Europa Occidental.
\item
  World Geodetic System 1984 (WGS84), usado a nivel global.
\end{itemize}

\hypertarget{sistemas-de-coordenadas-proyectadas}{%
\subsubsection{Sistemas de Coordenadas
Proyectadas}\label{sistemas-de-coordenadas-proyectadas}}

Un Sistema de Coordenadas Proyectadas (PCS por sus siglas en inglés) es
un sistema de referencia que permite identificar localizaciones
terrestres y realizar mediciones en una superficie plana, es decir, en
un mapa. Estos sistemas de coordenadas se basan en las coordenadas
cartesianas, por lo que tienen un origen, un eje X y un eje Y y usan una
unidad lineal de medida (en este trabajo, metro). Pasar de una
superficie elíptica (GCR) a una superficie plana (PCS) requiere de
transformaciones matemáticas apropiadas y siempre induce deformaciones
en los datos.

Al proyectar la superficie terrestre en una superficie plana siempre se
modifican algunas propiedades de los objetos, como el área, la
dirección, la distancia o la forma. Un PCS solo puede conservar alguna
de estas propiedades, por lo que es habitual clasificar los PCS en
función de la propiedad que mantienen: las proyecciones de igual área
preservan el área, las azimutales preservan la dirección, las
equidistantes preservan la distancia y las conformales preservan la
forma local. La mayoría de las proyecciones también se pueden clasificar
en planas, cilíndricas o cónicas en función de cómo se realiza la
proyección.

Un caso particular y ampliamente usado de PCS cilíndrico son los
Universe Transverse Mercator (UTM), en el los que se proyecta el
elipsoide sobre un cilindro tangente a este por las líneas de longitud
(los meridianos). De esta forma, se divide el globo en 60 zonas de 6º de
longitud, para cada una de las cuales existe un PCS UTM correspondiente
que está asociado al meridiano central. Se trata de proyecciones
conformales, por lo que preservan ángulos y formas en pequeñas regiones,
pero distorsionan distancias y áreas.

A lo largo de este trabajo se utilizará ampliamente el Sistema de
coordenadas proyectadas UTM30N (es habitual especificar el hemisferio
para evitar confusión en los valores del eje Y, ya que miden distancia
al ecuador, de ahí la N de hemisferio norte).

\hypertarget{anuxe1lisis-exploratorio-de-datos}{%
\section{Análisis exploratorio de
datos}\label{anuxe1lisis-exploratorio-de-datos}}

El análisis exploratorio de datos (EDA, por sus siglas en inglés), es
una parte fundamental de todo proyecto de Machine Learning y en general
de cualquier proyecto en el que se deba trabajar con datos de cualquier
procedencia para extraer de ellos conclusiones. Antes del procesamiento
de los datos es siempre necesario explorar, entender y evaluar la
calidad de estos, pues como indica la expresión inglesa \emph{garbage
in, garbage out}, si trabajamos con datos pobres, no podemos esperar
obtener buenos resultados con ellos.

El EDA hace referencia al conjunto de técnicas estadísticas con las que
se pretende explorar, describir y resumir la naturaleza de los datos,
comprender las relaciones existentes entre las distintas variables
presentes, identificar posibles errores o revelar posibles valores
atípicos, todo esto con el objetivo de maximizar nuestra compresión
sobre el conjunto de datos.

\hypertarget{depuraciuxf3n-de-los-datos}{%
\subsection{Depuración de los datos}\label{depuraciuxf3n-de-los-datos}}

La depuración de los datos o \emph{data cleaning} es el proceso de
detectar y corregir o eliminar datos incorrectos, corruptos, con formato
incorrecto, duplicados o incompletos dentro de un conjunto de datos.
Puede considerarse una fase dentro del EDA (como se sugiere en R4DS,
Wickman) o una fase previa a este.

Puede entenderse que el \emph{data cleaning} es el proceso de pasar de
\emph{raw data} o datos en bruto a datos técnicamente correctos y
finalmente a datos consistentes.

Entendemos por datos técnicamente correcto cuando cada valor pertenece a
una variable y está almacenado en el tipo que que le corresponde en base
al conocimiento del dominio del problema. Para ello se debe reajustar el
tipo de cada variable al que le corresponda en base al conocimiento que
se tenga sobre esta, codificando los valores en las clases adecuadas si
fuese necesario.

Decimos que un conjunto de datos es consistente cuando es técnicamente
correcto y adecuado para el análisis estadístico. Se trata, por tanto,
de datos que han eliminado, corregido o imputado los valores faltantes,
los valores especiales, los valores atípicos y los errores.

\hypertarget{pca}{%
\subsection{PCA}\label{pca}}

\hypertarget{modelos}{%
\section{Modelos}\label{modelos}}

El problema que se aborda en este trabajo se engloba dentro de lo que se
conoce como aprendizaje supervisado, ya que para cada observación del
conjunto de entrenamiento se conoce el valor de la variable objetivo (en
este caso si ha habido incendio o no). Más concretamente, se trata de un
problema de clasificación binaria, ya que el objetivo es asignar cada
observación a una de las dos clases posibles (incendio o no incendio).
Existen numerosas técnicas de clasificación binaria supervisada, en este
trabajo se explorarán algunas de las de uso más común en problemas
similares.

\hypertarget{regresiuxf3n-loguxedstica-con-penalizaciuxf3n}{%
\subsection{Regresión logística (con
penalización)}\label{regresiuxf3n-loguxedstica-con-penalizaciuxf3n}}

La regresión logística es un caso particular de modelo lineal
generalizado basado en las siguientes hipótesis: - Hipótesis
distribucional. Dadas las variables explicativas,
\(\underline X_i \space \space \text{con} \space \space i = 1,2,...,n\),
se verifica que las variables \(Y|_{\underline X= \underline x_i}\) y su
distribución pertenece a la famila Bernouilli, es decir,

\[Y|_{\underline X= \underline x_i} \sim Be(\pi( \underline x_i))\]

\begin{itemize}
\tightlist
\item
  Hipótesis estructural. La esperanzara
  \(E(Y|_{\underline X= \underline x_i}) = \pi_i\) está relacionada con
  un predictor lineal (\(\eta_i = \beta^t z_i\)) a través de la función
  logit (con \(\underline z_i = \left(1,\underline x_i\right)\)) Es
  decir, dado que
  \[\eta_i = \underline \beta^t \underline z_i= \ln(\frac{\pi_i}{1-\pi_i})\]
  O equivalentemente,
  \[\pi_i = \frac{\exp(\underline \beta^t \underline z_i)}{1 + \exp(\underline \beta^t \underline z_i)}\]
\end{itemize}

Bajo estas hipótesis, la función de log-verosimilitud dada una muestra
\(\{ (\underline x_i,y_i) \}_{i=1,...,n}\) es:

\[ l(\underline \beta) = 
\sum_{i=1}^n \left[ 
y_i\ln \left( \frac{\pi_i}{1-\pi_i} \right) + 
\ln \left( 1 - \pi_i\right) \right]\]

En la regresión logística clásica se estima el vector de parámetros
\(\underline \beta\) maximizando la función de log-verosimilud, o lo que
es equivalente, minimizando su opuesta. Por tanto, el problema de
optimización a resolver será
\[\min_{\underline \beta} -l(\underline \beta)\]

Sin embargo, con el objetivo de evitar el sobreajuste y construir
modelos con mayor capacidad de generalización existen variaciones de la
regresión logística que incluyen un término de penalización en la
función objetivo. Las dos variantes de uso más extendido son la
regresión \emph{ridge} y \emph{lasso}.

Sea \(\underline \beta = \left( \beta_0, \underline \beta_1 \right)\),
donde \(\underline \beta_1\) contiene los coeficientes de las
covariables. En la regresión \emph{ridge} el término de penalización es
de la forma \(\| \underline \beta_1 \|^2_2\) mientras que en la
regresión \emph{lasso} el penalización es de la forma
\(\| \underline \beta_1 \|_1\). Por tanto, el problema de optimización
será

\[\min_{\underline \beta} -l(\underline \beta)  + \lambda \sum \beta_i^2 \]
en el caso de la regresión logística \emph{ridge}, y
\[\min_{\underline \beta} -l(\underline \beta)  + \lambda \sum |\beta_i|\]
en el caso de la regresión logística \emph{lasso}.

El paquete glmnet implementa una combinación de ambos métodos (llamada
\emph{elastic net}), en la que se añade un parámetro de mezcla
\(\alpha \in \left[0,1\right]\) que combina ambos enfoques. El problema
de optimización resultante es:

\[\min_{\underline \beta} -l(\underline \beta)  + \lambda \left[(1-\alpha)\sum \beta_i^2 + \alpha \sum |\beta_i| \right]\]

\hypertarget{support-vector-machine}{%
\subsection{Support Vector Machine}\label{support-vector-machine}}

Las Máquinas de Vector Soporte (SVM por sus siglas en inglés) son una
familia de modelos principalmente usados en problemas de clasificación
binaria (si bien se pueden extender a problemas de clasificación
multiclase o regresión) que parten de la idea de encontrar el hiperplano
que mejor separa al conjunto de puntos.

\hypertarget{svm-lineal}{%
\subsubsection{SVM lineal}\label{svm-lineal}}

Dada una muestra \(\left\{(\underline x_i,y_i) \right\}_{i=1,...,n}\)
con \(\underline x_i \in \mathbb{R}^d\) y \(y_i \in \{-1,1\}\) para todo
\(i \in \{1,...,n\}\), el objetivo es encontrar al hiperplano de la
forma
\[h(x) = w_1x_1 +w_2x_2+...+w_dx_d +b = \underline w^t \underline x = 0 \]
que mejor separe a la muestra.

Se dice que la muestra muestra es linealmente separable si existe un
hiperplano, denominado hiperplano de separación, que cumple, para todo
\(i \in 1,...,n\):
\[\underline w^t \underline x_i + b \ge 0 \space \space \text{si} \space \space y_i=+1\]
\[\underline w^t \underline x_i + b \le 0 \space \space \text{si} \space \space y_i=-1\]

Dado un hiperplano de separación de una muestra linealmente separable,
se define el margen como la menor de las distancias del hiperplano a
cualquier elemento de la muestra. Se denotará por \(\tau\).

Dado un punto \(\underline x_i\) y un hiperplano
\(h(x) = w_1x_1 +w_2x_2+...+w_dx_d +b = \underline w^t \underline x = 0\),
la distancia entre ambos viene dada por:
\[d(h,\underline x_i) = \frac{|h(\underline x_i)|}{\|w\|} = \frac{y_i(\underline w^t \underline x_i+b)}{\|w\|}\]
Donde \(\|\cdot\|\) hace referencia a la norma euclídea.

Dada una muestra linealmente separable
\(\left\{(\underline x_i,y_i) \right\}_{i=1,...,n}\) con
\(\underline x_i \in \mathbb{R}^d\) y \(y_i \in \{-1,1\}\) y un
hiperplano de separación \(h(x) = \underline w^t \underline x = 0\) con
margen \(\tau\), se verifica que
\[\frac{y_i(\underline w^t \underline x_i+b)}{\|w\|} \ge \tau \;\;\; \forall i\in \{1,...,n\}\]
O equivalentemente,
\[y_i(\underline w^t \underline x_i+b) \ge \tau\|w\| \;\;\; \forall i\in \{1,...,n\}\]
Y, además, es posible reescribir el mismo hiperplano \(h\) de forma que
\(\tau\|w\| = 1\).

De está ultima expresión se deduce que maximizar el margen \(\tau\) es
equivalente a minimizar la norma euclídea de \(w\). Por tanto, para
encontrar el hiperplano de separación óptimo para una muestra en las
condiciones de la proposición anterior basta resolver el problema de
optimización siguiente:

\begin{equation}
\begin{aligned}
\min_{w,b} \quad & \frac{1}{2}w^{t}w\\
\textrm{s.t.} \quad & \underline w^t \underline x_i+b \ge 1, \quad & \forall i\in \{1,...,n\} \\
  & w \in \mathbb{R}^d, \space b \in \mathbb{R} \\ 
\end{aligned}
\end{equation}

En general, las muestras no son separables, por lo que es necesario
permitir que pueda haber casos mal clasificados y penalizarlos
proporcionalmente a la distancia a la que se encuentren del subespacio
correcto (holgura). Para ello, se introducen en la formulación del
modelo las variables artificiales \(\xi_i,\quad i=1,...,n\). Se habla
entonces de hiperplano de separación \emph{soft margin}. De esta forma,
se llega al problema de optimización siguiente:

\begin{equation}
\begin{aligned}
\min_{w,b,\xi} \quad & \frac{1}{2}w^{t}w+C\sum_{i=1}^{n}{\xi_{i}}\\
\textrm{s.t.} \quad & \underline w^t \underline x_i+b \ge 1, \quad & \forall i\in \{1,...,n\}\\
  &\xi\geq0,   \quad & \forall i\in \{1,...,n\} \\
  & w \in \mathbb{R}^d, \space b \in \mathbb{R} \\
\end{aligned}
\end{equation}

donde \(C>0\) es un parámetro de regularización que permite controlar
los errores de clasificación permitidos por el modelo. Recibe el nombre
de coste (\emph{cost} en inglés).

\hypertarget{svm-no-lineal}{%
\subsubsection{SVM no lineal}\label{svm-no-lineal}}

Existen muchos casos en los que el SVM no es capaz de obtener buenos
resultados, debido a la estructura de la distribución de las clases en
la muestra. En estos casos, es común recurrir a una técnica llamada
\emph{kernel trick}. Esta técnica consiste en realizar una inmersión del
conjunto de los vectores de la muestra en un espacio de dimensión
superior (\emph{feature space}) en el que los casos sí sean separables
(o al menos mejore la separabilidad de estos). Esta inmersión en un
espacio de dimensión superior se hace indirectamente a través de
funciones \emph{kernel}, que calculan los productos escalares entre los
vectores de la muestra en el espacio de inmersión. Existen distintos
tipos de funciones \emph{kernel} que se corresponden con distintas
inmersiones en espacios de dimensión superior: - Kernel polinomial:
\(k(x,z) = \left( \gamma(x^tz + c_0) \right)^p\) - Kernel RDF (o
gaussiano): \(k(x,z) = \exp(-\gamma \| x-z\|^2)\)

\hypertarget{decision-trees}{%
\subsection{Decision Trees}\label{decision-trees}}

Un árbol de decisión (DT por sus siglas en inglés) es un algoritmo de
aprendizaje supervisado no paramétrico, que puede aplicarse tanto a
problemas de clasificación como de regresión. La idea de este método es
segmentar el espacio predictor en rectángulos, de forma que para
predecir una observación se usa la moda (o la media) de la región a la
que pertenece. Se trata de un modelo jerárquico con estructura de árbol,
que consta de un nodo raiz, ramas, nodos internos y nodos hojas. Cada
nodo representa una test sobre una variable, y cada rama que nace de ese
nodo uno de los posibles valores que puede tomar esa variable. De esta
forma, para clasificar una nueva instancia basta comenzar en el nodo
raíz e ir descendiendo por el árbol hasta llegar al nodo hoja
correspondiente, que indicará la clasificación asignada a dicha
instancia. La simplicidad del método muestra su principal ventaja, su
fácil comprensión dada su estructura de árbol.

Existen diversas técnicas para construir árboles de clasificación (y
regresión), la que aquí se plantea es una de las más usadas y recibe el
nombre de CART (Clasification And Regression Trees). Se explica para el
caso de árboles de clasificación binarios, i.e.~en los que de cada nodo
salen dos ramas.

Dada una muestra \(\left\{ (\underline x_i,y_i) \right\}\) con
\(\underline x_i = (x_{i1},...,x{id})\), un árbol de clasificación con
\(J\) hojas se puede expresar como
\[f(\underline x) = \sum_{j=1}^J c_j I(\underline x \in R_j)\] donde
\(\left\{ R_j\right\}_{j=1,...,J}\) es una partición del espacio
predictivo y \(c_j\) es la clase asignada en \(R_j\) para todo
\(j \in {1,...,J}\).

En la práctica, \(c_j\) se estima asignando la clase mayoritaria en el
recinto \(R_j\). Es decir,
\(\hat c_j = moda(\{y_i | \underline x_i \in R_j\})\).

Para construir un árbol de clasificación el algoritmo necesita decidir
las variables tests y los puntos de corte en cada nodo, así como la
topología del árbol. Para realizar esto, se vale de un método
\emph{greedy}, que en cada nodo elige la variable y el punto de corte
que mejor separa los datos en base a una medida de impureza. Es decir,
la construcción de un árbol de clasificación no se hace mediante la
resolución de un solo problema de optimización global, si no a partir de
la resolución de muchos problemas de optimización locales, con las
implicaciones que esto pueda tener.

Las medidas de impureza más comúnmente usadas son: - Error de
clasificación: \(1 - \max(p,1-p)\) - Índice de Gini: \(2p(1-p)\) -
Entropía: \(-p \log p - (1 − p) \log (1 - p)\) donde p es la proporción
de casos positivos en la muestra.

Así, el algoritmo de construcción de un árbol de clasiÖcación es: 1.
Comenzar con el nodo raíz, que incluye todos los casos. 2. Determinar el
par (variable,división) que conduce a una mayor reducción de la
impureza. Es decir, dada una medida de impureza \(\Phi\) se busca la
variable \(j \in {1,..,d}\) y la división \(s \in \mathbb{R}\) solución
de

\[\min_{j,s}\left[ 
\min_{c_i} \Phi \left(\{y_i |\underline x_i \in R_1(j,s)\} \right)  + 
\min_{c_i} \Phi \left(\{y_i |\underline x_i \in R_2(j,s)\} \right)\right]\]
donde \(R_1(j,s) = \{X | X_j \le s \}\) y
\(R_2(j,s) = \{X | X_j \gt s \}\).

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\item
  Aplicar iterativamente el proceso anterior a cada nuevo nodo, hasta
  que se verifiquen las condiciones de finalización. En este caso, el
  criterio será finalizar el proceso de división en el nodo una vez que
  el número de casos en este sea igual o inferior a una cantidad
  \(n_{min}\) fijada de antemano. En los nodos hoja se asigna la clase
  mayoritaria en el nodo.
\item
  Podar o recortar el árbol obtenido en base a un criterio de
  coste-complejidad. La idea fundamental es que dado un árbol completo
  \(T\) y un valor del parámetro de coste-complejidad \(\alpha\), se
  elije el subárbol \(T_0 \subset T\) obtenido a partir de \(T\)
  mediante poda, es decir, colapsando nodos no terminales, que minimice
  el criterio de coste complejidad definido como:
\end{enumerate}

\[C_{\alpha}(T) = \Phi(T) + \alpha|T_0|\] El parámetro \(\alpha\)
permite controlar la capacidad de generalización del modelo
(\emph{Bias-Variance tradeoff}) y se estima mediante Validación Cruzada.

El gran inconveniente de los árboles de decisión es que son modelos con
una varianza elevada, por lo que tienden a ser muy inestables y a
producir sobreajuste. Para evitar esto, se recurre al uso de técnicas de
\emph{Bagging} y \emph{Boosting}. El ejemplo más extendido con árboles
de decisión son los Bosques Aleatorios (\emph{Random Forest} en inglés).
\#\#\# Random Forest

\hypertarget{redes-neuronales}{%
\subsection{Redes Neuronales}\label{redes-neuronales}}

\hypertarget{validaciuxf3n-del-ajuste}{%
\section{Validación del ajuste}\label{validaciuxf3n-del-ajuste}}

Partición entrenamiento/ validación / test

\hypertarget{evaluaciuxf3n-modelos}{%
\section{Evaluación modelos}\label{evaluaciuxf3n-modelos}}

Una vez construido un modelo predictivo es necesario conocer el
rendimiento de este sobre nuevos datos, con el objetivo de estimar su
capacidad de generalización. Esto es fundamental de cara a determinar si
el modelo es adecuado para el propósito previsto o si necesita ajustes o
mejoras. Además, la evaluación del rendimiento permite comparar entre
diferentes modelos y seleccionar el que mejor se adapte a las
necesidades específicas del problema en cuestión. Para ello, se recurre
a distintas métricas, en función de las características propias de cada
problema.

\hypertarget{clasificaciuxf3n-binaria}{%
\subsection{Clasificación binaria}\label{clasificaciuxf3n-binaria}}

En el presente trabajo el problema que se aborda es un problema de
clasificación binaria, pues tenemos solo dos clases que son la clase
positiva y la clase negativa. A la hora de clasificar una nueva
instancia pueden darse 4 situaciones:

\begin{itemize}
\item
  Que se clasifique como positiva siendo realmente positiva, en cuyo
  caso se dirá que forma parte de las \emph{True Positives (TP)}
\item
  Que se clasifique como negativa siendo realmente negativa, en cuyo
  caso se dirá que forma parte de las \emph{True Negatives (TN)}
\item
  Que se clasifique como positiva siendo realmente negativa, en cuyo
  caso se dirá que forma parte de las \emph{False Positives (FP)}
\item
  Que se clasifique como negativa siendo realmente positiva, en cuyo
  caso se dirá que forma parte de las \emph{False Negatives (FN)}
\end{itemize}

Se definen las siguientes métricas de rendimiento de un modelo de
clasificación binaria:

\textbf{Tasa de acierto o exactitud}. Mide la proporción de casos que
han sido correctamente clasificados.
\[Exactitud = \frac{TP + TN}{TP + FP + TN + FN}\]

\textbf{Precisión}. Mide la proporción de casos clasificados como
positivos que realmente lo son. \[ Precisión = \frac{TP}{TP + FP}\]

\textbf{Especificidad}. Mide la proporción de casos negativos que han
sido correctamente clasificados por el modelo.
\[ Especificidad = \frac{TN}{TN + FP}\]

\textbf{Sensibilidad o recall}. Mide la proporción de casos positivos
que han sido correctamente clasificados por el modelo.
\[ Recall = \frac{TP}{TP + FN}\]

\textbf{AUC-ROC}. Mide el área bajo la curva ROC (\emph{Receiver
Operating Characteristic} o Característica Operativa del Receptor en
castellano). Esta curva es una representación gráfica del rendimiento de
un modelo de clasificación binaria para todos los umbrales de
clasificación.

\hypertarget{herramientas}{%
\section{Herramientas}\label{herramientas}}

Toda la parte práctica del presente trabajo se ha llevado a cabo
empleado el lenguaje de programación R a través del entorno de
desarrollo integrado (IDE) que ofrece RStudio. R es un lenguaje y
entorno de programación de código abierto desarrollado dentro del
proyecto GNU y orientado a la computación estadística. R puede extender
sus funcionalidades fácilmente a través de la gran cantidad de paquetes
disponibles dentro del repositorio de paquetes de CRAN (The
Comprehensive R Archive Network), siendo este uno de sus puntos fuertes,
dada la gran comunidad de usuarios y desarrolladores con las que cuenta
este lenguaje.

Los paquetes que se han utilizado han sido:

\begin{itemize}
\item
  tidyverse:

  • ggplot2, para la visualización. • dplyr, para la manipulación. •
  tidyr, para la ordenación. • readr, para la importación. • purrr, para
  la programación funcional
\item
  tidymodels
\item
  sf
\item
  terra
\item
  nasapower: obtención de información climática satelital
\item
  mapSpain:
\end{itemize}

\ldots{} (podemos seguir hasta el infinito)

\FloatBarrier

\ifdefined\ifprincipal
\else
\setlength{\parindent}{1em}
\pagestyle{fancy}
\setcounter{tocdepth}{4}
\tableofcontents

\fi

\ifdefined\ifdoblecara
\fancyhead{}{}
\fancyhead[LE,RO]{\scriptsize\rightmark}
\fancyfoot[LO,RE]{\scriptsize\slshape \leftmark}
\fancyfoot[C]{}
\fancyfoot[LE,RO]{\footnotesize\thepage}
\else
\fancyhead{}{}
\fancyhead[RO]{\scriptsize\rightmark}
\fancyfoot[LO]{\scriptsize\slshape \leftmark}
\fancyfoot[C]{}
\fancyfoot[RO]{\footnotesize\thepage}
\fi

\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0.4pt}

\hypertarget{construcciuxf3n-del-conjunto-de-datos}{%
\chapter{Construcción del conjunto de
datos}\label{construcciuxf3n-del-conjunto-de-datos}}

El primer paso a la hora de construir cualquier modelo de predicción es
disponer de datos adecuados que permitan explicar correctamente el
fenómeno en estudio, en este caso los incendios forestales en Andalucía.
Con este fin, se ha llevado a cabo un extenso estudio previo del dominio
del problema para conocer qué variables pueden ser relevantes de cara a
la predicción de incendios forestales, analizando estudios similares
realizados anteriormente así como otras fuentes relativas a la ecología
del fuego, que nos permitiesen conocer el efecto que cabría esperar de
estas variables.

Se ha querido adoptar un enfoque dinámico, es decir, el objetivo no es
construir un modelo estacionario que nos indique si una determinada zona
se verá afectada por un incendio forestal a lo largo de un amplio
periodo temporal, si no que se pretende ser capaz de predecir si un
determinado punto del territorio andaluz se verá afectado por un
incendio forestal en un momento concreto, en base a las covariables
correspondientes a ese lugar en ese momento. Es decir, se considera no
solo la dimensión espacial de los datos si no también la temporal, al
mayor nivel de desagregación disponible. Este es un enfoque mucho menos
explorado, debido fundamentalmente a dos factores:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  La dificultad de disponer de información fiable y de calidad
  desagregada espacio-temporalmente
\item
  La dificultad de trabajar con datos de estas características de cara
  al análisis y principalmente a la modelización, ya que son datos
  correlados en el tiempo y en el espacio.
\end{enumerate}

Queda claro, por tanto, que se trata de un problema complejo que
requiere de simplificaciones para poder ser abordado, más aun dadas las
limitaciones en los recursos computacionales disponibles y la enorme
cantidad de de datos que se están considerando y que requieren de un
procesamiento sumamente costoso desde un punto de vista computacional.

Por todo ello, esta sección es probablemente la de mayor importancia y
dificultad de todo el trabajo, ya que implica la toma de decisiones que
serán determinantes de cara al correcto desempeño de los modelos que se
construirán más adelante, requiere de un vasto conocimiento del problema
que permita un enfoque adecuado que haga posible la consecución de los
objetivos que se esperan conseguir, necesita del uso de técnicas
específicas de procesamiento de datos espaciales que no han sido
tratadas durante el grado y se ve fuertemente limitada por los escasos
recursos computacionales disponibles.

\hypertarget{determinaciuxf3n-del-marco-del-estudio}{%
\section{Determinación del marco del
estudio}\label{determinaciuxf3n-del-marco-del-estudio}}

El primer paso ha sido limitar el área y la franja temporal que abarcará
el estudio. Para ello, ha sido necesario basarse principalmente en la
disponibilidad y consistencia de la información requerida para el
proyecto y en las limitaciones computacionales impuestas por el equipo
disponible.

En cuanto a la disponibilidad de información, hay que diferenciar entre
la información de incendios forestales y la información de variables que
permitan explicar este fenómeno considerando la mayor desagregación
espacial y temporal posible.

\hypertarget{incendios-forestales}{%
\subsection{Incendios forestales}\label{incendios-forestales}}

En lo referente a los datos sobre incendios forestales cabe mencionar
que España cuenta con una de las mayores y más completas bases de datos
sobre incendios forestales a nivel europeo. Se trata de la Estadística
General de Incendios Forestales (EGIF), que en su versión definitiva
actualmente contiene toda la información que se recoge en cada parte de
incendio forestal que ha tenido lugar en España desde 1983 hasta 2015,
incluyendo su información espacial con sus coordenadas de origen. Se ha
explorado extensamente el uso de esta base de datos para el proyecto,
dada su exhaustividad y completitud. Sin embargo, lamentablemente no ha
sido posible en este caso incorporarla al trabajo por diversas razones.

La principal de ellas fue que hasta marzo de 2024 la base de datos de la
EGIF solo se encontraba disponible en el Catálogo de Datos del Gobierno
de España en formato TURTLE\footnote{TURTLE es una sintaxis para RDF con
  compatible con SPARQL. RDF (Resource Description Framework) es un
  estándar de semántica web utilizado para el intercambiar de datos en
  la Web.} y esto conllevó numerosas dificultades. Se exploraron
distintas librerías de R (y alguna de Python) para el manejo de datos en
este formato como RDFlib. Sin embargo, al tratarse de una base de datos
de un tamaño considerable (aproximadamente 1GB y con más de una decena
de millones de tripletas), esta librería no era suficientemente
eficiente para poder realizar consultas en un tiempo razonable al
conjunto de datos. Tras explorar otras alternativas, se valoró la
posibilidad de usar un triplestore, es decir, una base de datos
especialmente diseñada para el almacenamiento y recuperación de
tripletas a través de consultas semánticas. En este caso se usó Apache
Jena Fuseki, ya que cuenta con una interfaz que facilita su uso. Sin
embargo, aunque esto supuso una mejora considerable en la eficiencia y
permitió realizar consultas sencillas a la base de datos, en este caso
fue la complejidad del gráfico de datos (ontología) y la escasa
documentación disponible sobre esta, la que impidió que se pudiesen
realizar las consultas más complejas que requería para llevar a cabo el
proyecto. Además, se debe tener en cuenta que se trata de una base de
datos muy heterogénea y con numerosos datos faltantes debida su
naturaleza, por lo que requiere de un preprocesamiento que probablemente
será complicado y costoso en tiempo y en recursos computacionales. Al no
disponer de ninguno de estos, finalmente se optó por buscar una
alternativa más abarcable dada las limitaciones con las que cuenta un
Trabajo de Fin de Estudios, aunque queda abierta la posibilidad de
explorar esta base de datos en futuros estudios, la cual aportar nuevas
dimensiones al estudio de los incendios forestales en España gracias a
la enorme cantidad de información que ofrece.

Ante esta situación, la solución planteada fue limitar el área en
estudio a la Comunidad Autónoma de Andalucía, aprovechando la enorme
disponibilidad de información medioambiental que ofrece la Red de
Información Ambiental de Andalucía (REDIAM). En particular, se emplea la
cartografía generada por la REDIAM sobre las áreas recorridas por los
incendios forestales entre 1975 y 2022. Esta contiene los perímetros de
incendios forestales mayores de 100 ha en Andalucía obtenidos a partir
de imágenes de satélite y datos de campo. Se trata por tanto de una
información que no es exhaustiva, pues los incendios con una extensión
inferior a 100ha no han sido considerados. Sin embargo, frente a no
disponer de otra información operativa de mayor calidad, se utilizará
esta teniendo en cuenta que tendrá un efecto sobre las conclusiones que
se puedan sacar de los modelos que se construyan.

\hypertarget{variables-predictoras}{%
\subsection{Variables predictoras}\label{variables-predictoras}}

Una vez limitada la extensión territorial del estudio el siguiente paso
era acotar la franja temporal que abarcaría el estudio en base a la
disponibilidad de datos adecuados para explicar el fenómeno en cuestión
desagregados espacial y temporalmente.

Los incendios forestales son un proceso sumamente complejo, en el que
actúan numerosos factores de muy distinta índole (\ldots). Además,
dentro de un incendio forestal se pueden distinguir distintas fases que
presentan características muy diversas y sobre las que actúan distintos
agentes: ignición, propagación y extinción. Dada la información sobre
incendios forestales disponible, se está obligado a adoptar un enfoque
global, pues no se dispone de los puntos de ignición u origen de los
incendios forestales. El enfoque será, por tanto, intentar predecir si
una determinada localización se verá afectada por un incendio forestal
(de más de 100 ha) en un momento concreto.

Además, es importante tener en cuenta que existen factores estructurales
que tienen una influencia directa sobre los regímenes de incendios
forestales como son las tendencias de uso y explotación de los bosques,
la presencia de interfaz urbano forestal, los tipos y técnicas de
agricultura que se llevan a cabo, la presencia e intensidad del
pastoreo, los cambios en los usos de suelo e incluso conductas sociales
y tendencias demográficas diversas. Se trata de variables que cambian a
lo largo de periodos relativamente largos de tiempo y que muy
difícilmente pueden ser incluidos en los modelos, dada la falta de datos
sobre ellas, así como su carácter transversal. Por ello, se ha
considerado conveniente no extender en exceso el periodo de estudio,
reconocida la imposibilidad de incluir en el modelo todas las variables
que tienen un impacto relevante en la aparición de incendios y que son
cambiantes en el tiempo.

Todo ello hace necesario que el conjunto de datos utilizado contenga
información sobre todas las dimensiones (o al menos las principales) que
influyen en cualquiera de las fases de un incendio forestal. Es decir,
se deben incluir la dimensión antropogénica, la demográfica, la
hidrográficas, la topográfica, la meteorológica y la vegetación. Es
importante recalcar que siempre se hace referencia a datos geoespaciales
pues debe ser la información relativa al lugar (y al momento) del
incendio, con la dificultad posterior que esto supondrá.

Por último, es importante diferenciar entre características que se
considerarán estructurales (y por tanto invariantes a lo largo del
periodo de estudio) y aquellas que se considerarán variables en el
tiempo. Dentro de las primeras se encuentran todas las características
relacionadas con la topografía del terreno, las infraestructuras y los
usos del suelo, como por ejemplo el modelo de elevaciones, la
distribución de asentamientos de población, la red de carreteras y el
uso de suelo. Todas las demás variables de carácter demográfico,
meteorológico o de vegetación se considerarán, por tanto, desagregadas
temporalmente.

En base a todo lo mencionado y a la disponibilidad de información de
calidad de las categorías comentadas, se ha decidido limitar la franja
temporal del estudio a 20 años que van de 2002 a 2022, ambos inclusive.

\hypertarget{fuentes-de-datos}{%
\section{Fuentes de datos}\label{fuentes-de-datos}}

Como se ha comentado en la sección anterior, los datos sobre los
incendios forestales se han obtenido de los perímetros de incendios
forestales mayores de 100 ha en Andalucía entre 1975 y 2020 disponibles
la REDIAM. De cada incendio registrado se dispone de su fecha de inicio,
del área recorrida por el fuego y del municipio en el que originó, así
como de otras variables que dependen del año de la campaña y que no son
relevantes de cara a nuestro estudio.

Tomando como base estudios similares (\ldots) y partiendo de las 6
categorías ya mencionadas se han recopilado 23 conjuntos de datos de
distinto tipo que se usarán para explicar y predecir los incendios
forestales en Andalucía. Estos conjuntos se recogen en la Tabla
\ref{tab:fuentes}, donde también se indica la fuente de la que ha sido
obtenido cada uno de ellos, el tipo de datos que contiene (indicando su
resolución en el caso de los datos ráster) y la frecuencia de las
observaciones (o resolución temporal) en el caso de las variables
temporales. En realidad, el número de archivos de datos que se manejan
es mucho mayor, ya que por ejemplo para la variable NDVI se dispone de
un archivo tiff para cada mes del periodo de estudio, lo que añade
cierta complejidad al manejo de la información.

Es relevante la heterogeneidad de los datos recopilados, pues se dispone
tanto de datos tabulares como de datos espaciales y dentro de estos
últimos de datos vectoriales y datos ráster, con distintas resoluciones,
distintas frecuencias y distintos sistemas de referencia de coordenadas.
Esto hará que el procesamiento de estos datos hasta obtener datos
adecuados para el análisis estadístico sea costoso y que deban
utilizarse técnicas específicas de geocumputación.

Cabe también mencionar que se ha optado por el uso de datos
meteorológicos basados en modelos y en observaciones satelitares, en
lugar del uso de datos provenientes de estaciones meteorológicas. Si
bien la información de estaciones meteorológica puede ser más precisa,
la dificultad de disponer de datos consistentes y continuos en el tiempo
a lo largo del periodo de estudio de las variables meteorológicas
seleccionadas ha hecho que este enfoque no sea viable. En esta dirección
se ha explorado la API de la AEMET y algunos paquetes de R como
\texttt{climate}, sin llegar a resultados satisfactorios. Por otro lado,
el paquete \texttt{nasapower} permite la descarga de una gran cantidad
de variables meteorológicas con frecuencia diaria y con una resolución
de aproximadamente \(0.5 \times 0.625\) grados de latitud y longitud
(unos 50km). Si bien es cierto que no es lo ideal, es la única opción
que se ha considerado viable y de cara a la construcción de unos
primeros modelos aproximativos podría ser suficiente. Si quisiese
extenderse el estudio, sería conveniente profundizar en la búsqueda de
alternativas que permitan obtener información meteorológica de una mayor
calidad y detalle.

\begin{table}
\begin{threeparttable}[]
\resizebox{\textwidth}{!}{
\begin{tabular}{lllll}
\hline
\textbf{Categoría}     & \textbf{Datos}          & \textbf{Fuente}    & \textbf{Tipo de dato}       & \textbf{Frecuencia} \\ \hline
\multirow{4}{*}{Topográficas}   & Altitud                                                        & DERA\tnote{a}  & TIFF (100m)        & -          \\
                                & Orientación                                                    & REDIAM\tnote{b}     & TIFF (100m)        & -          \\
                                & Pendiente                                                      & REDIAM     & TIFF (100m)        & -          \\
                                & Curvatura                                                      & REDIAM     & TIFF (100m)        & -          \\ \hline
Vegetación                      & NDVI                                                           & REDIAM     & TIFF (250m)        & Mensual    \\ \hline
\multirow{7}{*}{Antropogénicas} & Uso de suelo                                                   & DERA       & Shapefile          & -          \\
                                & Red de carreteras                                              & DERA       & Shapefile          & -          \\
                                & Red de ferrocarril                                             & DERA       & Shapefile          & -          \\
                                & Línea eléctrica                                                & DERA       & Shapefile          & -          \\
                                & Espacio protegido                                              & DERA       & Shapefile          & -          \\
                                & Senderos / Vías Verde / Carriles Bici                          & DERA       & Shapefile          & -          \\
                                & Caminos / Vías Pecuarias                                       & DERA       & Shapefile          & -          \\ \hline
Demográficas                    & Población del municipio                                        & IECA\tnote{c}       & csv                & Anual      \\ \hline
Hidrográficas                   & Principales Ríos                                               & MAGRAMA\tnote{d}    & Shapefile          & -          \\ \hline
\multirow{6}{*}{Meteorológicas} & Precipitación (mm/day)                                         & NASA POWER\tnote{e} & df (0.5º x 0.625º) & Diaria     \\
                                & Temperatura a 2m sobre la superficie (º)                       & NASA POWER & df (0.5º x 0.625º) & Diaria     \\
                                & Humedad del suelo (\%)                                         & NASA POWER & df (0.5º x 0.625º) & Diaria     \\
                                & Dirección del viento a 10 metros sobre la superficie terrestre(º) & NASA POWER & df (0.5º x 0.625º) & Diaria     \\
                                & Humedad relativa a 2m sobre la superficie (\%)                 & NASA POWER & df (0.5º x 0.625º) & Diaria     \\
                                & Cantidad de precipitaciones (mm/day)                           & NASA POWER & dfdf (0.5º x 0.625º) & Diaria     \\ \hline
\footnotesize Fuente: Elaboración propia
\end{tabular}}
\begin{tablenotes}
\raggedright
\item[a] {\footnotesize \href{https://www.juntadeandalucia.es/institutodeestadisticaycartografia/dega/datos-espaciales-de-referencia-de-andalucia-dera/descarga-de-informacion}{Datos Espaciales de Referencia de Andalucía (DERA)}}
\item[b] {\footnotesize \href{https://portalrediam.cica.es/descargas?path=%2F}{Descargas Rediam}}%2F}{Descargas Rediam}}
\item[c] {\footnotesize \href{https://www.juntadeandalucia.es/institutodeestadisticaycartografia/dega/}{Instituto de Estadística y Cartografía de Andalucía (IECA)}}
\item[d] {\footnotesize Ministerio de Agricultura, Alimentación y Medio Ambiente (MAGRAMA)}
\item[e] {\footnotesize \href{https://power.larc.nasa.gov/#resources}{NASA Prediction Of Worlwide Energy Resources (NASA POWER)}}
\end{tablenotes}
\caption{Datos brutos}
\label{tab:fuentes}
\end{threeparttable}
\end{table}

\hypertarget{procesamiento-de-los-datos}{%
\section{Procesamiento de los datos}\label{procesamiento-de-los-datos}}

Una vez se dispone de todos los conjuntos de datos que se usarán en el
estudio, el siguiente paso será combinarlos de manera adecuada y
transformarlos a un formato apto para el análisis estadístico y la
construcción de modelos predictivos, es decir, a un data frame. Dado que
el objetivo que se persigue es predecir si, dada unas condiciones
meteorológicas concretas en un momento dado, un punto del territorio
andaluz se verá afectado o no por un incendio forestal, será necesario
disponer de una cantidad suficiente de muestras negativas y positivas
distribuidas espacial y temporalmente que tengan asociadas las variables
explicativas correspondientes.

Intuitivamente, las muestras positivas serán aquellas observaciones
(puntos definidos en el tiempo y en el espacio) dentro del marco
espacio-temporal del estudio en las que se ha detectado un incendio
forestal en el día de la observación. Es decir, son observaciones dentro
de los polígonos de incendios el día que estos se han producido. Por
tanto, las muestras negativas serán observaciones dentro del marco
espacio-temporal definido en las que no se ha detectado un incendio
forestal. Es imporante tener en cuenta que dado que solo se dispone de
los incendios con una extensión mayor a 100 ha, la muestra cuenta con un
importante sesgo, ya que los casos positivos están infrarepresentados.
Por ello, no podremos hacer inferencia a todos los incendios forestales,
si no solo a los de una extensión superior a 100ha.

A continuación se detalla el proceso seguido para generar el conjunto de
datos depurado sobre el que desarrollar el estudio a partir de los
distintos conjuntos de datos en bruto:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Generación de una muestra balanceada de casos positivos y negativos.
\end{enumerate}

Para poder construir cualquier modelo de clasificación binaria se
necesita disponer de una muestra que cuente con un número suficiente de
casos positivos y negativo. Además, es aconsejable trabajar con
conjuntos de datos balanceados para evitar sesgos en los modelos de
clasificación {[}ARTICULO{]}.

Como ya se ha comentado, se considerarán observaciones positivas
aquellas que se hayan visto afectadas por un incendio forestal en el día
y lugar de la observación. En cambio, serán observaciones negativas
aquellas que no se hayan visto afectadas por un incendio forestal en el
día y lugar de la observación. Estas observaciones deberán generarse a
partir de los polígonos de incendios disponibles. Para ello, se usará un
enfoque similar al utilizado en
{[}\url{https://www.researchgate.net/publication/228527438_Learning_to_predict_forest_fires_with_different_data_mining_techniques}{]},
con una diferencia fundamental. En {[}cita al paper{]} usan los puntos
de ignición como muestras positivas y su objetivo es predecir los puntos
de origen de los incendios forestales. En cambio, en el presente trabajo
no se disponen de los puntos de ignición de los incendios, por lo que el
enfoque adoptado es ligeramente diferente; el objetivo es predecir las
zonas que pueden verse afectadas por un incendio forestal (superior a
100ha) bajo unas circunstancias concretas. De esta forma, los casos
positivos serán puntos aletorios tomados dentro de los polígonos de
incendio en los días que estos ocurrieron. Los casos negativos se
generarán igual que en {[}cita paper{]}: se toman fechas aleatorias
dentro del periodo de estudio y a cada una de ellas se le asocia una
localización aleatoria dentro del área de estudio satisfaciendo que
deben estar a al menos 15km de de cualquier incendio detectado en un
margen de ± 3 días. Esta forma de tomar los casos negativos asegura que
estén lo suficientemente alejados de los incendios forestales para
representar condiciones no influidas por estos, dando prioridad así a
las áreas con una menor prioridad de ocurrencia de incendio en un
período definido. Las ubicaciones de los ejemplos positivos y negativos
de ocurrencia de incendios estaban vinculadas espacial y temporalmente a
los datos descriptivos. Sería conveniente en estudios futuros plantearse
si esos parámetros (franja de ± 3 días y distancia superior a 15km) son
adecuados o tal vez sería más conveniente tomar otros valores, basados,
por ejemplo, en la duración media de los incendios en Andalucía y otras
características propias de los incendios en la región.

Tanto en (cita paper anterior) como en otros estudios consultados se
generan los casos negativos tomando fechas completamente aleatorias
dentro de la franja temporal del estudio. Sin embargo, esto induce un
claro sesgo en los datos, ya que las observaciones positivas no se
distribuyen uniformemente entre los 12 meses, si no que se concentran
marcadamente en los meses de verano. Generar el conjunto de datos sin
tener en cuenta este hecho hace que las muestras positivas y negativas
tengan características meteorológicas muy diferenciadas que no responden
al verdadero proceso latente de aparición de incendios forestales sino
al proceso de selección de la muestra, puediendo provocar un marcado
sesgo positivo en las medidas de evaluación de los modelos que en
realidad no estarían reflejando la realidad si no los sesgos
introducidos en los conjuntos de datos. Por ello, en el presente trabajo
se explorará también la posibilidad de generar los días de las muestras
negativas siguiendo una distribución de probabilidad proporcional a la
cantidad de incendios observados a lo largo del periodo de estudio en
cada mes.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\item
  Asignación a cada observación los valores correspondientes a ese día y
  a esa localización concreta de todas las variables predictoras a
  partir de los conjuntos de datos que se han recopilado (recogidos en
  la Tabla \ref{tab:fuentes}). Para ello se ha hecho uso de las
  funciones disponibles en los paquetes \texttt{terra} y \texttt{sf}.
\item
  Depuración de la muestra generada, se eliminan los valores perdidos y
  se ajustan adecuadamente los tipos de las variables. Las variable
  \texttt{WD10M} se codifica mediante los 4 puntos cardinales y sus
  bisectrices, generando así 8 clases. En el caso de la variable
  \texttt{orientacion} se procede de manera idéntica pero se incluye
  también la clase ``plano'', si la pendiente del punto es 0.
\end{enumerate}

El resultado es un conjunto de datos con 20998 observaciones de las
variables que se muestran en la Tabla \ref{tab:variables}.

\begin{table}[]
\resizebox{\columnwidth}{!}{%
\begin{tabular}{llll}
\hline
Categoría         & Nombre           & Descripción                                                                            & Tipo       \\ \hline
Topográficas      & elevacion        & Elevación sobre el nivel del mar (m)                                                   & numérica   \\
                  & orientacion      & Orientación de la pendiente descendiente                                               & categórica \\
                  & pendiente        & Pendiente del terreno (º)                                                              & numérica   \\
                  & curvatura        & Curvatura de la superfie                                                               & numérica   \\
Vegetación        & NDVI             & Índice de vegetación de diferencia normalizada                                         & numérica   \\
Antropogénicas    & uso\_suelo       & Clasificación del uso del suelo                                                        & categórica \\
                  & dist\_carretera  & Distancia a la carretera más cercana (m)                                               & numérica   \\
                  & dist\_ferocarril & Distancia a la vía de ferrocarril más cercana (m)                                      & numérica   \\
                  & dist\_electr     & Distancia a la línea electrica más cercana (m)                                         & numérica   \\
                  & enp              & Espacio Natural Protegido                                                              & categórica \\
                  & dist\_sendero    & Distancia a la vía verde, al carril bici o al sendero más cercano (m)                  & numérica   \\
                  & dist\_camino     & Distancia al camino o a la vía pecuaria más cercano (m)                                & numérica   \\
Demográficas      & poblacion        & Número de habitantes del municipio                                                     & numérica   \\
Hidrográficas     & dist\_rios       & Distancia al río más próximo (m)                                                       & numérica   \\
Meteorológicas &
  PRECTRORCORR &
  \begin{tabular}[c]{@{}l@{}}Promedio corregido del total de precipitaciones en la superficie \\ de la tierra en masa de agua (incluye el contenido de agua en la nieve) (mm/día)\end{tabular} &
  numérica \\
                  & T2M              & Temperatura promedio del aire a 2 metros sobre la superficie de la tierra (ºC)         & numérica   \\
                  & GWETTOP          & Porcentaje de humedad del suelo                                                        & numérica   \\
                  & WD10M            & Promedio de la dirección del viento a 10 metros sobre la superficie de la tierra       & categórica \\
                  & WS10M            & Promedio de la velocidad del viento a 10 metros sobre la superficie de la tierra (m/s) & numérica   \\
                  & RH2M             & Humedad relativa a 2 metros sobre la superficie de la tierra                           & numérica   \\ \hline
Variable Objetivo & fire             & Incendio forestal                                                                      & categórica \\ \hline
Identificadoras   & date             & Fecha de la observación                                                                & fecha      \\
                  & municipio        & Nombre del municipio                                                                   & texto      \\
                  & cod\_municipio   & Código del municipio                                                                   & texto      \\
                  & geometry         & Geometría de los puntos                                                                & sfc       
\end{tabular}%
}
\caption{Conjunto de datos depurados}
\label{tab:variables}
\end{table}

\FloatBarrier

\ifdefined\ifprincipal
\else
\setlength{\parindent}{1em}
\pagestyle{fancy}
\setcounter{tocdepth}{4}
\tableofcontents

\fi

\ifdefined\ifdoblecara
\fancyhead{}{}
\fancyhead[LE,RO]{\scriptsize\rightmark}
\fancyfoot[LO,RE]{\scriptsize\slshape \leftmark}
\fancyfoot[C]{}
\fancyfoot[LE,RO]{\footnotesize\thepage}
\else
\fancyhead{}{}
\fancyhead[RO]{\scriptsize\rightmark}
\fancyfoot[LO]{\scriptsize\slshape \leftmark}
\fancyfoot[C]{}
\fancyfoot[RO]{\footnotesize\thepage}
\fi

\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0.4pt}

\hypertarget{cuerpo}{%
\chapter{Cuerpo}\label{cuerpo}}

\hypertarget{anuxe1lisis-exploratorio-de-datos-1}{%
\section{Análisis exploratorio de
datos}\label{anuxe1lisis-exploratorio-de-datos-1}}

\hypertarget{estudio-de-las-variables}{%
\section{Estudio de las variables}\label{estudio-de-las-variables}}

\hypertarget{modelizaciuxf3n}{%
\section{Modelización}\label{modelizaciuxf3n}}

\hypertarget{modelo-1}{%
\subsection{Modelo 1}\label{modelo-1}}

\hypertarget{modelo-2}{%
\subsection{Modelo 2}\label{modelo-2}}

\hypertarget{section}{%
\subsection{\ldots{}}\label{section}}

\hypertarget{comparaciuxf3n}{%
\section{Comparación}\label{comparaciuxf3n}}

\FloatBarrier

\ifdefined\ifprincipal
\else
\setlength{\parindent}{1em}
\pagestyle{fancy}
\setcounter{tocdepth}{4}
\tableofcontents

\fi

\ifdefined\ifdoblecara
\fancyhead{}{}
\fancyhead[LE,RO]{\scriptsize\rightmark}
\fancyfoot[LO,RE]{\scriptsize\slshape \leftmark}
\fancyfoot[C]{}
\fancyfoot[LE,RO]{\footnotesize\thepage}
\else
\fancyhead{}{}
\fancyhead[RO]{\scriptsize\rightmark}
\fancyfoot[LO]{\scriptsize\slshape \leftmark}
\fancyfoot[C]{}
\fancyfoot[RO]{\footnotesize\thepage}
\fi

\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0.4pt}

\hypertarget{estudios-de-casos-de-interuxe9s}{%
\chapter{Estudios de casos de
interés}\label{estudios-de-casos-de-interuxe9s}}

\FloatBarrier

\ifdefined\ifprincipal
\else
\setlength{\parindent}{1em}
\pagestyle{fancy}
\setcounter{tocdepth}{4}
\tableofcontents

\fi

\ifdefined\ifdoblecara
\fancyhead{}{}
\fancyhead[LE,RO]{\scriptsize\rightmark}
\fancyfoot[LO,RE]{\scriptsize\slshape \leftmark}
\fancyfoot[C]{}
\fancyfoot[LE,RO]{\footnotesize\thepage}
\else
\fancyhead{}{}
\fancyhead[RO]{\scriptsize\rightmark}
\fancyfoot[LO]{\scriptsize\slshape \leftmark}
\fancyfoot[C]{}
\fancyfoot[RO]{\footnotesize\thepage}
\fi

\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0.4pt}

\hypertarget{conclusiuxf3n}{%
\chapter{Conclusión}\label{conclusiuxf3n}}

\FloatBarrier
\appendix

\ifdefined\ifprincipal
\else
\setlength{\parindent}{1em}
\pagestyle{fancy}
\setcounter{tocdepth}{4}
\tableofcontents

\fi

\ifdefined\ifdoblecara
\fancyhead{}{}
\fancyhead[LE,RO]{\scriptsize\rightmark}
\fancyfoot[LO,RE]{\scriptsize\slshape \leftmark}
\fancyfoot[C]{}
\fancyfoot[LE,RO]{\footnotesize\thepage}
\else
\fancyhead{}{}
\fancyhead[RO]{\scriptsize\rightmark}
\fancyfoot[LO]{\scriptsize\slshape \leftmark}
\fancyfoot[C]{}
\fancyfoot[RO]{\footnotesize\thepage}
\fi

\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0.4pt}

\hypertarget{apuxe9ndice-tuxedtulo-del-apuxe9ndice}{%
\chapter{Apéndice: Título del
Apéndice}\label{apuxe9ndice-tuxedtulo-del-apuxe9ndice}}

\hypertarget{primera-secciuxf3n}{%
\section{Primera sección}\label{primera-secciuxf3n}}

\ifdefined\ifprincipal
\else
\setlength{\parindent}{1em}
\pagestyle{fancy}
\setcounter{tocdepth}{4}
\tableofcontents

\fi

\ifdefined\ifdoblecara
\fancyhead{}{}
\fancyhead[LE,RO]{\scriptsize\rightmark}
\fancyfoot[LO,RE]{\scriptsize\slshape \leftmark}
\fancyfoot[C]{}
\fancyfoot[LE,RO]{\footnotesize\thepage}
\else
\fancyhead{}{}
\fancyhead[RO]{\scriptsize\rightmark}
\fancyfoot[LO]{\scriptsize\slshape \leftmark}
\fancyfoot[C]{}
\fancyfoot[RO]{\footnotesize\thepage}
\fi

\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0.4pt}

\hypertarget{apuxe9ndice-tuxedtulo-del-apuxe9ndice-1}{%
\chapter{Apéndice: Título del
Apéndice}\label{apuxe9ndice-tuxedtulo-del-apuxe9ndice-1}}

\hypertarget{primera-secciuxf3n-1}{%
\section{Primera sección}\label{primera-secciuxf3n-1}}

\FloatBarrier
\cleardoublepage

\ifdefined\ifdoblecara
  \fancyhead[LE,RO]{}
  \fancyfoot[LO,RE]{}
  \fancyhead[CO,CE]{Bibliografía}
\else
  \fancyhead[RO]{}
  \fancyfoot[LO]{}
  \fancyhead[CO]{Bibliografía}
\fi

\ifdefined\ifcitapandoc

\hypertarget{bibliografuxeda}{%
\chapter*{Bibliografía}\label{bibliografuxeda}}
\addcontentsline{toc}{chapter}{Bibliografía}

\else

\nocite{Luque2017,Luque2019,RStudio,R-base2,
R-knitr,R-rmarkdown,Techopedia,
webfacmatematicasus1,webUS2a,webPedroLuque,
lopez2007aplicacion}

\fi

\bibliography{bib/library.bib,bib/paquetes.bib}


%


\end{document}
