---
title: "Modelos"
author: "Juan Baeza Ruiz-Henestrosa"
date: "`r Sys.Date()`"
output:
  html_document:
    df_print: paged
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = "D:/usuario/Documents/Universidad/5º/TFG - organizado") # Para que haga knitr desde el directorio del proyecto

```

```{r}
library(tidyverse)
library(tidymodels)
library(sf)
library(ggplot2)
library(akima) # interp
library(magrittr)
library(ggpubr)

```

## Carga de datos

Se cargan los datos depurados obtenidos a partir de la selección de casos estratificando según el mes.

```{r}
load("salidas_intermedias/datos_strat_depurados_geom_2024-04-27.RData")
# load("salidas_intermedias/datos_depurados_geom_2024-04-23.RData")
# load("salidas_intermedias/datos_depurados_geom_2024-04-26.RData")

```

## Funciones para la evaluación de modelos

Las siguientes funciones se utilizarán para evaluar el rendimiento de los modelos.

```{r}
# Función para obtener las medidas de rendimiento de los modelos a partir de un objeto predict 

get_metrics <- function(pred) {
  list(
    res = tibble(
      roc_auc = pred |> roc_auc(truth = fire, .pred_0) |> pull(.estimate),
      accuracy = pred |> accuracy(truth = fire, .pred_class) |> pull(.estimate),
      recall = pred |> sensitivity(truth = fire, .pred_class,event_level="second") |> pull(.estimate),
      specificity = pred |> spec(truth = fire, .pred_class,event_level="second") |> pull(.estimate)),
    conf_mat = pred |> conf_mat(truth = fire, .pred_class))
}


# Función para mostrar graficamente los resultados del tuning de un modelo con dos parámetros

tuning_plot = function(mod_res) {
  datos_metrics = mod_res %>% 
    collect_metrics()
  
  # min = min(datos_metrics$mean)
  # max = max(datos_metrics$mean)
  
  plots = list()
  
  for (metric in unique(datos_metrics$.metric)) {
    
    datos = datos_metrics %>% 
      filter(.metric==metric) 
    
    # Interpolar los datos faltantes
    datos_interp <- interp(datos[[1]], datos[[2]], datos$mean)
    
    # Crear un nuevo dataframe con los datos interpolados
    datos_interp_df <- data.frame(
      expand.grid(x = datos_interp$x, y = datos_interp$y), z = as.vector(datos_interp$z))
    
    # Crear el gráfico de mapa de calor con interpolación
    p = ggplot(datos_interp_df, aes(x = x, y = y, fill = z)) +
      geom_tile() +
      # scale_fill_viridis_c(option = "turbo", limits = c(min,max), name = NULL)+
      scale_fill_viridis_c(option = "turbo", name = NULL)+
      labs(title = metric,
           x = colnames(datos)[1],
           y = colnames(datos)[2],
           fill = metric)
    
    plots[[metric]] = p
    
  }
  
  # ggarrange(plotlist = plots,
  #           legend = "right",
  #           common.legend = T)
  
  ggarrange(plotlist = plots)
  
  
}

```


## Partición temporal entrenamiento / validación / test

Se crea una partición temporal en entrenamiento / validación / test, para evitar el sesgo look-ahead (mirar al futuro). Ordenando los casos según su fecha, se toman el primer 60% para entrenar los modelos, el 20% siguiente para la validación y el último 20% para la evaluación del rendimiento.

```{r}
set.seed(123)

splits = initial_validation_time_split(datos, 
                                       prop=c(0.6,0.2))

training <- training(splits) %>%  st_drop_geometry()
val_set <- validation_set(splits) %>% st_drop_geometry()
test  <- testing(splits) %>% st_drop_geometry()

# # Para hacerlo por fecha manualmente:
# splits <- make_splits(
#   x = list(analysis = which(year(datos$date)<2021),
#            assessment = which(year(datos$date)>=2021)),
#   data=datos
# )
# 
# training_val <- training(splits)
# test  <- testing(splits)
# 
#
# length(splits$out_id)/length(splits$in_id)
# [1] 0.1149578
# 
# 1.a. Partición train- validation
# set.seed(234)
# val <- make_splits(
#   x = list(analysis = which(year(datos$date)<2018),
#            assessment = which(year(datos$date)>=2018)),
#   data=datos
# )
# 
# val_set <- validation_time_split(training_val, 
#                                  prop = 0.80)

```


## Modelos

### Logistic regression with penalty

<!-- https://glmnet.stanford.edu/articles/glmnet.html -->
<!-- mixture = 0 : lasso regression -->
<!-- mixture = 1 : ridge regression -->
<!-- penalty: parámetro de regularización -->

En primer lugar se usa un modelo de regresión logística con término de penalización. En función de si el término de regularización es de grado 1 o 2, se le da el nombre de lasso o ridge (logistic) regression. Se ajusta el grado del término de penalización así como su coeficiente usando para ello el conjunto de validación.

```{r}
# 1º Definimos el modelo:

lr_mod <- 
  logistic_reg(penalty = tune(), mixture = tune()) %>% 
  set_engine("glmnet")


# 2º Creamos la receta

lr_recipe <- 
  recipe(fire ~ ., data = training) %>% 
  step_date(date,features = c("dow","month")) %>% 
  # step_holiday(date, holidays = holidays) %>% 
  step_rm(date,cod_municipio,municipio) %>% # Se eliminan variables identificadoras
  step_dummy(all_nominal_predictors()) %>% # Se crean variables dummy para los factores
  step_zv(all_predictors()) %>% # Eliminar variables con varianza nula
  step_normalize(all_predictors()) # Se normalizan todos los predictores


# 3º Creamos el workflow

lr_workflow <- 
  workflow() %>% 
  add_model(lr_mod) %>% 
  add_recipe(lr_recipe)


# 4º Creamos el grid para los parámetros

lr_reg_grid <- expand_grid(penalty = 10^seq(-4, -1, length.out = 30),
                           mixture = c(0,1))


# 5º Ajustamos el modelo

lr_res <- 
  lr_workflow %>% 
  tune_grid(val_set,
            grid = lr_reg_grid,
            control = control_grid(save_pred = TRUE),
            metrics = metric_set(accuracy,roc_auc,recall,spec))

# 6º Evaluación de modelos

# Se muestran las medidas de rendimiento en función del parámetro de penalización
lr_plot <- 
  lr_res %>% 
  collect_metrics() %>% 
  # filter(.metric == "accuracy") %>%
  ggplot(aes(x = penalty, y = mean,col=.metric,linetype=as.factor(mixture))) + 
  geom_point() + 
  geom_line() + 
  ylab("Medidas de rendimiento") +
  scale_x_log10(labels = scales::label_number())
lr_plot

lr_res |> 
  collect_metrics() |> 
  group_by(.metric)|> 
  summarise(max = max(mean),min=min(mean))

lr_res %>% 
  show_best(metric = "accuracy", n = 15)

lr_res %>% tuning_plot()

# 7º Selección del mejor modelo

lr_best <- 
  lr_res %>% 
  select_best(metric="accuracy")

lr_best


# 8º Se evalúa el modelo:

# Curva ROC
lr_auc <- 
  lr_res %>% 
  collect_predictions(parameters = lr_best) %>% 
  roc_curve(fire, .pred_0) %>% 
  mutate(model = "Logistic Regression")

autoplot(lr_auc)


#   Medidas de rendimiento
lr_metrics <- lr_res |> 
  collect_predictions(parameters = lr_best) |> 
  get_metrics()

lr_metrics

```

#### Análisis gráfico de los "residuos" / errores:

```{r}
## No tengo claro como medir los residuos en un modelo de clasificación:
lr_residuals = lr_res %>%
  collect_predictions(parameters = lr_best) %>% 
  mutate(residual = fire !=.pred_class,
         geometry = st_geometry(validation(splits))) %>% 
  st_as_sf(sf_column_name ="geometry")


ggplot(lr_residuals,aes(col=residual)) + 
  geom_sf(alpha=0.6) +
  labs(col = "Error")
```

#### Interpretación coeficientes

```{r}
# Cómo obtengo los p_valores de los contrastes individuales?

lr_workflow %>% 
  finalize_workflow(lr_best) %>%
  fit(training) %>% 
  extract_fit_parsnip() %>% 
  tidy()

lr_workflow %>% 
  finalize_workflow(lr_best) %>%
  fit(training) %>%
  extract_fit_engine() %>% 
  tidy()

```


## Logistic regression 

Regresión logística sin parámetro de regularización.

```{r}
# 1º definimos el modelo 
# lrs: logistic regression simple

lrs_mod <- 
  logistic_reg(penalty = NULL, mixture = NULL) %>% 
  set_engine("glm")

# 2º Creamos la receta

lrs_recipe <- 
  recipe(fire ~ ., data = training) %>% 
  step_date(date,features = c("dow","month")) %>% 
  # step_holiday(date, holidays = holidays) %>% 
  step_rm(date,cod_municipio,municipio) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_zv(all_predictors()) %>% 
  step_normalize(all_predictors())

# 3º Creamos el workflow
lrs_workflow <- 
  workflow() %>% 
  add_model(lrs_mod) %>% 
  add_recipe(lrs_recipe)

# 4º Ajustamos el modelo.
lrs_res <- 
  lrs_workflow %>% 
  fit(training)

# 5º Se evalúa su rendimiento en los datos de validación.
lrs_pred = cbind(predict(lrs_res,new_data = validation(splits),type="prob"),
             predict(lrs_res,new_data = validation(splits),type="class"),
             fire = validation(splits)$fire)

# 6º Se evalúa el modelo:
#   Curva ROC
lrs_auc <- lrs_pred %>% 
  roc_curve(fire, .pred_0) %>% 
  mutate(model = "Logistic Regression")

autoplot(lrs_auc)


#   Medidas de rendimiento
lrs_metrics <- lrs_pred |> 
  get_metrics()

lrs_metrics
```

#### Coeficientes
```{r}
lrs_res %>% extract_fit_engine() %>% summary()
```
 
GWETTOP WD10M, orientacion, date_dow no significativas


### Logistic regression with feature selection
```{r}
# 1º definimos el modelo 
# lrs2: logistic regression simple

lrs2_mod <- 
  logistic_reg(penalty = NULL, mixture = NULL) %>% 
  set_engine("glm")

# 2º Creamos la receta

lrs2_recipe <- 
  recipe(fire ~ ., data = training) %>% 
  step_date(date,features = c("month")) %>% 
  # step_holiday(date, holidays = holidays) %>% 
  step_rm(date,cod_municipio,municipio, GWETTOP, WD10M, orientacion) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_zv(all_predictors()) %>% 
  step_normalize(all_predictors())

# 3º Creamos el workflow
lrs2_workflow <- 
  workflow() %>% 
  add_model(lrs2_mod) %>% 
  add_recipe(lrs2_recipe)

# 4º Ajustamos el modelo.
lrs2_res <- 
  lrs2_workflow %>% 
  fit(training)

# 5º Se evalúa su rendimiento en los datos de validación.
lrs2_pred = cbind(predict(lrs2_res,new_data = validation(splits),type="prob"),
             predict(lrs2_res,new_data = validation(splits),type="class"),
             fire = validation(splits)$fire)

# 6º Se evalúa el modelo:
#   Curva ROC
lrs2_auc <- lrs2_pred %>% 
  roc_curve(fire, .pred_0) %>% 
  mutate(model = "Logistic Regression")

autoplot(lrs2_auc)


#   Medidas de rendimiento
lrs2_metrics <- lrs2_pred |> 
  get_metrics()

```


### Logistic regression + PCA

```{r}
lr_pca_mod <- 
  logistic_reg(penalty = NULL, mixture = NULL) %>% 
  set_engine("glm")

# 3º Creamos la receta

lr_pca_recipe <- 
  recipe(fire ~ ., data = training) %>% 
  step_pca(all_numeric_predictors(),num_comp = 14) %>% 
  step_date(date,features = c("dow","month")) %>% 
  # step_holiday(date, holidays = holidays) %>% 
  step_rm(date,cod_municipio,municipio) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_zv(all_predictors()) %>% 
  step_normalize(all_predictors())

# 4º Creamos el workflow
lr_pca_workflow <- 
  workflow() %>% 
  add_model(lr_pca_mod) %>% 
  add_recipe(lr_pca_recipe)

# 5º Ajustamos el modelo.
lr_pca_res <- 
  lr_pca_workflow %>% 
  fit(training)

lr_pca_res %>% extract_fit_engine() %>% summary()

lr_pca_pred = cbind(predict(lr_pca_res,new_data = validation(splits),type="prob"),
             predict(lr_pca_res,new_data = validation(splits),type="class"),
             fire = validation(splits)$fire)

# 6º Se evalúa el modelo:
#   Curva ROC
lr_pca_auc <- lr_pca_pred %>% 
  roc_curve(fire, .pred_0) %>% 
  mutate(model = "Logistic Regression")

autoplot(lr_pca_auc)


#   Medidas de rendimiento
lr_pca_metrics <- lr_pca_pred |> 
  get_metrics()

lr_pca_metrics

```

Este enfoque no parece aportar ninguna mejora.


## 2. Random Forest

```{r}
# Detectar el número de núcleos para trabajar en paralelo
cores <- parallel::detectCores()
cores

# Construimos el modelo, especificando el número de núcleos a usar en la computación en paralelo de forma que la computación sea más eficiente

# 1º Construir el modelo
rf_mod <- 
  rand_forest(mtry = tune(), min_n = tune(), trees = 1000) %>% 
  set_engine("ranger", num.threads = cores) %>% 
  set_mode("classification")

# 2º Construir la receta con el preprocesamiento
rf_recipe <- 
  recipe(fire ~ ., data = training) %>% 
  step_date(date,features = c("dow", "month")) %>% 
  # step_holiday(date) %>% 
  step_rm(date, cod_municipio, municipio) 
# No normalizamos en este caso pues no es necesario

# 3º Ensamblar todo con workflow
rf_workflow <- 
  workflow() %>% 
  add_model(rf_mod) %>% 
  add_recipe(rf_recipe)

# 4º Train and tune
rf_mod
extract_parameter_set_dials(rf_mod)
```

```{r eval=FALSE, include=TRUE}

set.seed(345)
rf_res <- 
  rf_workflow %>% 
  tune_grid(val_set,
            grid = 25,
            control = control_grid(save_pred = TRUE),
            metrics = metric_set(accuracy,roc_auc,recall,spec))

```

```{r include=FALSE}
# Tarda demasiado, lo cargamos

load("salidas_intermedias/trained_models/trained_models_strat_24_04_27.RData")
```


``` {r}
# Resultados del tuning

rf_res |> 
  collect_metrics() |> 
  group_by(.metric)|> 
  summarise(max = max(mean),min=min(mean))
# Se observa mayor variabilidad que en el caso de la regresión logística con penalización




tuning_plot(rf_res)

# Se observa que al disminuir mtry y aumentar min_n aumenta la tasa de acierto. Sin embargo, los valores más elevados de recall se encuentran para valores pequeños de min_n, al contrario de lo que ocurre con la especificidad. Sin embargo, dado que la variación en la especificidad es de casi un 20% mientras que la de la sensitividad es de tan solo un 7% se opta por maximizar la tasa de acierto como solución de compromiso.

# En comparación con el modelo de regresión logística este nos permite conseguir niveles de sensitividad más elevados (superiores al 90%) 

# Mejor modelo
rf_best <- 
  rf_res %>% 
  select_best(metric = "accuracy")

rf_best

# mtry = 1 --> BOOSTING DECISION TREES??


# Curva ROC
rf_auc <- 
  rf_res %>% 
  collect_predictions(parameters = rf_best) %>% 
  roc_curve(fire, .pred_0) %>% 
  mutate(model = "Random Forest")

autoplot(rf_auc)

#   Medidas de rendimiento:

rf_metrics <- rf_res |> 
  collect_predictions(parameters = rf_best) |> 
  get_metrics()

rf_metrics
lr_metrics

# save(rf_res,file = "salidas_intermedias/trained_models/trained_models_strat_24_04_27.RData")
```


## 3. Neuronal Networks
```{r}
load("salidas_intermedias/trained_models/svm_res_strat.RData")
```



## 4. SVM
```{r}
# 1º Construir el modelo
svm_mod <- 
  svm_linear(cost = tune(), margin = tune()) %>% 
  set_engine("kernlab") %>% 
  set_mode("classification")

# 2º Construir la receta con el preprocesamiento
svm_recipe <- 
  recipe(fire ~ ., data = training) %>% 
  step_date(date,features = c("dow", "month")) %>% 
  step_rm(date, cod_municipio, municipio) %>%
  step_dummy(all_nominal_predictors()) %>% 
  step_zv(all_predictors()) %>% 
  step_normalize(all_predictors())

# 3º Ensamblar todo con workflow
svm_workflow <- 
  workflow() %>% 
  add_model(svm_mod) %>% 
  add_recipe(svm_recipe)
```


```{r eval=FALSE, include=TRUE}
# 4º Train and tune
set.seed(345)
svm_res <- 
  svm_workflow %>% 
  tune_grid(val_set,
            grid = 25,
            control = control_grid(save_pred = TRUE),
            metrics = metric_set(accuracy,roc_auc,recall,spec))

```

```{r include=FALSE}
# Tada demasiado, lo cargamos:
load("salidas_intermedias/trained_models/svm_res_strat.RData")
```


```{r}
# 5º Evaluación de resultados del tuning
svm_res |> 
  collect_metrics() |> 
  group_by(.metric)|> 
  summarise(max = max(mean),min=min(mean))

tuning_plot(svm_res)

# Se observa que hay muy poca variabilidad en los resultados de cada combinación de parámetros, menos de un 2.5% para cada medida de rendimiento. Ante este resultado, de nuevo, se opta por maximizar la tasa de acierto


# Mejor modelo
svm_best <- 
  svm_res %>% 
  select_best(metric="accuracy")

svm_best

# Curva ROC
svm_auc <- 
  svm_res %>% 
  collect_predictions(parameters = svm_best) %>% 
  roc_curve(fire, .pred_0) %>% 
  mutate(model = "Support Vector Machine")

autoplot(svm_auc)


svm_metrics <- svm_res |> 
  collect_predictions(parameters = svm_best) |> 
  get_metrics()

```


# Comparación

```{r}
results = rbind(
  lr = lr_res %>% 
    collect_predictions(parameters = lr_best) %>% 
    get_metrics() %>% 
    .$res %>%
    as.data.frame(),
  lrs = lrs_metrics$res %>% 
    as.data.frame(),
  lrs2 = lrs2_metrics$res %>% 
    as.data.frame(),
  lr_pca = lr_pca_metrics$res %>% 
    as.data.frame(),
  rf = rf_res %>% 
    collect_predictions(parameters = rf_best) %>%
    get_metrics() %>% 
    .$res %>%
    as.data.frame(),
  svm = svm_res %>% 
    collect_predictions(parameters = svm_best) %>%
    get_metrics() %>% 
    .$res %>%
    as.data.frame()
)

library(knitr)

kable(results,digits=3)
```

```{r}
bind_rows(rf_auc, lr_auc,svm_auc) %>% 
  ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) + 
  geom_path(lwd = 1, alpha = 0.7) +
  geom_abline(lty = 3) + 
  coord_equal() + 
  scale_color_viridis_d(option = "turbo", end = .6)
```

# Test

Se unen los conjuntos training y validation para entrenar el modelo final

### Logistic Regression 
```{r}
# the last model
lr_best # penalty = 0.000259 
        # mixture = 1
last_lr_mod <- logistic_reg(penalty = lr_best$penalty, mixture = lr_best$penalty) |> 
  set_engine("glmnet")
  
# the last workflow
last_lr_workflow <- 
  lr_workflow %>% 
  update_model(last_lr_mod)

# the last fit
set.seed(345)
last_lr_fit <- 
  last_lr_workflow %>% 
  last_fit(splits,
           add_validation_set = T)

last_lr_fit

last_lr_fit %>% 
  collect_metrics()

last_lr_fit %>% 
  collect_predictions() %>% 
  get_metrics()

```

### RF
```{r}
# the last model
rf_best # mtry = 4
        # min_n = 35 

last_rf_mod <- 
  rand_forest(mtry = rf_best$mtry, min_n = rf_best$min_n, trees = 1000) %>% 
  set_engine("ranger", num.threads = cores,importance="impurity") %>% 
  set_mode("classification")
  
# the last workflow
last_rf_workflow <- 
  rf_workflow %>% 
  update_model(last_rf_mod)

# the last fit

set.seed(345)
last_rf_fit <- 
  last_rf_workflow %>% 
  last_fit(splits,
           add_validation_set = T)

last_rf_fit

last_rf_fit %>% 
  collect_metrics()

last_rf_fit %>% 
  collect_predictions() %>% 
  get_metrics()


library(vip)
last_rf_fit %>% 
  extract_fit_parsnip() %>% 
  vip(num_features = 20)

last_rf_fit %>% 
  collect_predictions() %>% 
  roc_curve(fire, .pred_0) %>% 
  autoplot()
```



### SVM
```{r}
# the last model
svm_best # cost = 0.145
         # margin = 0.0270
last_svm_mod <-  
  svm_linear(cost = svm_best$cost, margin = svm_best$margin) %>% 
  set_engine("kernlab") %>% 
  set_mode("classification")
  
# the last workflow
last_svm_workflow <- 
  svm_workflow %>% 
  update_model(last_svm_mod)

# the last fit
set.seed(345)
last_svm_fit <- 
  last_svm_workflow %>% 
  last_fit(splits,
           add_validation_set = T)

last_svm_fit %>% 
  collect_metrics()

last_svm_fit %>% 
  collect_predictions() %>% 
  get_metrics()
```


```{r}
results_test = rbind(
  lr = last_lr_fit %>% 
  collect_predictions() %>% 
  get_metrics() %>% 
    .$res %>%
    as.data.frame(),
  rf = last_rf_fit %>% 
  collect_predictions() %>% 
  get_metrics() %>% 
    .$res %>%
    as.data.frame(),
  svm = last_svm_fit %>% 
  collect_predictions() %>% 
  get_metrics() %>% 
    .$res %>%
    as.data.frame()
)

library(knitr)

kable(results_test,digits=3)
```


```{r}
save(lr_res,rf_res,svm_res,last_svm_fit,last_lr_fit,last_rf_fit,
     file = "salidas_intermedias/trained_models_strat_2024_04_28.RData")
```

