---
title: "Modelos"
author: "Juan Baeza Ruiz-Henestrosa"
date: "`r Sys.Date()`"
output: pdf_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(tidymodels)
```

## Load data
```{r}
load("salidas_intermedias/datos_depurados2024-04-09.RData")
```

## Función medidas de rendimiento
```{r}
# Será la función que usemos para medir el rendimiento del modelo

get_metrics <- function(pred) {
  list(
    res = tibble(
      roc_auc = pred |> roc_auc(truth = fire, .pred_0) |> pull(.estimate),
      accuracy = pred |> accuracy(truth = fire, .pred_class) |> pull(.estimate),
      recall = pred |> sensitivity(truth = fire, .pred_class) |> pull(.estimate),
      specificity = pred |> spec(truth = fire, .pred_class) |> pull(.estimate)),
    conf_mat = pred |> conf_mat(truth = fire, .pred_class))
}

```


## 1. Logistic regression
```{r}
# 1º Se crea una partición entrenamiento / validación / test en orden cronológico, para evitar el sesgo look-ahead.

set.seed(123)

splits = initial_validation_time_split(datos, 
                                       prop=c(0.6,0.2))

training <- training(splits)
val_set <- validation_set(splits)
test  <- testing(splits)

# # Para hacerlo por fecha
# splits <- make_splits(
#   x = list(analysis = which(year(datos$date)<2021),
#            assessment = which(year(datos$date)>=2021)),
#   data=datos
# )
# 
# training_val <- training(splits)
# test  <- testing(splits)
# 
#
# length(splits$out_id)/length(splits$in_id)
# [1] 0.1149578
# 
# 1.a. Partición train- validation
# set.seed(234)
# val <- make_splits(
#   x = list(analysis = which(year(datos$date)<2018),
#            assessment = which(year(datos$date)>=2018)),
#   data=datos
# )
# 
# val_set <- validation_time_split(training_val, 
#                                  prop = 0.80)



#2º definimos el modelo
lr_mod <- 
  logistic_reg(penalty = tune(), mixture = 0) %>% 
  set_engine("glmnet")

# 3º Creamos la receta
holidays <- c("AllSouls", "AshWednesday", "ChristmasEve", "Easter", 
              "ChristmasDay", "GoodFriday", "NewYearsDay", "PalmSunday")

lr_recipe <- 
  recipe(fire ~ ., data = training) %>% 
  step_date(date,features = c("dow", "month")) %>% 
  step_holiday(date, holidays = holidays) %>% 
  step_rm(date,cod_municipio,municipio) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_zv(all_predictors()) %>% 
  step_normalize(all_predictors())

# 4º Creamos el workflow
lr_workflow <- 
  workflow() %>% 
  add_model(lr_mod) %>% 
  add_recipe(lr_recipe)

# 5º Ajustamos el modelo.
# Create the grid of tuning
lr_reg_grid <- tibble(penalty = 10^seq(-4, -1, length.out = 30))

lr_res <- 
  lr_workflow %>% 
  tune_grid(val_set,
            grid = lr_reg_grid,
            control = control_grid(save_pred = TRUE),
            metrics = metric_set(accuracy,roc_auc,recall,spec))


# Se muestra la tasa de acierto en función del parámetro de penalización
lr_plot <- 
  lr_res %>% 
  collect_metrics() %>% 
  # filter(.metric == "accuracy") %>%
  ggplot(aes(x = penalty, y = mean,col=.metric)) + 
  geom_point() + 
  geom_line() + 
  ylab("Medidas de rendimiento") +
  scale_x_log10(labels = scales::label_number())

lr_plot

top_models <-
  lr_res %>% 
  show_best(metric = "accuracy", n = 15) %>% 
  arrange(penalty) 
top_models


# Se elige el que tiene una precisión más elevada
lr_best <- 
  lr_res %>% 
  select_best(metric="accuracy")

lr_best

# 6º Se evalúa el modelo:
#   Curva ROC
lr_auc <- 
  lr_res %>% 
  collect_predictions(parameters = lr_best) %>% 
  roc_curve(fire, .pred_0) %>% 
  mutate(model = "Logistic Regression")

autoplot(lr_auc)

#   Medidas de rendimiento
lr_metrics <- lr_res |> 
  collect_predictions(parameters = lr_best) |> 
  get_metrics()

lr_metrics
```




## 2. Random Forest

```{r}
# Detectar el número de núcleos para trabajar en paralelo
cores <- parallel::detectCores()
cores

# Construimos el modelo, especificando el número de núcleos a usar en la computación en paralelo de forma que la computación sea más eficiente

# 1º Construir el modelo
rf_mod <- 
  rand_forest(mtry = tune(), min_n = tune(), trees = 1000) %>% 
  set_engine("ranger", num.threads = cores) %>% 
  set_mode("classification")

# 2º Construir la receta con el preprocesamiento
rf_recipe <- 
  recipe(fire ~ ., data = training) %>% 
  step_date(date,features = c("dow", "month")) %>% 
  step_holiday(date) %>% 
  step_rm(date, cod_municipio, municipio) 
# No normalizamos en este caso pues no es necesario

# 3º Ensamblar todo con workflow
rf_workflow <- 
  workflow() %>% 
  add_model(rf_mod) %>% 
  add_recipe(rf_recipe)

# 4º Train and tune
rf_mod
extract_parameter_set_dials(rf_mod)

set.seed(345)
rf_res <- 
  rf_workflow %>% 
  tune_grid(val_set,
            grid = 25,
            control = control_grid(save_pred = TRUE),
            metrics = metric_set(accuracy,roc_auc,recall,spec))

# Resultados del tuning
rf_res %>% 
  show_best(metric = "roc_auc")


autoplot(rf_res)

# Mejor modelo
rf_best <- 
  rf_res %>% 
  select_best

rf_best

# Predicciones
rf_res %>% 
  collect_predictions() 

rf_auc <- 
  rf_res %>% 
  collect_predictions(parameters = rf_best) %>% 
  roc_curve(fire, .pred_0) %>% 
  mutate(model = "Random Forest")

autoplot(rf_auc)

```


## 3. Neuronal Networks

## 4. SVM
```{r}
# 1º Construir el modelo
sv_mod <- 
  svm_linear(cost = tune(), margin = tune()) %>% 
  set_engine("kernlab") %>% 
  set_mode("classification")

# 2º Construir la receta con el preprocesamiento
sv_recipe <- 
  recipe(fire ~ ., data = hotel_other) %>% 
  step_date(date,features = c("dow", "month")) %>% 
  step_rm(date, cod_municipio, municipio) %>%
  step_dummy(all_nominal_predictors()) %>% 
  step_zv(all_predictors()) %>% 
  step_normalize(all_predictors())

# 3º Ensamblar todo con workflow
sv_workflow <- 
  workflow() %>% 
  add_model(sv_mod) %>% 
  add_recipe(sv_recipe)

# 4º Train and tune
set.seed(345)
sv_res <- 
  sv_workflow %>% 
  tune_grid(val_set,
            grid = 25,
            control = control_grid(save_pred = TRUE),
            metrics = metric_set(roc_auc))

# Resultados del tuning
sv_res %>% 
  show_best(metric = "roc_auc")

autoplot(sv_res)

# Mejor modelo
sv_best <- 
  sv_res %>% 
  select_best()

sv_best

# Predicciones
sv_res %>% 
  collect_predictions() 

sv_auc <- 
  sv_res %>% 
  collect_predictions(parameters = sv_best) %>% 
  roc_curve(fire, .pred_0) %>% 
  mutate(model = "Support Vector Machine")

autoplot(sv_auc)

##########################
# Test
# the last model
last_sv_mod <-  
  rand_forest(cost = 3, margin = 3) %>% 
  set_engine("kernlab") %>% 
  set_mode("classification")
  
# the last workflow
last_sv_workflow <- 
  sv_workflow %>% 
  update_model(last_sv_mod)

# the last fit
set.seed(345)
last_sv_fit <- 
  last_sv_workflow %>% 
  last_fit(splits)

last_sv_fit

last_sv_fit %>% 
  collect_metrics()

res_sv = last_sv_fit |> collect_predictions()

res_sv %>% roc_auc(truth = fire, .pred_0)
res_sv %>% accuracy(truth = fire, .pred_class)
res_sv %>% conf_mat(truth = fire, .pred_class)
res_sv %>% sensitivity(truth = fire, .pred_class)
res_sv %>% spec(truth = fire, .pred_class)

```


# Comparación

### Logistic Regression
```{r}
# the last model
last_lr_mod <- logistic_reg(penalty = 0.00281,mixture = 0) |> 
  set_engine("glmnet")
  
# the last workflow
last_lr_workflow <- 
  lr_workflow %>% 
  update_model(last_lr_mod)

# the last fit
set.seed(345)
last_lr_fit <- 
  last_lr_workflow %>% 
  last_fit(splits)

last_lr_fit

last_lr_fit %>% 
  collect_metrics()

res_lr = last_lr_fit |> collect_predictions()

res_lr %>% roc_auc(truth = fire, .pred_0)
res_lr %>% accuracy(truth = fire, .pred_class)
res_lr %>% conf_mat(truth = fire, .pred_class)
res_lr %>% sensitivity(truth = fire, .pred_class)
res_lr %>% spec(truth = fire, .pred_class)

```

### RF
```{r}
# Test
# the last model
last_rf_mod <- 
  rand_forest(mtry = 3, min_n = 3, trees = 1000) %>% 
  set_engine("ranger", num.threads = cores) %>% 
  set_mode("classification")
  
# the last workflow
last_rf_workflow <- 
  rf_workflow %>% 
  update_model(last_rf_mod)

# the last fit

set.seed(345)
last_rf_fit <- 
  last_rf_workflow %>% 
  last_fit(splits)

last_rf_fit

last_rf_fit %>% 
  collect_metrics()

res_rf = last_rf_fit |> collect_predictions()

res_rf %>% roc_auc(truth = fire, .pred_0)
res_rf %>% accuracy(truth = fire, .pred_class)
res_rf %>% conf_mat(truth = fire, .pred_class)
res_rf %>% sensitivity(truth = fire, .pred_class)
res_rf %>% spec(truth = fire, .pred_class)

# Comparación
bind_rows(rf_auc, lr_auc) %>% 
  ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) + 
  geom_path(lwd = 1.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal() + 
  scale_color_viridis_d(option = "plasma", end = .6)


####
# the last model
last_rf_mod <- 
  rand_forest(mtry = 8, min_n = 7, trees = 1000) %>% 
  set_engine("ranger", num.threads = cores, importance = "impurity") %>% 
  set_mode("classification")

# the last workflow
last_rf_workflow <- 
  rf_workflow %>% 
  update_model(last_rf_mod)

# the last fit
set.seed(345)
last_rf_fit <- 
  last_rf_workflow %>% 
  last_fit(splits)

last_rf_fit

last_rf_fit %>% 
  collect_metrics()

library(vip)
last_rf_fit %>% 
  extract_fit_parsnip() %>% 
  vip(num_features = 20)

last_rf_fit %>% 
  collect_predictions() %>% 
  roc_curve(fire, .pred_0) %>% 
  autoplot()

rf_coef = last_rf_fit %>% 
  extract_fit_engine() |> 
  tidy()
  

str(rf_parnship)
```

